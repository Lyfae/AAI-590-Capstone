{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.3404825737265416,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002680965147453083,
      "grad_norm": 4.094821929931641,
      "learning_rate": 0.0009994638069705094,
      "loss": 41.5,
      "step": 1
    },
    {
      "epoch": 0.005361930294906166,
      "grad_norm": 5.413318634033203,
      "learning_rate": 0.0009989276139410188,
      "loss": 42.0,
      "step": 2
    },
    {
      "epoch": 0.00804289544235925,
      "grad_norm": 7.137814044952393,
      "learning_rate": 0.000998391420911528,
      "loss": 41.75,
      "step": 3
    },
    {
      "epoch": 0.010723860589812333,
      "grad_norm": 9.622596740722656,
      "learning_rate": 0.0009978552278820374,
      "loss": 40.25,
      "step": 4
    },
    {
      "epoch": 0.013404825737265416,
      "grad_norm": 9.371594429016113,
      "learning_rate": 0.0009973190348525468,
      "loss": 37.25,
      "step": 5
    },
    {
      "epoch": 0.0160857908847185,
      "grad_norm": 7.442983150482178,
      "learning_rate": 0.0009967828418230562,
      "loss": 34.25,
      "step": 6
    },
    {
      "epoch": 0.01876675603217158,
      "grad_norm": 7.496304512023926,
      "learning_rate": 0.0009962466487935656,
      "loss": 33.25,
      "step": 7
    },
    {
      "epoch": 0.021447721179624665,
      "grad_norm": 8.448534965515137,
      "learning_rate": 0.000995710455764075,
      "loss": 31.125,
      "step": 8
    },
    {
      "epoch": 0.024128686327077747,
      "grad_norm": 7.0896077156066895,
      "learning_rate": 0.0009951742627345845,
      "loss": 29.375,
      "step": 9
    },
    {
      "epoch": 0.02680965147453083,
      "grad_norm": 8.449664115905762,
      "learning_rate": 0.0009946380697050939,
      "loss": 28.0,
      "step": 10
    },
    {
      "epoch": 0.029490616621983913,
      "grad_norm": 7.011782169342041,
      "learning_rate": 0.0009941018766756033,
      "loss": 26.625,
      "step": 11
    },
    {
      "epoch": 0.032171581769437,
      "grad_norm": 6.380695343017578,
      "learning_rate": 0.0009935656836461127,
      "loss": 24.875,
      "step": 12
    },
    {
      "epoch": 0.03485254691689008,
      "grad_norm": 6.666173458099365,
      "learning_rate": 0.0009930294906166219,
      "loss": 24.0,
      "step": 13
    },
    {
      "epoch": 0.03753351206434316,
      "grad_norm": 6.85632848739624,
      "learning_rate": 0.0009924932975871313,
      "loss": 22.5,
      "step": 14
    },
    {
      "epoch": 0.040214477211796246,
      "grad_norm": 7.1602582931518555,
      "learning_rate": 0.0009919571045576407,
      "loss": 21.375,
      "step": 15
    },
    {
      "epoch": 0.04289544235924933,
      "grad_norm": 7.728709697723389,
      "learning_rate": 0.00099142091152815,
      "loss": 19.75,
      "step": 16
    },
    {
      "epoch": 0.045576407506702415,
      "grad_norm": 11.000395774841309,
      "learning_rate": 0.0009908847184986595,
      "loss": 18.0,
      "step": 17
    },
    {
      "epoch": 0.04825737265415549,
      "grad_norm": 8.063335418701172,
      "learning_rate": 0.0009903485254691689,
      "loss": 15.9375,
      "step": 18
    },
    {
      "epoch": 0.05093833780160858,
      "grad_norm": 8.15415096282959,
      "learning_rate": 0.0009898123324396783,
      "loss": 14.25,
      "step": 19
    },
    {
      "epoch": 0.05361930294906166,
      "grad_norm": 10.970466613769531,
      "learning_rate": 0.0009892761394101877,
      "loss": 12.0625,
      "step": 20
    },
    {
      "epoch": 0.05630026809651475,
      "grad_norm": 9.255367279052734,
      "learning_rate": 0.000988739946380697,
      "loss": 9.6875,
      "step": 21
    },
    {
      "epoch": 0.058981233243967826,
      "grad_norm": 6.71974515914917,
      "learning_rate": 0.0009882037533512065,
      "loss": 7.1562,
      "step": 22
    },
    {
      "epoch": 0.06166219839142091,
      "grad_norm": 4.004016399383545,
      "learning_rate": 0.0009876675603217157,
      "loss": 5.7812,
      "step": 23
    },
    {
      "epoch": 0.064343163538874,
      "grad_norm": 1.8248860836029053,
      "learning_rate": 0.000987131367292225,
      "loss": 5.1875,
      "step": 24
    },
    {
      "epoch": 0.06702412868632708,
      "grad_norm": 0.9842170476913452,
      "learning_rate": 0.0009865951742627345,
      "loss": 4.9062,
      "step": 25
    },
    {
      "epoch": 0.06970509383378017,
      "grad_norm": 0.7422013282775879,
      "learning_rate": 0.000986058981233244,
      "loss": 4.8125,
      "step": 26
    },
    {
      "epoch": 0.07238605898123325,
      "grad_norm": 0.5527276992797852,
      "learning_rate": 0.0009855227882037533,
      "loss": 4.7188,
      "step": 27
    },
    {
      "epoch": 0.07506702412868632,
      "grad_norm": 0.433874249458313,
      "learning_rate": 0.0009849865951742627,
      "loss": 4.625,
      "step": 28
    },
    {
      "epoch": 0.0777479892761394,
      "grad_norm": 0.3552132546901703,
      "learning_rate": 0.0009844504021447721,
      "loss": 4.5625,
      "step": 29
    },
    {
      "epoch": 0.08042895442359249,
      "grad_norm": 0.3121449053287506,
      "learning_rate": 0.0009839142091152815,
      "loss": 4.5,
      "step": 30
    },
    {
      "epoch": 0.08310991957104558,
      "grad_norm": 0.299744576215744,
      "learning_rate": 0.000983378016085791,
      "loss": 4.5,
      "step": 31
    },
    {
      "epoch": 0.08579088471849866,
      "grad_norm": 0.4459809958934784,
      "learning_rate": 0.0009828418230563003,
      "loss": 4.4688,
      "step": 32
    },
    {
      "epoch": 0.08847184986595175,
      "grad_norm": 0.28072455525398254,
      "learning_rate": 0.0009823056300268097,
      "loss": 4.4375,
      "step": 33
    },
    {
      "epoch": 0.09115281501340483,
      "grad_norm": 0.2792031168937683,
      "learning_rate": 0.000981769436997319,
      "loss": 4.4062,
      "step": 34
    },
    {
      "epoch": 0.0938337801608579,
      "grad_norm": 0.29252859950065613,
      "learning_rate": 0.0009812332439678283,
      "loss": 4.3438,
      "step": 35
    },
    {
      "epoch": 0.09651474530831099,
      "grad_norm": 0.3004789352416992,
      "learning_rate": 0.0009806970509383377,
      "loss": 4.3125,
      "step": 36
    },
    {
      "epoch": 0.09919571045576407,
      "grad_norm": 0.28331777453422546,
      "learning_rate": 0.0009801608579088471,
      "loss": 4.2812,
      "step": 37
    },
    {
      "epoch": 0.10187667560321716,
      "grad_norm": 0.29593804478645325,
      "learning_rate": 0.0009796246648793566,
      "loss": 4.2188,
      "step": 38
    },
    {
      "epoch": 0.10455764075067024,
      "grad_norm": 0.3360207974910736,
      "learning_rate": 0.000979088471849866,
      "loss": 4.2188,
      "step": 39
    },
    {
      "epoch": 0.10723860589812333,
      "grad_norm": 0.9746589660644531,
      "learning_rate": 0.0009785522788203754,
      "loss": 4.125,
      "step": 40
    },
    {
      "epoch": 0.10991957104557641,
      "grad_norm": 0.40555500984191895,
      "learning_rate": 0.0009780160857908848,
      "loss": 4.0312,
      "step": 41
    },
    {
      "epoch": 0.1126005361930295,
      "grad_norm": 0.4915933609008789,
      "learning_rate": 0.0009774798927613942,
      "loss": 3.9688,
      "step": 42
    },
    {
      "epoch": 0.11528150134048257,
      "grad_norm": 0.5140253901481628,
      "learning_rate": 0.0009769436997319036,
      "loss": 3.9062,
      "step": 43
    },
    {
      "epoch": 0.11796246648793565,
      "grad_norm": 0.59853595495224,
      "learning_rate": 0.0009764075067024129,
      "loss": 3.7656,
      "step": 44
    },
    {
      "epoch": 0.12064343163538874,
      "grad_norm": 0.6076123118400574,
      "learning_rate": 0.0009758713136729223,
      "loss": 3.6562,
      "step": 45
    },
    {
      "epoch": 0.12332439678284182,
      "grad_norm": 3.3200981616973877,
      "learning_rate": 0.0009753351206434317,
      "loss": 3.6094,
      "step": 46
    },
    {
      "epoch": 0.1260053619302949,
      "grad_norm": 1.0076669454574585,
      "learning_rate": 0.0009747989276139411,
      "loss": 3.4062,
      "step": 47
    },
    {
      "epoch": 0.128686327077748,
      "grad_norm": 0.8188648223876953,
      "learning_rate": 0.0009742627345844505,
      "loss": 3.2656,
      "step": 48
    },
    {
      "epoch": 0.13136729222520108,
      "grad_norm": 1.087272047996521,
      "learning_rate": 0.0009737265415549598,
      "loss": 3.1094,
      "step": 49
    },
    {
      "epoch": 0.13404825737265416,
      "grad_norm": 0.8814945220947266,
      "learning_rate": 0.0009731903485254692,
      "loss": 2.9688,
      "step": 50
    },
    {
      "epoch": 0.13672922252010725,
      "grad_norm": 1.3187233209609985,
      "learning_rate": 0.0009726541554959786,
      "loss": 2.8125,
      "step": 51
    },
    {
      "epoch": 0.13941018766756033,
      "grad_norm": 9.174578666687012,
      "learning_rate": 0.000972117962466488,
      "loss": 2.6562,
      "step": 52
    },
    {
      "epoch": 0.14209115281501342,
      "grad_norm": 0.9857268333435059,
      "learning_rate": 0.0009715817694369974,
      "loss": 2.6094,
      "step": 53
    },
    {
      "epoch": 0.1447721179624665,
      "grad_norm": 0.899191677570343,
      "learning_rate": 0.0009710455764075067,
      "loss": 2.5625,
      "step": 54
    },
    {
      "epoch": 0.14745308310991956,
      "grad_norm": 1.0879570245742798,
      "learning_rate": 0.0009705093833780161,
      "loss": 2.4062,
      "step": 55
    },
    {
      "epoch": 0.15013404825737264,
      "grad_norm": 0.8324114084243774,
      "learning_rate": 0.0009699731903485255,
      "loss": 2.2031,
      "step": 56
    },
    {
      "epoch": 0.15281501340482573,
      "grad_norm": 0.910655677318573,
      "learning_rate": 0.0009694369973190349,
      "loss": 2.125,
      "step": 57
    },
    {
      "epoch": 0.1554959785522788,
      "grad_norm": 0.6608592867851257,
      "learning_rate": 0.0009689008042895443,
      "loss": 1.9766,
      "step": 58
    },
    {
      "epoch": 0.1581769436997319,
      "grad_norm": 0.7495169043540955,
      "learning_rate": 0.0009683646112600536,
      "loss": 1.8828,
      "step": 59
    },
    {
      "epoch": 0.16085790884718498,
      "grad_norm": 1.437414526939392,
      "learning_rate": 0.000967828418230563,
      "loss": 1.8516,
      "step": 60
    },
    {
      "epoch": 0.16353887399463807,
      "grad_norm": 0.5739189982414246,
      "learning_rate": 0.0009672922252010724,
      "loss": 1.6562,
      "step": 61
    },
    {
      "epoch": 0.16621983914209115,
      "grad_norm": 0.5036547780036926,
      "learning_rate": 0.0009667560321715818,
      "loss": 1.6172,
      "step": 62
    },
    {
      "epoch": 0.16890080428954424,
      "grad_norm": 0.49479731917381287,
      "learning_rate": 0.0009662198391420912,
      "loss": 1.5391,
      "step": 63
    },
    {
      "epoch": 0.17158176943699732,
      "grad_norm": 0.5023074746131897,
      "learning_rate": 0.0009656836461126005,
      "loss": 1.4531,
      "step": 64
    },
    {
      "epoch": 0.1742627345844504,
      "grad_norm": 1.9169942140579224,
      "learning_rate": 0.00096514745308311,
      "loss": 1.4375,
      "step": 65
    },
    {
      "epoch": 0.1769436997319035,
      "grad_norm": 0.556627631187439,
      "learning_rate": 0.0009646112600536194,
      "loss": 1.3047,
      "step": 66
    },
    {
      "epoch": 0.17962466487935658,
      "grad_norm": 0.7504363656044006,
      "learning_rate": 0.0009640750670241288,
      "loss": 1.2188,
      "step": 67
    },
    {
      "epoch": 0.18230563002680966,
      "grad_norm": 0.4650214910507202,
      "learning_rate": 0.0009635388739946382,
      "loss": 1.1406,
      "step": 68
    },
    {
      "epoch": 0.18498659517426275,
      "grad_norm": 0.49165165424346924,
      "learning_rate": 0.0009630026809651475,
      "loss": 1.1094,
      "step": 69
    },
    {
      "epoch": 0.1876675603217158,
      "grad_norm": 0.3694368898868561,
      "learning_rate": 0.0009624664879356569,
      "loss": 1.0312,
      "step": 70
    },
    {
      "epoch": 0.1903485254691689,
      "grad_norm": 0.45134884119033813,
      "learning_rate": 0.0009619302949061663,
      "loss": 0.9727,
      "step": 71
    },
    {
      "epoch": 0.19302949061662197,
      "grad_norm": 0.6614948511123657,
      "learning_rate": 0.0009613941018766757,
      "loss": 0.9609,
      "step": 72
    },
    {
      "epoch": 0.19571045576407506,
      "grad_norm": 0.41007348895072937,
      "learning_rate": 0.0009608579088471851,
      "loss": 0.918,
      "step": 73
    },
    {
      "epoch": 0.19839142091152814,
      "grad_norm": 0.39174968004226685,
      "learning_rate": 0.0009603217158176944,
      "loss": 0.8984,
      "step": 74
    },
    {
      "epoch": 0.20107238605898123,
      "grad_norm": 0.2502189576625824,
      "learning_rate": 0.0009597855227882038,
      "loss": 0.875,
      "step": 75
    },
    {
      "epoch": 0.2037533512064343,
      "grad_norm": 0.3806720972061157,
      "learning_rate": 0.0009592493297587132,
      "loss": 0.8086,
      "step": 76
    },
    {
      "epoch": 0.2064343163538874,
      "grad_norm": 0.2548300325870514,
      "learning_rate": 0.0009587131367292226,
      "loss": 0.7461,
      "step": 77
    },
    {
      "epoch": 0.20911528150134048,
      "grad_norm": 0.25718289613723755,
      "learning_rate": 0.000958176943699732,
      "loss": 0.6758,
      "step": 78
    },
    {
      "epoch": 0.21179624664879357,
      "grad_norm": 0.26958543062210083,
      "learning_rate": 0.0009576407506702413,
      "loss": 0.668,
      "step": 79
    },
    {
      "epoch": 0.21447721179624665,
      "grad_norm": 0.29016542434692383,
      "learning_rate": 0.0009571045576407507,
      "loss": 0.6406,
      "step": 80
    },
    {
      "epoch": 0.21715817694369974,
      "grad_norm": 0.30451810359954834,
      "learning_rate": 0.0009565683646112601,
      "loss": 0.6133,
      "step": 81
    },
    {
      "epoch": 0.21983914209115282,
      "grad_norm": 0.2497199922800064,
      "learning_rate": 0.0009560321715817695,
      "loss": 0.582,
      "step": 82
    },
    {
      "epoch": 0.2225201072386059,
      "grad_norm": 0.2642647325992584,
      "learning_rate": 0.0009554959785522789,
      "loss": 0.582,
      "step": 83
    },
    {
      "epoch": 0.225201072386059,
      "grad_norm": 0.33330249786376953,
      "learning_rate": 0.0009549597855227882,
      "loss": 0.582,
      "step": 84
    },
    {
      "epoch": 0.22788203753351208,
      "grad_norm": 0.2956954836845398,
      "learning_rate": 0.0009544235924932976,
      "loss": 0.5664,
      "step": 85
    },
    {
      "epoch": 0.23056300268096513,
      "grad_norm": 0.1924213320016861,
      "learning_rate": 0.000953887399463807,
      "loss": 0.5703,
      "step": 86
    },
    {
      "epoch": 0.23324396782841822,
      "grad_norm": 0.2678512930870056,
      "learning_rate": 0.0009533512064343164,
      "loss": 0.5273,
      "step": 87
    },
    {
      "epoch": 0.2359249329758713,
      "grad_norm": 0.15428487956523895,
      "learning_rate": 0.0009528150134048258,
      "loss": 0.5273,
      "step": 88
    },
    {
      "epoch": 0.2386058981233244,
      "grad_norm": 0.1895735263824463,
      "learning_rate": 0.0009522788203753351,
      "loss": 0.4648,
      "step": 89
    },
    {
      "epoch": 0.24128686327077747,
      "grad_norm": 0.2505476176738739,
      "learning_rate": 0.0009517426273458445,
      "loss": 0.4941,
      "step": 90
    },
    {
      "epoch": 0.24396782841823056,
      "grad_norm": 0.17959438264369965,
      "learning_rate": 0.0009512064343163539,
      "loss": 0.4648,
      "step": 91
    },
    {
      "epoch": 0.24664879356568364,
      "grad_norm": 0.30983635783195496,
      "learning_rate": 0.0009506702412868633,
      "loss": 0.4609,
      "step": 92
    },
    {
      "epoch": 0.24932975871313673,
      "grad_norm": 0.14923739433288574,
      "learning_rate": 0.0009501340482573728,
      "loss": 0.457,
      "step": 93
    },
    {
      "epoch": 0.2520107238605898,
      "grad_norm": 0.20119239389896393,
      "learning_rate": 0.0009495978552278822,
      "loss": 0.4316,
      "step": 94
    },
    {
      "epoch": 0.2546916890080429,
      "grad_norm": 0.1832408756017685,
      "learning_rate": 0.0009490616621983915,
      "loss": 0.4082,
      "step": 95
    },
    {
      "epoch": 0.257372654155496,
      "grad_norm": 0.13889513909816742,
      "learning_rate": 0.0009485254691689009,
      "loss": 0.4258,
      "step": 96
    },
    {
      "epoch": 0.26005361930294907,
      "grad_norm": 0.18250522017478943,
      "learning_rate": 0.0009479892761394103,
      "loss": 0.4082,
      "step": 97
    },
    {
      "epoch": 0.26273458445040215,
      "grad_norm": 0.16289867460727692,
      "learning_rate": 0.0009474530831099197,
      "loss": 0.4004,
      "step": 98
    },
    {
      "epoch": 0.26541554959785524,
      "grad_norm": 0.2458774447441101,
      "learning_rate": 0.0009469168900804291,
      "loss": 0.4141,
      "step": 99
    },
    {
      "epoch": 0.2680965147453083,
      "grad_norm": 0.12051112949848175,
      "learning_rate": 0.0009463806970509384,
      "loss": 0.3965,
      "step": 100
    },
    {
      "epoch": 0.2707774798927614,
      "grad_norm": 0.14612841606140137,
      "learning_rate": 0.0009458445040214478,
      "loss": 0.3574,
      "step": 101
    },
    {
      "epoch": 0.2734584450402145,
      "grad_norm": 0.1700150966644287,
      "learning_rate": 0.0009453083109919572,
      "loss": 0.3516,
      "step": 102
    },
    {
      "epoch": 0.2761394101876676,
      "grad_norm": 0.20658668875694275,
      "learning_rate": 0.0009447721179624666,
      "loss": 0.3262,
      "step": 103
    },
    {
      "epoch": 0.27882037533512066,
      "grad_norm": 0.3899141252040863,
      "learning_rate": 0.000944235924932976,
      "loss": 0.3555,
      "step": 104
    },
    {
      "epoch": 0.28150134048257375,
      "grad_norm": 0.2859193980693817,
      "learning_rate": 0.0009436997319034852,
      "loss": 0.3105,
      "step": 105
    },
    {
      "epoch": 0.28418230563002683,
      "grad_norm": 0.25368359684944153,
      "learning_rate": 0.0009431635388739946,
      "loss": 0.3262,
      "step": 106
    },
    {
      "epoch": 0.2868632707774799,
      "grad_norm": 0.17001894116401672,
      "learning_rate": 0.000942627345844504,
      "loss": 0.3242,
      "step": 107
    },
    {
      "epoch": 0.289544235924933,
      "grad_norm": 0.15162426233291626,
      "learning_rate": 0.0009420911528150134,
      "loss": 0.3555,
      "step": 108
    },
    {
      "epoch": 0.29222520107238603,
      "grad_norm": 0.20448242127895355,
      "learning_rate": 0.0009415549597855228,
      "loss": 0.3262,
      "step": 109
    },
    {
      "epoch": 0.2949061662198391,
      "grad_norm": 0.23639830946922302,
      "learning_rate": 0.0009410187667560321,
      "loss": 0.3203,
      "step": 110
    },
    {
      "epoch": 0.2975871313672922,
      "grad_norm": 0.1214347779750824,
      "learning_rate": 0.0009404825737265415,
      "loss": 0.3066,
      "step": 111
    },
    {
      "epoch": 0.3002680965147453,
      "grad_norm": 0.12820422649383545,
      "learning_rate": 0.0009399463806970509,
      "loss": 0.293,
      "step": 112
    },
    {
      "epoch": 0.30294906166219837,
      "grad_norm": 0.18421916663646698,
      "learning_rate": 0.0009394101876675603,
      "loss": 0.3223,
      "step": 113
    },
    {
      "epoch": 0.30563002680965146,
      "grad_norm": 0.12301234155893326,
      "learning_rate": 0.0009388739946380697,
      "loss": 0.2656,
      "step": 114
    },
    {
      "epoch": 0.30831099195710454,
      "grad_norm": 0.2842372953891754,
      "learning_rate": 0.000938337801608579,
      "loss": 0.3105,
      "step": 115
    },
    {
      "epoch": 0.3109919571045576,
      "grad_norm": 0.1376074105501175,
      "learning_rate": 0.0009378016085790884,
      "loss": 0.2715,
      "step": 116
    },
    {
      "epoch": 0.3136729222520107,
      "grad_norm": 0.14928561449050903,
      "learning_rate": 0.0009372654155495978,
      "loss": 0.3086,
      "step": 117
    },
    {
      "epoch": 0.3163538873994638,
      "grad_norm": 0.1665962040424347,
      "learning_rate": 0.0009367292225201072,
      "loss": 0.2637,
      "step": 118
    },
    {
      "epoch": 0.3190348525469169,
      "grad_norm": 0.1384596973657608,
      "learning_rate": 0.0009361930294906166,
      "loss": 0.293,
      "step": 119
    },
    {
      "epoch": 0.32171581769436997,
      "grad_norm": 0.11943553388118744,
      "learning_rate": 0.0009356568364611259,
      "loss": 0.2793,
      "step": 120
    },
    {
      "epoch": 0.32439678284182305,
      "grad_norm": 0.15297870337963104,
      "learning_rate": 0.0009351206434316353,
      "loss": 0.2617,
      "step": 121
    },
    {
      "epoch": 0.32707774798927614,
      "grad_norm": 0.1746274083852768,
      "learning_rate": 0.0009345844504021447,
      "loss": 0.2578,
      "step": 122
    },
    {
      "epoch": 0.3297587131367292,
      "grad_norm": 0.23644663393497467,
      "learning_rate": 0.0009340482573726542,
      "loss": 0.2715,
      "step": 123
    },
    {
      "epoch": 0.3324396782841823,
      "grad_norm": 0.23075322806835175,
      "learning_rate": 0.0009335120643431636,
      "loss": 0.25,
      "step": 124
    },
    {
      "epoch": 0.3351206434316354,
      "grad_norm": 0.11434713006019592,
      "learning_rate": 0.0009329758713136729,
      "loss": 0.3145,
      "step": 125
    },
    {
      "epoch": 0.3378016085790885,
      "grad_norm": 0.2182014435529709,
      "learning_rate": 0.0009324396782841823,
      "loss": 0.2832,
      "step": 126
    },
    {
      "epoch": 0.34048257372654156,
      "grad_norm": 0.1463240683078766,
      "learning_rate": 0.0009319034852546917,
      "loss": 0.2617,
      "step": 127
    },
    {
      "epoch": 0.34316353887399464,
      "grad_norm": 0.16704979538917542,
      "learning_rate": 0.0009313672922252011,
      "loss": 0.2656,
      "step": 128
    },
    {
      "epoch": 0.34584450402144773,
      "grad_norm": 0.10418020188808441,
      "learning_rate": 0.0009308310991957105,
      "loss": 0.248,
      "step": 129
    },
    {
      "epoch": 0.3485254691689008,
      "grad_norm": 0.10229937732219696,
      "learning_rate": 0.0009302949061662198,
      "loss": 0.2266,
      "step": 130
    },
    {
      "epoch": 0.3512064343163539,
      "grad_norm": 0.12001902610063553,
      "learning_rate": 0.0009297587131367292,
      "loss": 0.2412,
      "step": 131
    },
    {
      "epoch": 0.353887399463807,
      "grad_norm": 0.09072788059711456,
      "learning_rate": 0.0009292225201072386,
      "loss": 0.2354,
      "step": 132
    },
    {
      "epoch": 0.35656836461126007,
      "grad_norm": 0.09861829876899719,
      "learning_rate": 0.000928686327077748,
      "loss": 0.2578,
      "step": 133
    },
    {
      "epoch": 0.35924932975871315,
      "grad_norm": 0.1684478521347046,
      "learning_rate": 0.0009281501340482574,
      "loss": 0.293,
      "step": 134
    },
    {
      "epoch": 0.36193029490616624,
      "grad_norm": 0.14725051820278168,
      "learning_rate": 0.0009276139410187667,
      "loss": 0.2539,
      "step": 135
    },
    {
      "epoch": 0.3646112600536193,
      "grad_norm": 0.12352783977985382,
      "learning_rate": 0.0009270777479892761,
      "loss": 0.2188,
      "step": 136
    },
    {
      "epoch": 0.3672922252010724,
      "grad_norm": 0.1417471468448639,
      "learning_rate": 0.0009265415549597855,
      "loss": 0.2354,
      "step": 137
    },
    {
      "epoch": 0.3699731903485255,
      "grad_norm": 0.08479699492454529,
      "learning_rate": 0.0009260053619302949,
      "loss": 0.2598,
      "step": 138
    },
    {
      "epoch": 0.3726541554959786,
      "grad_norm": 0.20304365456104279,
      "learning_rate": 0.0009254691689008043,
      "loss": 0.2266,
      "step": 139
    },
    {
      "epoch": 0.3753351206434316,
      "grad_norm": 0.22194866836071014,
      "learning_rate": 0.0009249329758713136,
      "loss": 0.2285,
      "step": 140
    },
    {
      "epoch": 0.3780160857908847,
      "grad_norm": 0.11341596394777298,
      "learning_rate": 0.000924396782841823,
      "loss": 0.2363,
      "step": 141
    },
    {
      "epoch": 0.3806970509383378,
      "grad_norm": 0.13468405604362488,
      "learning_rate": 0.0009238605898123324,
      "loss": 0.2197,
      "step": 142
    },
    {
      "epoch": 0.38337801608579086,
      "grad_norm": 0.076375313103199,
      "learning_rate": 0.0009233243967828418,
      "loss": 0.2031,
      "step": 143
    },
    {
      "epoch": 0.38605898123324395,
      "grad_norm": 0.10989118367433548,
      "learning_rate": 0.0009227882037533512,
      "loss": 0.2119,
      "step": 144
    },
    {
      "epoch": 0.38873994638069703,
      "grad_norm": 0.1874612271785736,
      "learning_rate": 0.0009222520107238605,
      "loss": 0.2285,
      "step": 145
    },
    {
      "epoch": 0.3914209115281501,
      "grad_norm": 0.10381874442100525,
      "learning_rate": 0.0009217158176943699,
      "loss": 0.2129,
      "step": 146
    },
    {
      "epoch": 0.3941018766756032,
      "grad_norm": 0.0737760066986084,
      "learning_rate": 0.0009211796246648793,
      "loss": 0.1992,
      "step": 147
    },
    {
      "epoch": 0.3967828418230563,
      "grad_norm": 0.13673646748065948,
      "learning_rate": 0.0009206434316353887,
      "loss": 0.2002,
      "step": 148
    },
    {
      "epoch": 0.39946380697050937,
      "grad_norm": 0.1284661889076233,
      "learning_rate": 0.0009201072386058981,
      "loss": 0.2002,
      "step": 149
    },
    {
      "epoch": 0.40214477211796246,
      "grad_norm": 0.07227208465337753,
      "learning_rate": 0.0009195710455764074,
      "loss": 0.2197,
      "step": 150
    },
    {
      "epoch": 0.40482573726541554,
      "grad_norm": 0.053542088717222214,
      "learning_rate": 0.0009190348525469168,
      "loss": 0.2178,
      "step": 151
    },
    {
      "epoch": 0.4075067024128686,
      "grad_norm": 0.0772889256477356,
      "learning_rate": 0.0009184986595174263,
      "loss": 0.2119,
      "step": 152
    },
    {
      "epoch": 0.4101876675603217,
      "grad_norm": 0.12116805464029312,
      "learning_rate": 0.0009179624664879357,
      "loss": 0.2363,
      "step": 153
    },
    {
      "epoch": 0.4128686327077748,
      "grad_norm": 0.12567327916622162,
      "learning_rate": 0.0009174262734584451,
      "loss": 0.2109,
      "step": 154
    },
    {
      "epoch": 0.4155495978552279,
      "grad_norm": 0.1039428859949112,
      "learning_rate": 0.0009168900804289544,
      "loss": 0.1963,
      "step": 155
    },
    {
      "epoch": 0.41823056300268097,
      "grad_norm": 0.06572925299406052,
      "learning_rate": 0.0009163538873994638,
      "loss": 0.1875,
      "step": 156
    },
    {
      "epoch": 0.42091152815013405,
      "grad_norm": 0.27020809054374695,
      "learning_rate": 0.0009158176943699732,
      "loss": 0.1943,
      "step": 157
    },
    {
      "epoch": 0.42359249329758714,
      "grad_norm": 0.09034383296966553,
      "learning_rate": 0.0009152815013404826,
      "loss": 0.207,
      "step": 158
    },
    {
      "epoch": 0.4262734584450402,
      "grad_norm": 0.083586186170578,
      "learning_rate": 0.000914745308310992,
      "loss": 0.2041,
      "step": 159
    },
    {
      "epoch": 0.4289544235924933,
      "grad_norm": 0.07210909575223923,
      "learning_rate": 0.0009142091152815014,
      "loss": 0.1797,
      "step": 160
    },
    {
      "epoch": 0.4316353887399464,
      "grad_norm": 0.17327412962913513,
      "learning_rate": 0.0009136729222520107,
      "loss": 0.2002,
      "step": 161
    },
    {
      "epoch": 0.4343163538873995,
      "grad_norm": 0.1793481856584549,
      "learning_rate": 0.0009131367292225201,
      "loss": 0.2109,
      "step": 162
    },
    {
      "epoch": 0.43699731903485256,
      "grad_norm": 0.10437821596860886,
      "learning_rate": 0.0009126005361930295,
      "loss": 0.2002,
      "step": 163
    },
    {
      "epoch": 0.43967828418230565,
      "grad_norm": 0.059607233852148056,
      "learning_rate": 0.0009120643431635389,
      "loss": 0.1943,
      "step": 164
    },
    {
      "epoch": 0.44235924932975873,
      "grad_norm": 0.14320355653762817,
      "learning_rate": 0.0009115281501340483,
      "loss": 0.2061,
      "step": 165
    },
    {
      "epoch": 0.4450402144772118,
      "grad_norm": 0.09005400538444519,
      "learning_rate": 0.0009109919571045576,
      "loss": 0.1816,
      "step": 166
    },
    {
      "epoch": 0.4477211796246649,
      "grad_norm": 0.13945037126541138,
      "learning_rate": 0.000910455764075067,
      "loss": 0.1963,
      "step": 167
    },
    {
      "epoch": 0.450402144772118,
      "grad_norm": 0.11178182065486908,
      "learning_rate": 0.0009099195710455764,
      "loss": 0.1836,
      "step": 168
    },
    {
      "epoch": 0.45308310991957107,
      "grad_norm": 0.07875479757785797,
      "learning_rate": 0.0009093833780160858,
      "loss": 0.21,
      "step": 169
    },
    {
      "epoch": 0.45576407506702415,
      "grad_norm": 0.0738530308008194,
      "learning_rate": 0.0009088471849865952,
      "loss": 0.1992,
      "step": 170
    },
    {
      "epoch": 0.4584450402144772,
      "grad_norm": 0.10385853052139282,
      "learning_rate": 0.0009083109919571045,
      "loss": 0.2324,
      "step": 171
    },
    {
      "epoch": 0.46112600536193027,
      "grad_norm": 0.09368574619293213,
      "learning_rate": 0.0009077747989276139,
      "loss": 0.1934,
      "step": 172
    },
    {
      "epoch": 0.46380697050938335,
      "grad_norm": 0.12734414637088776,
      "learning_rate": 0.0009072386058981233,
      "loss": 0.1875,
      "step": 173
    },
    {
      "epoch": 0.46648793565683644,
      "grad_norm": 0.1611405313014984,
      "learning_rate": 0.0009067024128686327,
      "loss": 0.1924,
      "step": 174
    },
    {
      "epoch": 0.4691689008042895,
      "grad_norm": 0.061755552887916565,
      "learning_rate": 0.0009061662198391421,
      "loss": 0.1943,
      "step": 175
    },
    {
      "epoch": 0.4718498659517426,
      "grad_norm": 0.05461574345827103,
      "learning_rate": 0.0009056300268096514,
      "loss": 0.1797,
      "step": 176
    },
    {
      "epoch": 0.4745308310991957,
      "grad_norm": 0.1244334727525711,
      "learning_rate": 0.0009050938337801608,
      "loss": 0.1846,
      "step": 177
    },
    {
      "epoch": 0.4772117962466488,
      "grad_norm": 0.0824100449681282,
      "learning_rate": 0.0009045576407506702,
      "loss": 0.2139,
      "step": 178
    },
    {
      "epoch": 0.47989276139410186,
      "grad_norm": 0.05865325406193733,
      "learning_rate": 0.0009040214477211797,
      "loss": 0.1787,
      "step": 179
    },
    {
      "epoch": 0.48257372654155495,
      "grad_norm": 0.08235130459070206,
      "learning_rate": 0.0009034852546916891,
      "loss": 0.2383,
      "step": 180
    },
    {
      "epoch": 0.48525469168900803,
      "grad_norm": 0.10341799259185791,
      "learning_rate": 0.0009029490616621984,
      "loss": 0.1943,
      "step": 181
    },
    {
      "epoch": 0.4879356568364611,
      "grad_norm": 0.1217346340417862,
      "learning_rate": 0.0009024128686327078,
      "loss": 0.2021,
      "step": 182
    },
    {
      "epoch": 0.4906166219839142,
      "grad_norm": 0.1199263259768486,
      "learning_rate": 0.0009018766756032172,
      "loss": 0.2129,
      "step": 183
    },
    {
      "epoch": 0.4932975871313673,
      "grad_norm": 0.11809111386537552,
      "learning_rate": 0.0009013404825737266,
      "loss": 0.1875,
      "step": 184
    },
    {
      "epoch": 0.4959785522788204,
      "grad_norm": 0.0808391124010086,
      "learning_rate": 0.000900804289544236,
      "loss": 0.1572,
      "step": 185
    },
    {
      "epoch": 0.49865951742627346,
      "grad_norm": 0.07058607041835785,
      "learning_rate": 0.0009002680965147453,
      "loss": 0.1611,
      "step": 186
    },
    {
      "epoch": 0.5013404825737265,
      "grad_norm": 0.05178661644458771,
      "learning_rate": 0.0008997319034852547,
      "loss": 0.1758,
      "step": 187
    },
    {
      "epoch": 0.5040214477211796,
      "grad_norm": 0.05557653680443764,
      "learning_rate": 0.0008991957104557641,
      "loss": 0.1689,
      "step": 188
    },
    {
      "epoch": 0.5067024128686327,
      "grad_norm": 0.3273487389087677,
      "learning_rate": 0.0008986595174262735,
      "loss": 0.2178,
      "step": 189
    },
    {
      "epoch": 0.5093833780160858,
      "grad_norm": 0.045605387538671494,
      "learning_rate": 0.0008981233243967829,
      "loss": 0.1895,
      "step": 190
    },
    {
      "epoch": 0.5120643431635389,
      "grad_norm": 0.04935331642627716,
      "learning_rate": 0.0008975871313672922,
      "loss": 0.1729,
      "step": 191
    },
    {
      "epoch": 0.514745308310992,
      "grad_norm": 0.08618231862783432,
      "learning_rate": 0.0008970509383378016,
      "loss": 0.2012,
      "step": 192
    },
    {
      "epoch": 0.517426273458445,
      "grad_norm": 0.08344287425279617,
      "learning_rate": 0.000896514745308311,
      "loss": 0.1758,
      "step": 193
    },
    {
      "epoch": 0.5201072386058981,
      "grad_norm": 0.19316883385181427,
      "learning_rate": 0.0008959785522788204,
      "loss": 0.1455,
      "step": 194
    },
    {
      "epoch": 0.5227882037533512,
      "grad_norm": 0.09006964415311813,
      "learning_rate": 0.0008954423592493298,
      "loss": 0.1807,
      "step": 195
    },
    {
      "epoch": 0.5254691689008043,
      "grad_norm": 0.053801920264959335,
      "learning_rate": 0.0008949061662198391,
      "loss": 0.1797,
      "step": 196
    },
    {
      "epoch": 0.5281501340482574,
      "grad_norm": 0.08190446346998215,
      "learning_rate": 0.0008943699731903485,
      "loss": 0.1748,
      "step": 197
    },
    {
      "epoch": 0.5308310991957105,
      "grad_norm": 0.09411688148975372,
      "learning_rate": 0.0008938337801608579,
      "loss": 0.1787,
      "step": 198
    },
    {
      "epoch": 0.5335120643431636,
      "grad_norm": 0.1295836716890335,
      "learning_rate": 0.0008932975871313673,
      "loss": 0.1934,
      "step": 199
    },
    {
      "epoch": 0.5361930294906166,
      "grad_norm": 0.08562759310007095,
      "learning_rate": 0.0008927613941018767,
      "loss": 0.1709,
      "step": 200
    },
    {
      "epoch": 0.5388739946380697,
      "grad_norm": 0.20388098061084747,
      "learning_rate": 0.000892225201072386,
      "loss": 0.1621,
      "step": 201
    },
    {
      "epoch": 0.5415549597855228,
      "grad_norm": 0.10955978184938431,
      "learning_rate": 0.0008916890080428954,
      "loss": 0.1826,
      "step": 202
    },
    {
      "epoch": 0.5442359249329759,
      "grad_norm": 0.0738179013133049,
      "learning_rate": 0.0008911528150134048,
      "loss": 0.1865,
      "step": 203
    },
    {
      "epoch": 0.546916890080429,
      "grad_norm": 0.061613500118255615,
      "learning_rate": 0.0008906166219839142,
      "loss": 0.2031,
      "step": 204
    },
    {
      "epoch": 0.5495978552278821,
      "grad_norm": 0.06783407926559448,
      "learning_rate": 0.0008900804289544236,
      "loss": 0.1494,
      "step": 205
    },
    {
      "epoch": 0.5522788203753352,
      "grad_norm": 0.10419407486915588,
      "learning_rate": 0.0008895442359249329,
      "loss": 0.1973,
      "step": 206
    },
    {
      "epoch": 0.5549597855227882,
      "grad_norm": 0.07235944271087646,
      "learning_rate": 0.0008890080428954423,
      "loss": 0.1602,
      "step": 207
    },
    {
      "epoch": 0.5576407506702413,
      "grad_norm": 0.13933910429477692,
      "learning_rate": 0.0008884718498659518,
      "loss": 0.1533,
      "step": 208
    },
    {
      "epoch": 0.5603217158176944,
      "grad_norm": 0.11489327251911163,
      "learning_rate": 0.0008879356568364612,
      "loss": 0.1592,
      "step": 209
    },
    {
      "epoch": 0.5630026809651475,
      "grad_norm": 0.044777993112802505,
      "learning_rate": 0.0008873994638069706,
      "loss": 0.1533,
      "step": 210
    },
    {
      "epoch": 0.5656836461126006,
      "grad_norm": 0.19592058658599854,
      "learning_rate": 0.0008868632707774799,
      "loss": 0.2236,
      "step": 211
    },
    {
      "epoch": 0.5683646112600537,
      "grad_norm": 0.05544446036219597,
      "learning_rate": 0.0008863270777479893,
      "loss": 0.1641,
      "step": 212
    },
    {
      "epoch": 0.5710455764075067,
      "grad_norm": 0.043395090848207474,
      "learning_rate": 0.0008857908847184987,
      "loss": 0.1445,
      "step": 213
    },
    {
      "epoch": 0.5737265415549598,
      "grad_norm": 0.0692722275853157,
      "learning_rate": 0.0008852546916890081,
      "loss": 0.1553,
      "step": 214
    },
    {
      "epoch": 0.5764075067024129,
      "grad_norm": 0.06531191617250443,
      "learning_rate": 0.0008847184986595175,
      "loss": 0.1777,
      "step": 215
    },
    {
      "epoch": 0.579088471849866,
      "grad_norm": 0.1452934741973877,
      "learning_rate": 0.0008841823056300268,
      "loss": 0.1611,
      "step": 216
    },
    {
      "epoch": 0.5817694369973191,
      "grad_norm": 0.049949754029512405,
      "learning_rate": 0.0008836461126005362,
      "loss": 0.1689,
      "step": 217
    },
    {
      "epoch": 0.5844504021447721,
      "grad_norm": 0.19114215672016144,
      "learning_rate": 0.0008831099195710456,
      "loss": 0.1729,
      "step": 218
    },
    {
      "epoch": 0.5871313672922251,
      "grad_norm": 0.06373316049575806,
      "learning_rate": 0.000882573726541555,
      "loss": 0.1797,
      "step": 219
    },
    {
      "epoch": 0.5898123324396782,
      "grad_norm": 0.11046735942363739,
      "learning_rate": 0.0008820375335120644,
      "loss": 0.1885,
      "step": 220
    },
    {
      "epoch": 0.5924932975871313,
      "grad_norm": 0.0927487462759018,
      "learning_rate": 0.0008815013404825738,
      "loss": 0.1797,
      "step": 221
    },
    {
      "epoch": 0.5951742627345844,
      "grad_norm": 0.07419651746749878,
      "learning_rate": 0.0008809651474530831,
      "loss": 0.1514,
      "step": 222
    },
    {
      "epoch": 0.5978552278820375,
      "grad_norm": 0.09310788661241531,
      "learning_rate": 0.0008804289544235925,
      "loss": 0.1621,
      "step": 223
    },
    {
      "epoch": 0.6005361930294906,
      "grad_norm": 0.1004592627286911,
      "learning_rate": 0.0008798927613941019,
      "loss": 0.1748,
      "step": 224
    },
    {
      "epoch": 0.6032171581769437,
      "grad_norm": 0.10664056241512299,
      "learning_rate": 0.0008793565683646113,
      "loss": 0.1455,
      "step": 225
    },
    {
      "epoch": 0.6058981233243967,
      "grad_norm": 0.05210558697581291,
      "learning_rate": 0.0008788203753351207,
      "loss": 0.1426,
      "step": 226
    },
    {
      "epoch": 0.6085790884718498,
      "grad_norm": 0.1263301819562912,
      "learning_rate": 0.00087828418230563,
      "loss": 0.1699,
      "step": 227
    },
    {
      "epoch": 0.6112600536193029,
      "grad_norm": 0.06279142200946808,
      "learning_rate": 0.0008777479892761394,
      "loss": 0.1582,
      "step": 228
    },
    {
      "epoch": 0.613941018766756,
      "grad_norm": 0.054576706141233444,
      "learning_rate": 0.0008772117962466488,
      "loss": 0.1533,
      "step": 229
    },
    {
      "epoch": 0.6166219839142091,
      "grad_norm": 0.06367652118206024,
      "learning_rate": 0.0008766756032171582,
      "loss": 0.1602,
      "step": 230
    },
    {
      "epoch": 0.6193029490616622,
      "grad_norm": 0.07885829359292984,
      "learning_rate": 0.0008761394101876676,
      "loss": 0.1494,
      "step": 231
    },
    {
      "epoch": 0.6219839142091153,
      "grad_norm": 0.06232517212629318,
      "learning_rate": 0.0008756032171581769,
      "loss": 0.1592,
      "step": 232
    },
    {
      "epoch": 0.6246648793565683,
      "grad_norm": 0.049303747713565826,
      "learning_rate": 0.0008750670241286863,
      "loss": 0.167,
      "step": 233
    },
    {
      "epoch": 0.6273458445040214,
      "grad_norm": 0.06622445583343506,
      "learning_rate": 0.0008745308310991957,
      "loss": 0.1426,
      "step": 234
    },
    {
      "epoch": 0.6300268096514745,
      "grad_norm": 0.05847148224711418,
      "learning_rate": 0.0008739946380697052,
      "loss": 0.167,
      "step": 235
    },
    {
      "epoch": 0.6327077747989276,
      "grad_norm": 0.06749963760375977,
      "learning_rate": 0.0008734584450402146,
      "loss": 0.1514,
      "step": 236
    },
    {
      "epoch": 0.6353887399463807,
      "grad_norm": 0.07203942537307739,
      "learning_rate": 0.0008729222520107239,
      "loss": 0.1523,
      "step": 237
    },
    {
      "epoch": 0.6380697050938338,
      "grad_norm": 0.05947582423686981,
      "learning_rate": 0.0008723860589812333,
      "loss": 0.1885,
      "step": 238
    },
    {
      "epoch": 0.6407506702412868,
      "grad_norm": 0.0655047819018364,
      "learning_rate": 0.0008718498659517427,
      "loss": 0.1445,
      "step": 239
    },
    {
      "epoch": 0.6434316353887399,
      "grad_norm": 0.06399145722389221,
      "learning_rate": 0.0008713136729222521,
      "loss": 0.1504,
      "step": 240
    },
    {
      "epoch": 0.646112600536193,
      "grad_norm": 0.06266233325004578,
      "learning_rate": 0.0008707774798927615,
      "loss": 0.1787,
      "step": 241
    },
    {
      "epoch": 0.6487935656836461,
      "grad_norm": 0.051591433584690094,
      "learning_rate": 0.0008702412868632708,
      "loss": 0.1621,
      "step": 242
    },
    {
      "epoch": 0.6514745308310992,
      "grad_norm": 0.043494343757629395,
      "learning_rate": 0.0008697050938337802,
      "loss": 0.1367,
      "step": 243
    },
    {
      "epoch": 0.6541554959785523,
      "grad_norm": 0.08618957549333572,
      "learning_rate": 0.0008691689008042896,
      "loss": 0.1514,
      "step": 244
    },
    {
      "epoch": 0.6568364611260054,
      "grad_norm": 0.03784509748220444,
      "learning_rate": 0.000868632707774799,
      "loss": 0.1514,
      "step": 245
    },
    {
      "epoch": 0.6595174262734584,
      "grad_norm": 0.04608543962240219,
      "learning_rate": 0.0008680965147453084,
      "loss": 0.1719,
      "step": 246
    },
    {
      "epoch": 0.6621983914209115,
      "grad_norm": 0.04573862627148628,
      "learning_rate": 0.0008675603217158177,
      "loss": 0.1514,
      "step": 247
    },
    {
      "epoch": 0.6648793565683646,
      "grad_norm": 0.06628686934709549,
      "learning_rate": 0.0008670241286863271,
      "loss": 0.1387,
      "step": 248
    },
    {
      "epoch": 0.6675603217158177,
      "grad_norm": 0.42227041721343994,
      "learning_rate": 0.0008664879356568365,
      "loss": 0.1611,
      "step": 249
    },
    {
      "epoch": 0.6702412868632708,
      "grad_norm": 0.06363939493894577,
      "learning_rate": 0.0008659517426273459,
      "loss": 0.1543,
      "step": 250
    },
    {
      "epoch": 0.6729222520107239,
      "grad_norm": 0.1291762739419937,
      "learning_rate": 0.0008654155495978553,
      "loss": 0.1592,
      "step": 251
    },
    {
      "epoch": 0.675603217158177,
      "grad_norm": 0.08465231955051422,
      "learning_rate": 0.0008648793565683646,
      "loss": 0.1289,
      "step": 252
    },
    {
      "epoch": 0.67828418230563,
      "grad_norm": 0.07102254033088684,
      "learning_rate": 0.000864343163538874,
      "loss": 0.1245,
      "step": 253
    },
    {
      "epoch": 0.6809651474530831,
      "grad_norm": 0.04320219159126282,
      "learning_rate": 0.0008638069705093834,
      "loss": 0.1572,
      "step": 254
    },
    {
      "epoch": 0.6836461126005362,
      "grad_norm": 0.06793917715549469,
      "learning_rate": 0.0008632707774798928,
      "loss": 0.1494,
      "step": 255
    },
    {
      "epoch": 0.6863270777479893,
      "grad_norm": 0.0851428210735321,
      "learning_rate": 0.0008627345844504022,
      "loss": 0.1396,
      "step": 256
    },
    {
      "epoch": 0.6890080428954424,
      "grad_norm": 0.08463411778211594,
      "learning_rate": 0.0008621983914209115,
      "loss": 0.1475,
      "step": 257
    },
    {
      "epoch": 0.6916890080428955,
      "grad_norm": 0.060161739587783813,
      "learning_rate": 0.0008616621983914209,
      "loss": 0.1396,
      "step": 258
    },
    {
      "epoch": 0.6943699731903485,
      "grad_norm": 0.2213987112045288,
      "learning_rate": 0.0008611260053619303,
      "loss": 0.1641,
      "step": 259
    },
    {
      "epoch": 0.6970509383378016,
      "grad_norm": 0.10599017888307571,
      "learning_rate": 0.0008605898123324397,
      "loss": 0.165,
      "step": 260
    },
    {
      "epoch": 0.6997319034852547,
      "grad_norm": 0.062436603009700775,
      "learning_rate": 0.0008600536193029491,
      "loss": 0.1147,
      "step": 261
    },
    {
      "epoch": 0.7024128686327078,
      "grad_norm": 0.06163270026445389,
      "learning_rate": 0.0008595174262734584,
      "loss": 0.1348,
      "step": 262
    },
    {
      "epoch": 0.7050938337801609,
      "grad_norm": 0.0510433204472065,
      "learning_rate": 0.0008589812332439678,
      "loss": 0.1475,
      "step": 263
    },
    {
      "epoch": 0.707774798927614,
      "grad_norm": 0.08835092186927795,
      "learning_rate": 0.0008584450402144773,
      "loss": 0.1543,
      "step": 264
    },
    {
      "epoch": 0.710455764075067,
      "grad_norm": 0.05050614848732948,
      "learning_rate": 0.0008579088471849867,
      "loss": 0.1533,
      "step": 265
    },
    {
      "epoch": 0.7131367292225201,
      "grad_norm": 0.06976881623268127,
      "learning_rate": 0.0008573726541554961,
      "loss": 0.1504,
      "step": 266
    },
    {
      "epoch": 0.7158176943699732,
      "grad_norm": 0.05639524757862091,
      "learning_rate": 0.0008568364611260054,
      "loss": 0.1455,
      "step": 267
    },
    {
      "epoch": 0.7184986595174263,
      "grad_norm": 0.08397939056158066,
      "learning_rate": 0.0008563002680965148,
      "loss": 0.1553,
      "step": 268
    },
    {
      "epoch": 0.7211796246648794,
      "grad_norm": 0.052056312561035156,
      "learning_rate": 0.0008557640750670242,
      "loss": 0.1592,
      "step": 269
    },
    {
      "epoch": 0.7238605898123325,
      "grad_norm": 0.09511702507734299,
      "learning_rate": 0.0008552278820375336,
      "loss": 0.1357,
      "step": 270
    },
    {
      "epoch": 0.7265415549597856,
      "grad_norm": 0.09636112302541733,
      "learning_rate": 0.000854691689008043,
      "loss": 0.1396,
      "step": 271
    },
    {
      "epoch": 0.7292225201072386,
      "grad_norm": 0.04150195047259331,
      "learning_rate": 0.0008541554959785523,
      "loss": 0.1621,
      "step": 272
    },
    {
      "epoch": 0.7319034852546917,
      "grad_norm": 0.05346078798174858,
      "learning_rate": 0.0008536193029490617,
      "loss": 0.1543,
      "step": 273
    },
    {
      "epoch": 0.7345844504021448,
      "grad_norm": 0.05552854761481285,
      "learning_rate": 0.0008530831099195711,
      "loss": 0.1445,
      "step": 274
    },
    {
      "epoch": 0.7372654155495979,
      "grad_norm": 0.062398068606853485,
      "learning_rate": 0.0008525469168900805,
      "loss": 0.1338,
      "step": 275
    },
    {
      "epoch": 0.739946380697051,
      "grad_norm": 0.03844363987445831,
      "learning_rate": 0.0008520107238605899,
      "loss": 0.1475,
      "step": 276
    },
    {
      "epoch": 0.7426273458445041,
      "grad_norm": 0.052530135959386826,
      "learning_rate": 0.0008514745308310992,
      "loss": 0.1338,
      "step": 277
    },
    {
      "epoch": 0.7453083109919572,
      "grad_norm": 0.041506487876176834,
      "learning_rate": 0.0008509383378016086,
      "loss": 0.1187,
      "step": 278
    },
    {
      "epoch": 0.7479892761394102,
      "grad_norm": 0.08080233633518219,
      "learning_rate": 0.000850402144772118,
      "loss": 0.1177,
      "step": 279
    },
    {
      "epoch": 0.7506702412868632,
      "grad_norm": 0.0633867159485817,
      "learning_rate": 0.0008498659517426274,
      "loss": 0.1309,
      "step": 280
    },
    {
      "epoch": 0.7533512064343163,
      "grad_norm": 0.08870905637741089,
      "learning_rate": 0.0008493297587131368,
      "loss": 0.127,
      "step": 281
    },
    {
      "epoch": 0.7560321715817694,
      "grad_norm": 0.03916940093040466,
      "learning_rate": 0.0008487935656836462,
      "loss": 0.1279,
      "step": 282
    },
    {
      "epoch": 0.7587131367292225,
      "grad_norm": 0.10012105107307434,
      "learning_rate": 0.0008482573726541555,
      "loss": 0.1226,
      "step": 283
    },
    {
      "epoch": 0.7613941018766756,
      "grad_norm": 0.046535003930330276,
      "learning_rate": 0.0008477211796246649,
      "loss": 0.127,
      "step": 284
    },
    {
      "epoch": 0.7640750670241286,
      "grad_norm": 0.111423060297966,
      "learning_rate": 0.0008471849865951743,
      "loss": 0.1196,
      "step": 285
    },
    {
      "epoch": 0.7667560321715817,
      "grad_norm": 0.08397562056779861,
      "learning_rate": 0.0008466487935656837,
      "loss": 0.125,
      "step": 286
    },
    {
      "epoch": 0.7694369973190348,
      "grad_norm": 0.044796377420425415,
      "learning_rate": 0.0008461126005361931,
      "loss": 0.1216,
      "step": 287
    },
    {
      "epoch": 0.7721179624664879,
      "grad_norm": 0.12027771025896072,
      "learning_rate": 0.0008455764075067024,
      "loss": 0.1191,
      "step": 288
    },
    {
      "epoch": 0.774798927613941,
      "grad_norm": 0.10016563534736633,
      "learning_rate": 0.0008450402144772118,
      "loss": 0.124,
      "step": 289
    },
    {
      "epoch": 0.7774798927613941,
      "grad_norm": 0.42838335037231445,
      "learning_rate": 0.0008445040214477212,
      "loss": 0.1729,
      "step": 290
    },
    {
      "epoch": 0.7801608579088471,
      "grad_norm": 0.052443262189626694,
      "learning_rate": 0.0008439678284182307,
      "loss": 0.125,
      "step": 291
    },
    {
      "epoch": 0.7828418230563002,
      "grad_norm": 0.08069542795419693,
      "learning_rate": 0.0008434316353887401,
      "loss": 0.1504,
      "step": 292
    },
    {
      "epoch": 0.7855227882037533,
      "grad_norm": 0.03764799237251282,
      "learning_rate": 0.0008428954423592494,
      "loss": 0.1279,
      "step": 293
    },
    {
      "epoch": 0.7882037533512064,
      "grad_norm": 0.1475696861743927,
      "learning_rate": 0.0008423592493297588,
      "loss": 0.1445,
      "step": 294
    },
    {
      "epoch": 0.7908847184986595,
      "grad_norm": 0.07482592761516571,
      "learning_rate": 0.0008418230563002682,
      "loss": 0.1318,
      "step": 295
    },
    {
      "epoch": 0.7935656836461126,
      "grad_norm": 0.06922169774770737,
      "learning_rate": 0.0008412868632707776,
      "loss": 0.1123,
      "step": 296
    },
    {
      "epoch": 0.7962466487935657,
      "grad_norm": 0.05724015459418297,
      "learning_rate": 0.000840750670241287,
      "loss": 0.1211,
      "step": 297
    },
    {
      "epoch": 0.7989276139410187,
      "grad_norm": 0.05015747249126434,
      "learning_rate": 0.0008402144772117963,
      "loss": 0.1436,
      "step": 298
    },
    {
      "epoch": 0.8016085790884718,
      "grad_norm": 0.08031390607357025,
      "learning_rate": 0.0008396782841823057,
      "loss": 0.1328,
      "step": 299
    },
    {
      "epoch": 0.8042895442359249,
      "grad_norm": 0.03802446275949478,
      "learning_rate": 0.0008391420911528151,
      "loss": 0.1216,
      "step": 300
    },
    {
      "epoch": 0.806970509383378,
      "grad_norm": 0.1009891927242279,
      "learning_rate": 0.0008386058981233245,
      "loss": 0.1338,
      "step": 301
    },
    {
      "epoch": 0.8096514745308311,
      "grad_norm": 0.055782414972782135,
      "learning_rate": 0.0008380697050938339,
      "loss": 0.1533,
      "step": 302
    },
    {
      "epoch": 0.8123324396782842,
      "grad_norm": 0.0549689382314682,
      "learning_rate": 0.0008375335120643432,
      "loss": 0.1211,
      "step": 303
    },
    {
      "epoch": 0.8150134048257373,
      "grad_norm": 0.05988747254014015,
      "learning_rate": 0.0008369973190348526,
      "loss": 0.1201,
      "step": 304
    },
    {
      "epoch": 0.8176943699731903,
      "grad_norm": 0.05380743369460106,
      "learning_rate": 0.000836461126005362,
      "loss": 0.1533,
      "step": 305
    },
    {
      "epoch": 0.8203753351206434,
      "grad_norm": 0.06101953983306885,
      "learning_rate": 0.0008359249329758714,
      "loss": 0.1196,
      "step": 306
    },
    {
      "epoch": 0.8230563002680965,
      "grad_norm": 0.0857744961977005,
      "learning_rate": 0.0008353887399463808,
      "loss": 0.1436,
      "step": 307
    },
    {
      "epoch": 0.8257372654155496,
      "grad_norm": 0.04452688992023468,
      "learning_rate": 0.0008348525469168901,
      "loss": 0.127,
      "step": 308
    },
    {
      "epoch": 0.8284182305630027,
      "grad_norm": 0.0668758824467659,
      "learning_rate": 0.0008343163538873995,
      "loss": 0.1377,
      "step": 309
    },
    {
      "epoch": 0.8310991957104558,
      "grad_norm": 0.05803262069821358,
      "learning_rate": 0.0008337801608579089,
      "loss": 0.1465,
      "step": 310
    },
    {
      "epoch": 0.8337801608579088,
      "grad_norm": 0.14284424483776093,
      "learning_rate": 0.0008332439678284183,
      "loss": 0.1367,
      "step": 311
    },
    {
      "epoch": 0.8364611260053619,
      "grad_norm": 0.043850041925907135,
      "learning_rate": 0.0008327077747989277,
      "loss": 0.127,
      "step": 312
    },
    {
      "epoch": 0.839142091152815,
      "grad_norm": 0.05412169173359871,
      "learning_rate": 0.0008321715817694369,
      "loss": 0.1211,
      "step": 313
    },
    {
      "epoch": 0.8418230563002681,
      "grad_norm": 0.05096200853586197,
      "learning_rate": 0.0008316353887399463,
      "loss": 0.1113,
      "step": 314
    },
    {
      "epoch": 0.8445040214477212,
      "grad_norm": 0.043587151914834976,
      "learning_rate": 0.0008310991957104557,
      "loss": 0.1484,
      "step": 315
    },
    {
      "epoch": 0.8471849865951743,
      "grad_norm": 0.06496094167232513,
      "learning_rate": 0.0008305630026809651,
      "loss": 0.105,
      "step": 316
    },
    {
      "epoch": 0.8498659517426274,
      "grad_norm": 0.05088473856449127,
      "learning_rate": 0.0008300268096514745,
      "loss": 0.1338,
      "step": 317
    },
    {
      "epoch": 0.8525469168900804,
      "grad_norm": 0.03873397037386894,
      "learning_rate": 0.0008294906166219838,
      "loss": 0.126,
      "step": 318
    },
    {
      "epoch": 0.8552278820375335,
      "grad_norm": 0.044868286699056625,
      "learning_rate": 0.0008289544235924932,
      "loss": 0.1152,
      "step": 319
    },
    {
      "epoch": 0.8579088471849866,
      "grad_norm": 0.05869198963046074,
      "learning_rate": 0.0008284182305630026,
      "loss": 0.1152,
      "step": 320
    },
    {
      "epoch": 0.8605898123324397,
      "grad_norm": 0.040986642241477966,
      "learning_rate": 0.000827882037533512,
      "loss": 0.1426,
      "step": 321
    },
    {
      "epoch": 0.8632707774798928,
      "grad_norm": 0.05536036565899849,
      "learning_rate": 0.0008273458445040215,
      "loss": 0.1299,
      "step": 322
    },
    {
      "epoch": 0.8659517426273459,
      "grad_norm": 0.04869738593697548,
      "learning_rate": 0.0008268096514745308,
      "loss": 0.1328,
      "step": 323
    },
    {
      "epoch": 0.868632707774799,
      "grad_norm": 0.052299272269010544,
      "learning_rate": 0.0008262734584450402,
      "loss": 0.1226,
      "step": 324
    },
    {
      "epoch": 0.871313672922252,
      "grad_norm": 0.060558613389730453,
      "learning_rate": 0.0008257372654155496,
      "loss": 0.1108,
      "step": 325
    },
    {
      "epoch": 0.8739946380697051,
      "grad_norm": 0.04293038323521614,
      "learning_rate": 0.000825201072386059,
      "loss": 0.0933,
      "step": 326
    },
    {
      "epoch": 0.8766756032171582,
      "grad_norm": 0.07904618978500366,
      "learning_rate": 0.0008246648793565684,
      "loss": 0.1338,
      "step": 327
    },
    {
      "epoch": 0.8793565683646113,
      "grad_norm": 0.06765228509902954,
      "learning_rate": 0.0008241286863270777,
      "loss": 0.1504,
      "step": 328
    },
    {
      "epoch": 0.8820375335120644,
      "grad_norm": 0.08700711280107498,
      "learning_rate": 0.0008235924932975871,
      "loss": 0.1445,
      "step": 329
    },
    {
      "epoch": 0.8847184986595175,
      "grad_norm": 0.12562793493270874,
      "learning_rate": 0.0008230563002680965,
      "loss": 0.1123,
      "step": 330
    },
    {
      "epoch": 0.8873994638069705,
      "grad_norm": 0.044028472155332565,
      "learning_rate": 0.0008225201072386059,
      "loss": 0.1094,
      "step": 331
    },
    {
      "epoch": 0.8900804289544236,
      "grad_norm": 0.039789676666259766,
      "learning_rate": 0.0008219839142091153,
      "loss": 0.1206,
      "step": 332
    },
    {
      "epoch": 0.8927613941018767,
      "grad_norm": 0.05778714641928673,
      "learning_rate": 0.0008214477211796246,
      "loss": 0.1108,
      "step": 333
    },
    {
      "epoch": 0.8954423592493298,
      "grad_norm": 0.03794701024889946,
      "learning_rate": 0.000820911528150134,
      "loss": 0.1182,
      "step": 334
    },
    {
      "epoch": 0.8981233243967829,
      "grad_norm": 0.039037540555000305,
      "learning_rate": 0.0008203753351206434,
      "loss": 0.1104,
      "step": 335
    },
    {
      "epoch": 0.900804289544236,
      "grad_norm": 0.06138968467712402,
      "learning_rate": 0.0008198391420911528,
      "loss": 0.125,
      "step": 336
    },
    {
      "epoch": 0.903485254691689,
      "grad_norm": 0.0952741801738739,
      "learning_rate": 0.0008193029490616622,
      "loss": 0.1133,
      "step": 337
    },
    {
      "epoch": 0.9061662198391421,
      "grad_norm": 0.0911797434091568,
      "learning_rate": 0.0008187667560321715,
      "loss": 0.1216,
      "step": 338
    },
    {
      "epoch": 0.9088471849865952,
      "grad_norm": 0.04408690705895424,
      "learning_rate": 0.0008182305630026809,
      "loss": 0.1416,
      "step": 339
    },
    {
      "epoch": 0.9115281501340483,
      "grad_norm": 0.04354093596339226,
      "learning_rate": 0.0008176943699731903,
      "loss": 0.1328,
      "step": 340
    },
    {
      "epoch": 0.9142091152815014,
      "grad_norm": 0.04691474512219429,
      "learning_rate": 0.0008171581769436997,
      "loss": 0.1191,
      "step": 341
    },
    {
      "epoch": 0.9168900804289544,
      "grad_norm": 0.03107368014752865,
      "learning_rate": 0.0008166219839142091,
      "loss": 0.1089,
      "step": 342
    },
    {
      "epoch": 0.9195710455764075,
      "grad_norm": 0.07571236044168472,
      "learning_rate": 0.0008160857908847185,
      "loss": 0.1328,
      "step": 343
    },
    {
      "epoch": 0.9222520107238605,
      "grad_norm": 0.06326805800199509,
      "learning_rate": 0.0008155495978552278,
      "loss": 0.125,
      "step": 344
    },
    {
      "epoch": 0.9249329758713136,
      "grad_norm": 0.03810399770736694,
      "learning_rate": 0.0008150134048257372,
      "loss": 0.1289,
      "step": 345
    },
    {
      "epoch": 0.9276139410187667,
      "grad_norm": 0.04458208382129669,
      "learning_rate": 0.0008144772117962466,
      "loss": 0.126,
      "step": 346
    },
    {
      "epoch": 0.9302949061662198,
      "grad_norm": 0.04252784326672554,
      "learning_rate": 0.000813941018766756,
      "loss": 0.1089,
      "step": 347
    },
    {
      "epoch": 0.9329758713136729,
      "grad_norm": 0.04729359596967697,
      "learning_rate": 0.0008134048257372654,
      "loss": 0.1055,
      "step": 348
    },
    {
      "epoch": 0.935656836461126,
      "grad_norm": 0.07467181235551834,
      "learning_rate": 0.0008128686327077747,
      "loss": 0.1318,
      "step": 349
    },
    {
      "epoch": 0.938337801608579,
      "grad_norm": 0.03208956494927406,
      "learning_rate": 0.0008123324396782842,
      "loss": 0.1216,
      "step": 350
    },
    {
      "epoch": 0.9410187667560321,
      "grad_norm": 0.03872699663043022,
      "learning_rate": 0.0008117962466487936,
      "loss": 0.1118,
      "step": 351
    },
    {
      "epoch": 0.9436997319034852,
      "grad_norm": 0.11080144345760345,
      "learning_rate": 0.000811260053619303,
      "loss": 0.1514,
      "step": 352
    },
    {
      "epoch": 0.9463806970509383,
      "grad_norm": 0.050729699432849884,
      "learning_rate": 0.0008107238605898124,
      "loss": 0.1182,
      "step": 353
    },
    {
      "epoch": 0.9490616621983914,
      "grad_norm": 0.03472788259387016,
      "learning_rate": 0.0008101876675603217,
      "loss": 0.125,
      "step": 354
    },
    {
      "epoch": 0.9517426273458445,
      "grad_norm": 0.03542723506689072,
      "learning_rate": 0.0008096514745308311,
      "loss": 0.0947,
      "step": 355
    },
    {
      "epoch": 0.9544235924932976,
      "grad_norm": 0.03658240661025047,
      "learning_rate": 0.0008091152815013405,
      "loss": 0.1299,
      "step": 356
    },
    {
      "epoch": 0.9571045576407506,
      "grad_norm": 0.042796600610017776,
      "learning_rate": 0.0008085790884718499,
      "loss": 0.1167,
      "step": 357
    },
    {
      "epoch": 0.9597855227882037,
      "grad_norm": 0.045325081795454025,
      "learning_rate": 0.0008080428954423593,
      "loss": 0.1289,
      "step": 358
    },
    {
      "epoch": 0.9624664879356568,
      "grad_norm": 0.04898901283740997,
      "learning_rate": 0.0008075067024128686,
      "loss": 0.1226,
      "step": 359
    },
    {
      "epoch": 0.9651474530831099,
      "grad_norm": 0.05655749887228012,
      "learning_rate": 0.000806970509383378,
      "loss": 0.0986,
      "step": 360
    },
    {
      "epoch": 0.967828418230563,
      "grad_norm": 0.11963232606649399,
      "learning_rate": 0.0008064343163538874,
      "loss": 0.1396,
      "step": 361
    },
    {
      "epoch": 0.9705093833780161,
      "grad_norm": 0.08576340228319168,
      "learning_rate": 0.0008058981233243968,
      "loss": 0.1475,
      "step": 362
    },
    {
      "epoch": 0.9731903485254692,
      "grad_norm": 0.19159342348575592,
      "learning_rate": 0.0008053619302949062,
      "loss": 0.1035,
      "step": 363
    },
    {
      "epoch": 0.9758713136729222,
      "grad_norm": 0.08273222297430038,
      "learning_rate": 0.0008048257372654155,
      "loss": 0.1177,
      "step": 364
    },
    {
      "epoch": 0.9785522788203753,
      "grad_norm": 0.06683661788702011,
      "learning_rate": 0.0008042895442359249,
      "loss": 0.1113,
      "step": 365
    },
    {
      "epoch": 0.9812332439678284,
      "grad_norm": 0.05177105590701103,
      "learning_rate": 0.0008037533512064343,
      "loss": 0.1289,
      "step": 366
    },
    {
      "epoch": 0.9839142091152815,
      "grad_norm": 0.10126637667417526,
      "learning_rate": 0.0008032171581769437,
      "loss": 0.1309,
      "step": 367
    },
    {
      "epoch": 0.9865951742627346,
      "grad_norm": 0.039314959198236465,
      "learning_rate": 0.0008026809651474531,
      "loss": 0.126,
      "step": 368
    },
    {
      "epoch": 0.9892761394101877,
      "grad_norm": 0.05990242585539818,
      "learning_rate": 0.0008021447721179624,
      "loss": 0.1069,
      "step": 369
    },
    {
      "epoch": 0.9919571045576407,
      "grad_norm": 0.06147317588329315,
      "learning_rate": 0.0008016085790884718,
      "loss": 0.1157,
      "step": 370
    },
    {
      "epoch": 0.9946380697050938,
      "grad_norm": 0.07681682705879211,
      "learning_rate": 0.0008010723860589812,
      "loss": 0.1113,
      "step": 371
    },
    {
      "epoch": 0.9973190348525469,
      "grad_norm": 0.11717309802770615,
      "learning_rate": 0.0008005361930294906,
      "loss": 0.126,
      "step": 372
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.0514214001595974,
      "learning_rate": 0.0008,
      "loss": 0.1245,
      "step": 373
    },
    {
      "epoch": 1.002680965147453,
      "grad_norm": 0.09134218841791153,
      "learning_rate": 0.0007994638069705093,
      "loss": 0.126,
      "step": 374
    },
    {
      "epoch": 1.0053619302949062,
      "grad_norm": 0.04214397445321083,
      "learning_rate": 0.0007989276139410187,
      "loss": 0.1089,
      "step": 375
    },
    {
      "epoch": 1.0080428954423593,
      "grad_norm": 0.046443305909633636,
      "learning_rate": 0.0007983914209115281,
      "loss": 0.1348,
      "step": 376
    },
    {
      "epoch": 1.0107238605898123,
      "grad_norm": 0.053925734013319016,
      "learning_rate": 0.0007978552278820376,
      "loss": 0.106,
      "step": 377
    },
    {
      "epoch": 1.0134048257372654,
      "grad_norm": 0.04247036948800087,
      "learning_rate": 0.000797319034852547,
      "loss": 0.1318,
      "step": 378
    },
    {
      "epoch": 1.0160857908847185,
      "grad_norm": 0.04120238497853279,
      "learning_rate": 0.0007967828418230563,
      "loss": 0.1128,
      "step": 379
    },
    {
      "epoch": 1.0187667560321716,
      "grad_norm": 0.04126602038741112,
      "learning_rate": 0.0007962466487935657,
      "loss": 0.1377,
      "step": 380
    },
    {
      "epoch": 1.0214477211796247,
      "grad_norm": 0.03885405510663986,
      "learning_rate": 0.0007957104557640751,
      "loss": 0.0952,
      "step": 381
    },
    {
      "epoch": 1.0241286863270778,
      "grad_norm": 0.04503221809864044,
      "learning_rate": 0.0007951742627345845,
      "loss": 0.1152,
      "step": 382
    },
    {
      "epoch": 1.0268096514745308,
      "grad_norm": 0.054775744676589966,
      "learning_rate": 0.0007946380697050939,
      "loss": 0.1338,
      "step": 383
    },
    {
      "epoch": 1.029490616621984,
      "grad_norm": 0.031669747084379196,
      "learning_rate": 0.0007941018766756032,
      "loss": 0.0918,
      "step": 384
    },
    {
      "epoch": 1.032171581769437,
      "grad_norm": 0.08022021502256393,
      "learning_rate": 0.0007935656836461126,
      "loss": 0.1157,
      "step": 385
    },
    {
      "epoch": 1.03485254691689,
      "grad_norm": 0.045495133846998215,
      "learning_rate": 0.000793029490616622,
      "loss": 0.1045,
      "step": 386
    },
    {
      "epoch": 1.0375335120643432,
      "grad_norm": 0.07396295666694641,
      "learning_rate": 0.0007924932975871314,
      "loss": 0.1436,
      "step": 387
    },
    {
      "epoch": 1.0402144772117963,
      "grad_norm": 0.06620173901319504,
      "learning_rate": 0.0007919571045576408,
      "loss": 0.1367,
      "step": 388
    },
    {
      "epoch": 1.0428954423592494,
      "grad_norm": 0.05925356596708298,
      "learning_rate": 0.0007914209115281501,
      "loss": 0.1099,
      "step": 389
    },
    {
      "epoch": 1.0455764075067024,
      "grad_norm": 0.049868494272232056,
      "learning_rate": 0.0007908847184986595,
      "loss": 0.125,
      "step": 390
    },
    {
      "epoch": 1.0482573726541555,
      "grad_norm": 0.036422017961740494,
      "learning_rate": 0.0007903485254691689,
      "loss": 0.1318,
      "step": 391
    },
    {
      "epoch": 1.0509383378016086,
      "grad_norm": 0.04537982866168022,
      "learning_rate": 0.0007898123324396783,
      "loss": 0.1172,
      "step": 392
    },
    {
      "epoch": 1.0536193029490617,
      "grad_norm": 0.05305970087647438,
      "learning_rate": 0.0007892761394101877,
      "loss": 0.1152,
      "step": 393
    },
    {
      "epoch": 1.0563002680965148,
      "grad_norm": 0.16633930802345276,
      "learning_rate": 0.000788739946380697,
      "loss": 0.1152,
      "step": 394
    },
    {
      "epoch": 1.0589812332439679,
      "grad_norm": 0.06072550639510155,
      "learning_rate": 0.0007882037533512064,
      "loss": 0.1152,
      "step": 395
    },
    {
      "epoch": 1.061662198391421,
      "grad_norm": 0.04000595957040787,
      "learning_rate": 0.0007876675603217158,
      "loss": 0.0933,
      "step": 396
    },
    {
      "epoch": 1.064343163538874,
      "grad_norm": 0.05180668830871582,
      "learning_rate": 0.0007871313672922252,
      "loss": 0.1377,
      "step": 397
    },
    {
      "epoch": 1.0670241286863271,
      "grad_norm": 0.04465843364596367,
      "learning_rate": 0.0007865951742627346,
      "loss": 0.1118,
      "step": 398
    },
    {
      "epoch": 1.0697050938337802,
      "grad_norm": 0.051155298948287964,
      "learning_rate": 0.0007860589812332439,
      "loss": 0.0977,
      "step": 399
    },
    {
      "epoch": 1.0723860589812333,
      "grad_norm": 0.049843739718198776,
      "learning_rate": 0.0007855227882037533,
      "loss": 0.1191,
      "step": 400
    },
    {
      "epoch": 1.0750670241286864,
      "grad_norm": 0.06626590341329575,
      "learning_rate": 0.0007849865951742627,
      "loss": 0.1108,
      "step": 401
    },
    {
      "epoch": 1.0777479892761395,
      "grad_norm": 0.040575362741947174,
      "learning_rate": 0.0007844504021447721,
      "loss": 0.1377,
      "step": 402
    },
    {
      "epoch": 1.0804289544235925,
      "grad_norm": 0.040420133620500565,
      "learning_rate": 0.0007839142091152815,
      "loss": 0.1396,
      "step": 403
    },
    {
      "epoch": 1.0831099195710456,
      "grad_norm": 0.034759487956762314,
      "learning_rate": 0.0007833780160857908,
      "loss": 0.1108,
      "step": 404
    },
    {
      "epoch": 1.0857908847184987,
      "grad_norm": 0.03712080791592598,
      "learning_rate": 0.0007828418230563002,
      "loss": 0.1045,
      "step": 405
    },
    {
      "epoch": 1.0884718498659518,
      "grad_norm": 0.06715606153011322,
      "learning_rate": 0.0007823056300268097,
      "loss": 0.0967,
      "step": 406
    },
    {
      "epoch": 1.0911528150134049,
      "grad_norm": 0.03446543961763382,
      "learning_rate": 0.0007817694369973191,
      "loss": 0.1001,
      "step": 407
    },
    {
      "epoch": 1.093833780160858,
      "grad_norm": 0.10375390201807022,
      "learning_rate": 0.0007812332439678285,
      "loss": 0.1128,
      "step": 408
    },
    {
      "epoch": 1.096514745308311,
      "grad_norm": 0.051837362349033356,
      "learning_rate": 0.0007806970509383379,
      "loss": 0.1064,
      "step": 409
    },
    {
      "epoch": 1.0991957104557641,
      "grad_norm": 0.12197627872228622,
      "learning_rate": 0.0007801608579088472,
      "loss": 0.1177,
      "step": 410
    },
    {
      "epoch": 1.1018766756032172,
      "grad_norm": 0.3301435112953186,
      "learning_rate": 0.0007796246648793566,
      "loss": 0.1157,
      "step": 411
    },
    {
      "epoch": 1.1045576407506703,
      "grad_norm": 0.06505794078111649,
      "learning_rate": 0.000779088471849866,
      "loss": 0.0903,
      "step": 412
    },
    {
      "epoch": 1.1072386058981234,
      "grad_norm": 0.04316914454102516,
      "learning_rate": 0.0007785522788203754,
      "loss": 0.1104,
      "step": 413
    },
    {
      "epoch": 1.1099195710455765,
      "grad_norm": 0.05794845148921013,
      "learning_rate": 0.0007780160857908848,
      "loss": 0.0938,
      "step": 414
    },
    {
      "epoch": 1.1126005361930296,
      "grad_norm": 0.042976390570402145,
      "learning_rate": 0.0007774798927613941,
      "loss": 0.104,
      "step": 415
    },
    {
      "epoch": 1.1152815013404827,
      "grad_norm": 0.04087655991315842,
      "learning_rate": 0.0007769436997319035,
      "loss": 0.1216,
      "step": 416
    },
    {
      "epoch": 1.1179624664879357,
      "grad_norm": 0.04101721569895744,
      "learning_rate": 0.0007764075067024129,
      "loss": 0.1104,
      "step": 417
    },
    {
      "epoch": 1.1206434316353888,
      "grad_norm": 0.0990547388792038,
      "learning_rate": 0.0007758713136729223,
      "loss": 0.1147,
      "step": 418
    },
    {
      "epoch": 1.123324396782842,
      "grad_norm": 0.06716539710760117,
      "learning_rate": 0.0007753351206434317,
      "loss": 0.0996,
      "step": 419
    },
    {
      "epoch": 1.126005361930295,
      "grad_norm": 0.08820836991071701,
      "learning_rate": 0.000774798927613941,
      "loss": 0.1387,
      "step": 420
    },
    {
      "epoch": 1.128686327077748,
      "grad_norm": 0.06045705825090408,
      "learning_rate": 0.0007742627345844504,
      "loss": 0.127,
      "step": 421
    },
    {
      "epoch": 1.1313672922252012,
      "grad_norm": 0.0752866119146347,
      "learning_rate": 0.0007737265415549598,
      "loss": 0.104,
      "step": 422
    },
    {
      "epoch": 1.1340482573726542,
      "grad_norm": 0.11385703831911087,
      "learning_rate": 0.0007731903485254692,
      "loss": 0.1128,
      "step": 423
    },
    {
      "epoch": 1.1367292225201073,
      "grad_norm": 0.04446065425872803,
      "learning_rate": 0.0007726541554959786,
      "loss": 0.124,
      "step": 424
    },
    {
      "epoch": 1.1394101876675604,
      "grad_norm": 0.10648512095212936,
      "learning_rate": 0.0007721179624664879,
      "loss": 0.1104,
      "step": 425
    },
    {
      "epoch": 1.1420911528150135,
      "grad_norm": 0.0577947162091732,
      "learning_rate": 0.0007715817694369973,
      "loss": 0.1211,
      "step": 426
    },
    {
      "epoch": 1.1447721179624666,
      "grad_norm": 0.07637406140565872,
      "learning_rate": 0.0007710455764075067,
      "loss": 0.1309,
      "step": 427
    },
    {
      "epoch": 1.1474530831099194,
      "grad_norm": 0.13297788798809052,
      "learning_rate": 0.0007705093833780161,
      "loss": 0.1094,
      "step": 428
    },
    {
      "epoch": 1.1501340482573728,
      "grad_norm": 0.03967242315411568,
      "learning_rate": 0.0007699731903485255,
      "loss": 0.1196,
      "step": 429
    },
    {
      "epoch": 1.1528150134048256,
      "grad_norm": 0.05673348531126976,
      "learning_rate": 0.0007694369973190348,
      "loss": 0.1289,
      "step": 430
    },
    {
      "epoch": 1.155495978552279,
      "grad_norm": 0.08055753260850906,
      "learning_rate": 0.0007689008042895442,
      "loss": 0.1079,
      "step": 431
    },
    {
      "epoch": 1.1581769436997318,
      "grad_norm": 0.061955470591783524,
      "learning_rate": 0.0007683646112600536,
      "loss": 0.1128,
      "step": 432
    },
    {
      "epoch": 1.160857908847185,
      "grad_norm": 0.04938064143061638,
      "learning_rate": 0.000767828418230563,
      "loss": 0.1396,
      "step": 433
    },
    {
      "epoch": 1.163538873994638,
      "grad_norm": 0.03912496194243431,
      "learning_rate": 0.0007672922252010725,
      "loss": 0.0845,
      "step": 434
    },
    {
      "epoch": 1.1662198391420913,
      "grad_norm": 0.06021473929286003,
      "learning_rate": 0.0007667560321715818,
      "loss": 0.1211,
      "step": 435
    },
    {
      "epoch": 1.1689008042895441,
      "grad_norm": 0.09824032336473465,
      "learning_rate": 0.0007662198391420912,
      "loss": 0.105,
      "step": 436
    },
    {
      "epoch": 1.1715817694369974,
      "grad_norm": 0.04695698246359825,
      "learning_rate": 0.0007656836461126006,
      "loss": 0.1074,
      "step": 437
    },
    {
      "epoch": 1.1742627345844503,
      "grad_norm": 0.03620569407939911,
      "learning_rate": 0.00076514745308311,
      "loss": 0.1025,
      "step": 438
    },
    {
      "epoch": 1.1769436997319036,
      "grad_norm": 0.048227984458208084,
      "learning_rate": 0.0007646112600536194,
      "loss": 0.0894,
      "step": 439
    },
    {
      "epoch": 1.1796246648793565,
      "grad_norm": 0.05152279511094093,
      "learning_rate": 0.0007640750670241287,
      "loss": 0.1138,
      "step": 440
    },
    {
      "epoch": 1.1823056300268098,
      "grad_norm": 0.052527353167533875,
      "learning_rate": 0.0007635388739946381,
      "loss": 0.1011,
      "step": 441
    },
    {
      "epoch": 1.1849865951742626,
      "grad_norm": 0.05657774582505226,
      "learning_rate": 0.0007630026809651475,
      "loss": 0.1147,
      "step": 442
    },
    {
      "epoch": 1.1876675603217157,
      "grad_norm": 0.06153494119644165,
      "learning_rate": 0.0007624664879356569,
      "loss": 0.1138,
      "step": 443
    },
    {
      "epoch": 1.1903485254691688,
      "grad_norm": 0.036200109869241714,
      "learning_rate": 0.0007619302949061663,
      "loss": 0.1079,
      "step": 444
    },
    {
      "epoch": 1.193029490616622,
      "grad_norm": 0.06986283510923386,
      "learning_rate": 0.0007613941018766756,
      "loss": 0.1094,
      "step": 445
    },
    {
      "epoch": 1.195710455764075,
      "grad_norm": 0.046245086938142776,
      "learning_rate": 0.000760857908847185,
      "loss": 0.0947,
      "step": 446
    },
    {
      "epoch": 1.198391420911528,
      "grad_norm": 0.03321978449821472,
      "learning_rate": 0.0007603217158176944,
      "loss": 0.1118,
      "step": 447
    },
    {
      "epoch": 1.2010723860589811,
      "grad_norm": 0.03489430621266365,
      "learning_rate": 0.0007597855227882038,
      "loss": 0.1133,
      "step": 448
    },
    {
      "epoch": 1.2037533512064342,
      "grad_norm": 0.040769703686237335,
      "learning_rate": 0.0007592493297587132,
      "loss": 0.1094,
      "step": 449
    },
    {
      "epoch": 1.2064343163538873,
      "grad_norm": 0.07447294890880585,
      "learning_rate": 0.0007587131367292225,
      "loss": 0.1367,
      "step": 450
    },
    {
      "epoch": 1.2091152815013404,
      "grad_norm": 0.035061147063970566,
      "learning_rate": 0.0007581769436997319,
      "loss": 0.1289,
      "step": 451
    },
    {
      "epoch": 1.2117962466487935,
      "grad_norm": 0.04936068132519722,
      "learning_rate": 0.0007576407506702413,
      "loss": 0.127,
      "step": 452
    },
    {
      "epoch": 1.2144772117962466,
      "grad_norm": 0.06961347162723541,
      "learning_rate": 0.0007571045576407507,
      "loss": 0.1157,
      "step": 453
    },
    {
      "epoch": 1.2171581769436997,
      "grad_norm": 0.06500349193811417,
      "learning_rate": 0.0007565683646112601,
      "loss": 0.1123,
      "step": 454
    },
    {
      "epoch": 1.2198391420911527,
      "grad_norm": 0.05681437999010086,
      "learning_rate": 0.0007560321715817694,
      "loss": 0.1064,
      "step": 455
    },
    {
      "epoch": 1.2225201072386058,
      "grad_norm": 0.042462870478630066,
      "learning_rate": 0.0007554959785522788,
      "loss": 0.1089,
      "step": 456
    },
    {
      "epoch": 1.225201072386059,
      "grad_norm": 0.031834717839956284,
      "learning_rate": 0.0007549597855227882,
      "loss": 0.0928,
      "step": 457
    },
    {
      "epoch": 1.227882037533512,
      "grad_norm": 0.058806296437978745,
      "learning_rate": 0.0007544235924932976,
      "loss": 0.0796,
      "step": 458
    },
    {
      "epoch": 1.230563002680965,
      "grad_norm": 0.11173803359270096,
      "learning_rate": 0.000753887399463807,
      "loss": 0.1089,
      "step": 459
    },
    {
      "epoch": 1.2332439678284182,
      "grad_norm": 0.05673281103372574,
      "learning_rate": 0.0007533512064343163,
      "loss": 0.1299,
      "step": 460
    },
    {
      "epoch": 1.2359249329758712,
      "grad_norm": 0.03993743285536766,
      "learning_rate": 0.0007528150134048257,
      "loss": 0.0933,
      "step": 461
    },
    {
      "epoch": 1.2386058981233243,
      "grad_norm": 0.03976181149482727,
      "learning_rate": 0.0007522788203753352,
      "loss": 0.1113,
      "step": 462
    },
    {
      "epoch": 1.2412868632707774,
      "grad_norm": 0.060153573751449585,
      "learning_rate": 0.0007517426273458446,
      "loss": 0.1084,
      "step": 463
    },
    {
      "epoch": 1.2439678284182305,
      "grad_norm": 0.0652385801076889,
      "learning_rate": 0.000751206434316354,
      "loss": 0.0962,
      "step": 464
    },
    {
      "epoch": 1.2466487935656836,
      "grad_norm": 0.04295577481389046,
      "learning_rate": 0.0007506702412868633,
      "loss": 0.0957,
      "step": 465
    },
    {
      "epoch": 1.2493297587131367,
      "grad_norm": 0.03654337674379349,
      "learning_rate": 0.0007501340482573727,
      "loss": 0.1064,
      "step": 466
    },
    {
      "epoch": 1.2520107238605898,
      "grad_norm": 0.03914329409599304,
      "learning_rate": 0.0007495978552278821,
      "loss": 0.1011,
      "step": 467
    },
    {
      "epoch": 1.2546916890080428,
      "grad_norm": 0.055491309612989426,
      "learning_rate": 0.0007490616621983915,
      "loss": 0.0991,
      "step": 468
    },
    {
      "epoch": 1.257372654155496,
      "grad_norm": 0.14385013282299042,
      "learning_rate": 0.0007485254691689009,
      "loss": 0.1104,
      "step": 469
    },
    {
      "epoch": 1.260053619302949,
      "grad_norm": 0.041834477335214615,
      "learning_rate": 0.0007479892761394103,
      "loss": 0.1348,
      "step": 470
    },
    {
      "epoch": 1.262734584450402,
      "grad_norm": 0.04362652078270912,
      "learning_rate": 0.0007474530831099196,
      "loss": 0.1133,
      "step": 471
    },
    {
      "epoch": 1.2654155495978552,
      "grad_norm": 0.03786700591444969,
      "learning_rate": 0.000746916890080429,
      "loss": 0.0776,
      "step": 472
    },
    {
      "epoch": 1.2680965147453083,
      "grad_norm": 0.05023612454533577,
      "learning_rate": 0.0007463806970509384,
      "loss": 0.1035,
      "step": 473
    },
    {
      "epoch": 1.2707774798927614,
      "grad_norm": 0.06704123318195343,
      "learning_rate": 0.0007458445040214478,
      "loss": 0.1045,
      "step": 474
    },
    {
      "epoch": 1.2734584450402144,
      "grad_norm": 0.03324117138981819,
      "learning_rate": 0.0007453083109919572,
      "loss": 0.0981,
      "step": 475
    },
    {
      "epoch": 1.2761394101876675,
      "grad_norm": 0.08028421550989151,
      "learning_rate": 0.0007447721179624665,
      "loss": 0.1128,
      "step": 476
    },
    {
      "epoch": 1.2788203753351206,
      "grad_norm": 0.04832794889807701,
      "learning_rate": 0.0007442359249329759,
      "loss": 0.0981,
      "step": 477
    },
    {
      "epoch": 1.2815013404825737,
      "grad_norm": 0.058108240365982056,
      "learning_rate": 0.0007436997319034853,
      "loss": 0.0815,
      "step": 478
    },
    {
      "epoch": 1.2841823056300268,
      "grad_norm": 0.04811503365635872,
      "learning_rate": 0.0007431635388739947,
      "loss": 0.127,
      "step": 479
    },
    {
      "epoch": 1.2868632707774799,
      "grad_norm": 0.041951317340135574,
      "learning_rate": 0.0007426273458445041,
      "loss": 0.1021,
      "step": 480
    },
    {
      "epoch": 1.289544235924933,
      "grad_norm": 0.0333263985812664,
      "learning_rate": 0.0007420911528150134,
      "loss": 0.0981,
      "step": 481
    },
    {
      "epoch": 1.292225201072386,
      "grad_norm": 0.04993249475955963,
      "learning_rate": 0.0007415549597855228,
      "loss": 0.1001,
      "step": 482
    },
    {
      "epoch": 1.2949061662198391,
      "grad_norm": 0.0402793362736702,
      "learning_rate": 0.0007410187667560322,
      "loss": 0.0864,
      "step": 483
    },
    {
      "epoch": 1.2975871313672922,
      "grad_norm": 0.04187599569559097,
      "learning_rate": 0.0007404825737265416,
      "loss": 0.0781,
      "step": 484
    },
    {
      "epoch": 1.3002680965147453,
      "grad_norm": 0.032300300896167755,
      "learning_rate": 0.000739946380697051,
      "loss": 0.1094,
      "step": 485
    },
    {
      "epoch": 1.3029490616621984,
      "grad_norm": 0.0479496531188488,
      "learning_rate": 0.0007394101876675603,
      "loss": 0.0952,
      "step": 486
    },
    {
      "epoch": 1.3056300268096515,
      "grad_norm": 0.1377042680978775,
      "learning_rate": 0.0007388739946380697,
      "loss": 0.1216,
      "step": 487
    },
    {
      "epoch": 1.3083109919571045,
      "grad_norm": 0.0718507319688797,
      "learning_rate": 0.0007383378016085791,
      "loss": 0.0933,
      "step": 488
    },
    {
      "epoch": 1.3109919571045576,
      "grad_norm": 0.06362006068229675,
      "learning_rate": 0.0007378016085790886,
      "loss": 0.1143,
      "step": 489
    },
    {
      "epoch": 1.3136729222520107,
      "grad_norm": 0.059259604662656784,
      "learning_rate": 0.000737265415549598,
      "loss": 0.0952,
      "step": 490
    },
    {
      "epoch": 1.3163538873994638,
      "grad_norm": 0.048544492572546005,
      "learning_rate": 0.0007367292225201073,
      "loss": 0.1187,
      "step": 491
    },
    {
      "epoch": 1.3190348525469169,
      "grad_norm": 0.054381363093853,
      "learning_rate": 0.0007361930294906167,
      "loss": 0.0947,
      "step": 492
    },
    {
      "epoch": 1.32171581769437,
      "grad_norm": 0.06608201563358307,
      "learning_rate": 0.0007356568364611261,
      "loss": 0.1069,
      "step": 493
    },
    {
      "epoch": 1.324396782841823,
      "grad_norm": 0.04577617347240448,
      "learning_rate": 0.0007351206434316355,
      "loss": 0.1045,
      "step": 494
    },
    {
      "epoch": 1.3270777479892761,
      "grad_norm": 0.04107692092657089,
      "learning_rate": 0.0007345844504021449,
      "loss": 0.123,
      "step": 495
    },
    {
      "epoch": 1.3297587131367292,
      "grad_norm": 0.04668464511632919,
      "learning_rate": 0.0007340482573726542,
      "loss": 0.0806,
      "step": 496
    },
    {
      "epoch": 1.3324396782841823,
      "grad_norm": 0.04721759259700775,
      "learning_rate": 0.0007335120643431636,
      "loss": 0.0933,
      "step": 497
    },
    {
      "epoch": 1.3351206434316354,
      "grad_norm": 0.0504222996532917,
      "learning_rate": 0.000732975871313673,
      "loss": 0.1099,
      "step": 498
    },
    {
      "epoch": 1.3378016085790885,
      "grad_norm": 0.07776026427745819,
      "learning_rate": 0.0007324396782841824,
      "loss": 0.1011,
      "step": 499
    },
    {
      "epoch": 1.3404825737265416,
      "grad_norm": 0.04930930212140083,
      "learning_rate": 0.0007319034852546918,
      "loss": 0.1138,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1865,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2747151144714240.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
