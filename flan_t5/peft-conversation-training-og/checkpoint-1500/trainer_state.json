{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.021447721179625,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002680965147453083,
      "grad_norm": 4.094821929931641,
      "learning_rate": 0.0009994638069705094,
      "loss": 41.5,
      "step": 1
    },
    {
      "epoch": 0.005361930294906166,
      "grad_norm": 5.413318634033203,
      "learning_rate": 0.0009989276139410188,
      "loss": 42.0,
      "step": 2
    },
    {
      "epoch": 0.00804289544235925,
      "grad_norm": 7.137814044952393,
      "learning_rate": 0.000998391420911528,
      "loss": 41.75,
      "step": 3
    },
    {
      "epoch": 0.010723860589812333,
      "grad_norm": 9.622596740722656,
      "learning_rate": 0.0009978552278820374,
      "loss": 40.25,
      "step": 4
    },
    {
      "epoch": 0.013404825737265416,
      "grad_norm": 9.371594429016113,
      "learning_rate": 0.0009973190348525468,
      "loss": 37.25,
      "step": 5
    },
    {
      "epoch": 0.0160857908847185,
      "grad_norm": 7.442983150482178,
      "learning_rate": 0.0009967828418230562,
      "loss": 34.25,
      "step": 6
    },
    {
      "epoch": 0.01876675603217158,
      "grad_norm": 7.496304512023926,
      "learning_rate": 0.0009962466487935656,
      "loss": 33.25,
      "step": 7
    },
    {
      "epoch": 0.021447721179624665,
      "grad_norm": 8.448534965515137,
      "learning_rate": 0.000995710455764075,
      "loss": 31.125,
      "step": 8
    },
    {
      "epoch": 0.024128686327077747,
      "grad_norm": 7.0896077156066895,
      "learning_rate": 0.0009951742627345845,
      "loss": 29.375,
      "step": 9
    },
    {
      "epoch": 0.02680965147453083,
      "grad_norm": 8.449664115905762,
      "learning_rate": 0.0009946380697050939,
      "loss": 28.0,
      "step": 10
    },
    {
      "epoch": 0.029490616621983913,
      "grad_norm": 7.011782169342041,
      "learning_rate": 0.0009941018766756033,
      "loss": 26.625,
      "step": 11
    },
    {
      "epoch": 0.032171581769437,
      "grad_norm": 6.380695343017578,
      "learning_rate": 0.0009935656836461127,
      "loss": 24.875,
      "step": 12
    },
    {
      "epoch": 0.03485254691689008,
      "grad_norm": 6.666173458099365,
      "learning_rate": 0.0009930294906166219,
      "loss": 24.0,
      "step": 13
    },
    {
      "epoch": 0.03753351206434316,
      "grad_norm": 6.85632848739624,
      "learning_rate": 0.0009924932975871313,
      "loss": 22.5,
      "step": 14
    },
    {
      "epoch": 0.040214477211796246,
      "grad_norm": 7.1602582931518555,
      "learning_rate": 0.0009919571045576407,
      "loss": 21.375,
      "step": 15
    },
    {
      "epoch": 0.04289544235924933,
      "grad_norm": 7.728709697723389,
      "learning_rate": 0.00099142091152815,
      "loss": 19.75,
      "step": 16
    },
    {
      "epoch": 0.045576407506702415,
      "grad_norm": 11.000395774841309,
      "learning_rate": 0.0009908847184986595,
      "loss": 18.0,
      "step": 17
    },
    {
      "epoch": 0.04825737265415549,
      "grad_norm": 8.063335418701172,
      "learning_rate": 0.0009903485254691689,
      "loss": 15.9375,
      "step": 18
    },
    {
      "epoch": 0.05093833780160858,
      "grad_norm": 8.15415096282959,
      "learning_rate": 0.0009898123324396783,
      "loss": 14.25,
      "step": 19
    },
    {
      "epoch": 0.05361930294906166,
      "grad_norm": 10.970466613769531,
      "learning_rate": 0.0009892761394101877,
      "loss": 12.0625,
      "step": 20
    },
    {
      "epoch": 0.05630026809651475,
      "grad_norm": 9.255367279052734,
      "learning_rate": 0.000988739946380697,
      "loss": 9.6875,
      "step": 21
    },
    {
      "epoch": 0.058981233243967826,
      "grad_norm": 6.71974515914917,
      "learning_rate": 0.0009882037533512065,
      "loss": 7.1562,
      "step": 22
    },
    {
      "epoch": 0.06166219839142091,
      "grad_norm": 4.004016399383545,
      "learning_rate": 0.0009876675603217157,
      "loss": 5.7812,
      "step": 23
    },
    {
      "epoch": 0.064343163538874,
      "grad_norm": 1.8248860836029053,
      "learning_rate": 0.000987131367292225,
      "loss": 5.1875,
      "step": 24
    },
    {
      "epoch": 0.06702412868632708,
      "grad_norm": 0.9842170476913452,
      "learning_rate": 0.0009865951742627345,
      "loss": 4.9062,
      "step": 25
    },
    {
      "epoch": 0.06970509383378017,
      "grad_norm": 0.7422013282775879,
      "learning_rate": 0.000986058981233244,
      "loss": 4.8125,
      "step": 26
    },
    {
      "epoch": 0.07238605898123325,
      "grad_norm": 0.5527276992797852,
      "learning_rate": 0.0009855227882037533,
      "loss": 4.7188,
      "step": 27
    },
    {
      "epoch": 0.07506702412868632,
      "grad_norm": 0.433874249458313,
      "learning_rate": 0.0009849865951742627,
      "loss": 4.625,
      "step": 28
    },
    {
      "epoch": 0.0777479892761394,
      "grad_norm": 0.3552132546901703,
      "learning_rate": 0.0009844504021447721,
      "loss": 4.5625,
      "step": 29
    },
    {
      "epoch": 0.08042895442359249,
      "grad_norm": 0.3121449053287506,
      "learning_rate": 0.0009839142091152815,
      "loss": 4.5,
      "step": 30
    },
    {
      "epoch": 0.08310991957104558,
      "grad_norm": 0.299744576215744,
      "learning_rate": 0.000983378016085791,
      "loss": 4.5,
      "step": 31
    },
    {
      "epoch": 0.08579088471849866,
      "grad_norm": 0.4459809958934784,
      "learning_rate": 0.0009828418230563003,
      "loss": 4.4688,
      "step": 32
    },
    {
      "epoch": 0.08847184986595175,
      "grad_norm": 0.28072455525398254,
      "learning_rate": 0.0009823056300268097,
      "loss": 4.4375,
      "step": 33
    },
    {
      "epoch": 0.09115281501340483,
      "grad_norm": 0.2792031168937683,
      "learning_rate": 0.000981769436997319,
      "loss": 4.4062,
      "step": 34
    },
    {
      "epoch": 0.0938337801608579,
      "grad_norm": 0.29252859950065613,
      "learning_rate": 0.0009812332439678283,
      "loss": 4.3438,
      "step": 35
    },
    {
      "epoch": 0.09651474530831099,
      "grad_norm": 0.3004789352416992,
      "learning_rate": 0.0009806970509383377,
      "loss": 4.3125,
      "step": 36
    },
    {
      "epoch": 0.09919571045576407,
      "grad_norm": 0.28331777453422546,
      "learning_rate": 0.0009801608579088471,
      "loss": 4.2812,
      "step": 37
    },
    {
      "epoch": 0.10187667560321716,
      "grad_norm": 0.29593804478645325,
      "learning_rate": 0.0009796246648793566,
      "loss": 4.2188,
      "step": 38
    },
    {
      "epoch": 0.10455764075067024,
      "grad_norm": 0.3360207974910736,
      "learning_rate": 0.000979088471849866,
      "loss": 4.2188,
      "step": 39
    },
    {
      "epoch": 0.10723860589812333,
      "grad_norm": 0.9746589660644531,
      "learning_rate": 0.0009785522788203754,
      "loss": 4.125,
      "step": 40
    },
    {
      "epoch": 0.10991957104557641,
      "grad_norm": 0.40555500984191895,
      "learning_rate": 0.0009780160857908848,
      "loss": 4.0312,
      "step": 41
    },
    {
      "epoch": 0.1126005361930295,
      "grad_norm": 0.4915933609008789,
      "learning_rate": 0.0009774798927613942,
      "loss": 3.9688,
      "step": 42
    },
    {
      "epoch": 0.11528150134048257,
      "grad_norm": 0.5140253901481628,
      "learning_rate": 0.0009769436997319036,
      "loss": 3.9062,
      "step": 43
    },
    {
      "epoch": 0.11796246648793565,
      "grad_norm": 0.59853595495224,
      "learning_rate": 0.0009764075067024129,
      "loss": 3.7656,
      "step": 44
    },
    {
      "epoch": 0.12064343163538874,
      "grad_norm": 0.6076123118400574,
      "learning_rate": 0.0009758713136729223,
      "loss": 3.6562,
      "step": 45
    },
    {
      "epoch": 0.12332439678284182,
      "grad_norm": 3.3200981616973877,
      "learning_rate": 0.0009753351206434317,
      "loss": 3.6094,
      "step": 46
    },
    {
      "epoch": 0.1260053619302949,
      "grad_norm": 1.0076669454574585,
      "learning_rate": 0.0009747989276139411,
      "loss": 3.4062,
      "step": 47
    },
    {
      "epoch": 0.128686327077748,
      "grad_norm": 0.8188648223876953,
      "learning_rate": 0.0009742627345844505,
      "loss": 3.2656,
      "step": 48
    },
    {
      "epoch": 0.13136729222520108,
      "grad_norm": 1.087272047996521,
      "learning_rate": 0.0009737265415549598,
      "loss": 3.1094,
      "step": 49
    },
    {
      "epoch": 0.13404825737265416,
      "grad_norm": 0.8814945220947266,
      "learning_rate": 0.0009731903485254692,
      "loss": 2.9688,
      "step": 50
    },
    {
      "epoch": 0.13672922252010725,
      "grad_norm": 1.3187233209609985,
      "learning_rate": 0.0009726541554959786,
      "loss": 2.8125,
      "step": 51
    },
    {
      "epoch": 0.13941018766756033,
      "grad_norm": 9.174578666687012,
      "learning_rate": 0.000972117962466488,
      "loss": 2.6562,
      "step": 52
    },
    {
      "epoch": 0.14209115281501342,
      "grad_norm": 0.9857268333435059,
      "learning_rate": 0.0009715817694369974,
      "loss": 2.6094,
      "step": 53
    },
    {
      "epoch": 0.1447721179624665,
      "grad_norm": 0.899191677570343,
      "learning_rate": 0.0009710455764075067,
      "loss": 2.5625,
      "step": 54
    },
    {
      "epoch": 0.14745308310991956,
      "grad_norm": 1.0879570245742798,
      "learning_rate": 0.0009705093833780161,
      "loss": 2.4062,
      "step": 55
    },
    {
      "epoch": 0.15013404825737264,
      "grad_norm": 0.8324114084243774,
      "learning_rate": 0.0009699731903485255,
      "loss": 2.2031,
      "step": 56
    },
    {
      "epoch": 0.15281501340482573,
      "grad_norm": 0.910655677318573,
      "learning_rate": 0.0009694369973190349,
      "loss": 2.125,
      "step": 57
    },
    {
      "epoch": 0.1554959785522788,
      "grad_norm": 0.6608592867851257,
      "learning_rate": 0.0009689008042895443,
      "loss": 1.9766,
      "step": 58
    },
    {
      "epoch": 0.1581769436997319,
      "grad_norm": 0.7495169043540955,
      "learning_rate": 0.0009683646112600536,
      "loss": 1.8828,
      "step": 59
    },
    {
      "epoch": 0.16085790884718498,
      "grad_norm": 1.437414526939392,
      "learning_rate": 0.000967828418230563,
      "loss": 1.8516,
      "step": 60
    },
    {
      "epoch": 0.16353887399463807,
      "grad_norm": 0.5739189982414246,
      "learning_rate": 0.0009672922252010724,
      "loss": 1.6562,
      "step": 61
    },
    {
      "epoch": 0.16621983914209115,
      "grad_norm": 0.5036547780036926,
      "learning_rate": 0.0009667560321715818,
      "loss": 1.6172,
      "step": 62
    },
    {
      "epoch": 0.16890080428954424,
      "grad_norm": 0.49479731917381287,
      "learning_rate": 0.0009662198391420912,
      "loss": 1.5391,
      "step": 63
    },
    {
      "epoch": 0.17158176943699732,
      "grad_norm": 0.5023074746131897,
      "learning_rate": 0.0009656836461126005,
      "loss": 1.4531,
      "step": 64
    },
    {
      "epoch": 0.1742627345844504,
      "grad_norm": 1.9169942140579224,
      "learning_rate": 0.00096514745308311,
      "loss": 1.4375,
      "step": 65
    },
    {
      "epoch": 0.1769436997319035,
      "grad_norm": 0.556627631187439,
      "learning_rate": 0.0009646112600536194,
      "loss": 1.3047,
      "step": 66
    },
    {
      "epoch": 0.17962466487935658,
      "grad_norm": 0.7504363656044006,
      "learning_rate": 0.0009640750670241288,
      "loss": 1.2188,
      "step": 67
    },
    {
      "epoch": 0.18230563002680966,
      "grad_norm": 0.4650214910507202,
      "learning_rate": 0.0009635388739946382,
      "loss": 1.1406,
      "step": 68
    },
    {
      "epoch": 0.18498659517426275,
      "grad_norm": 0.49165165424346924,
      "learning_rate": 0.0009630026809651475,
      "loss": 1.1094,
      "step": 69
    },
    {
      "epoch": 0.1876675603217158,
      "grad_norm": 0.3694368898868561,
      "learning_rate": 0.0009624664879356569,
      "loss": 1.0312,
      "step": 70
    },
    {
      "epoch": 0.1903485254691689,
      "grad_norm": 0.45134884119033813,
      "learning_rate": 0.0009619302949061663,
      "loss": 0.9727,
      "step": 71
    },
    {
      "epoch": 0.19302949061662197,
      "grad_norm": 0.6614948511123657,
      "learning_rate": 0.0009613941018766757,
      "loss": 0.9609,
      "step": 72
    },
    {
      "epoch": 0.19571045576407506,
      "grad_norm": 0.41007348895072937,
      "learning_rate": 0.0009608579088471851,
      "loss": 0.918,
      "step": 73
    },
    {
      "epoch": 0.19839142091152814,
      "grad_norm": 0.39174968004226685,
      "learning_rate": 0.0009603217158176944,
      "loss": 0.8984,
      "step": 74
    },
    {
      "epoch": 0.20107238605898123,
      "grad_norm": 0.2502189576625824,
      "learning_rate": 0.0009597855227882038,
      "loss": 0.875,
      "step": 75
    },
    {
      "epoch": 0.2037533512064343,
      "grad_norm": 0.3806720972061157,
      "learning_rate": 0.0009592493297587132,
      "loss": 0.8086,
      "step": 76
    },
    {
      "epoch": 0.2064343163538874,
      "grad_norm": 0.2548300325870514,
      "learning_rate": 0.0009587131367292226,
      "loss": 0.7461,
      "step": 77
    },
    {
      "epoch": 0.20911528150134048,
      "grad_norm": 0.25718289613723755,
      "learning_rate": 0.000958176943699732,
      "loss": 0.6758,
      "step": 78
    },
    {
      "epoch": 0.21179624664879357,
      "grad_norm": 0.26958543062210083,
      "learning_rate": 0.0009576407506702413,
      "loss": 0.668,
      "step": 79
    },
    {
      "epoch": 0.21447721179624665,
      "grad_norm": 0.29016542434692383,
      "learning_rate": 0.0009571045576407507,
      "loss": 0.6406,
      "step": 80
    },
    {
      "epoch": 0.21715817694369974,
      "grad_norm": 0.30451810359954834,
      "learning_rate": 0.0009565683646112601,
      "loss": 0.6133,
      "step": 81
    },
    {
      "epoch": 0.21983914209115282,
      "grad_norm": 0.2497199922800064,
      "learning_rate": 0.0009560321715817695,
      "loss": 0.582,
      "step": 82
    },
    {
      "epoch": 0.2225201072386059,
      "grad_norm": 0.2642647325992584,
      "learning_rate": 0.0009554959785522789,
      "loss": 0.582,
      "step": 83
    },
    {
      "epoch": 0.225201072386059,
      "grad_norm": 0.33330249786376953,
      "learning_rate": 0.0009549597855227882,
      "loss": 0.582,
      "step": 84
    },
    {
      "epoch": 0.22788203753351208,
      "grad_norm": 0.2956954836845398,
      "learning_rate": 0.0009544235924932976,
      "loss": 0.5664,
      "step": 85
    },
    {
      "epoch": 0.23056300268096513,
      "grad_norm": 0.1924213320016861,
      "learning_rate": 0.000953887399463807,
      "loss": 0.5703,
      "step": 86
    },
    {
      "epoch": 0.23324396782841822,
      "grad_norm": 0.2678512930870056,
      "learning_rate": 0.0009533512064343164,
      "loss": 0.5273,
      "step": 87
    },
    {
      "epoch": 0.2359249329758713,
      "grad_norm": 0.15428487956523895,
      "learning_rate": 0.0009528150134048258,
      "loss": 0.5273,
      "step": 88
    },
    {
      "epoch": 0.2386058981233244,
      "grad_norm": 0.1895735263824463,
      "learning_rate": 0.0009522788203753351,
      "loss": 0.4648,
      "step": 89
    },
    {
      "epoch": 0.24128686327077747,
      "grad_norm": 0.2505476176738739,
      "learning_rate": 0.0009517426273458445,
      "loss": 0.4941,
      "step": 90
    },
    {
      "epoch": 0.24396782841823056,
      "grad_norm": 0.17959438264369965,
      "learning_rate": 0.0009512064343163539,
      "loss": 0.4648,
      "step": 91
    },
    {
      "epoch": 0.24664879356568364,
      "grad_norm": 0.30983635783195496,
      "learning_rate": 0.0009506702412868633,
      "loss": 0.4609,
      "step": 92
    },
    {
      "epoch": 0.24932975871313673,
      "grad_norm": 0.14923739433288574,
      "learning_rate": 0.0009501340482573728,
      "loss": 0.457,
      "step": 93
    },
    {
      "epoch": 0.2520107238605898,
      "grad_norm": 0.20119239389896393,
      "learning_rate": 0.0009495978552278822,
      "loss": 0.4316,
      "step": 94
    },
    {
      "epoch": 0.2546916890080429,
      "grad_norm": 0.1832408756017685,
      "learning_rate": 0.0009490616621983915,
      "loss": 0.4082,
      "step": 95
    },
    {
      "epoch": 0.257372654155496,
      "grad_norm": 0.13889513909816742,
      "learning_rate": 0.0009485254691689009,
      "loss": 0.4258,
      "step": 96
    },
    {
      "epoch": 0.26005361930294907,
      "grad_norm": 0.18250522017478943,
      "learning_rate": 0.0009479892761394103,
      "loss": 0.4082,
      "step": 97
    },
    {
      "epoch": 0.26273458445040215,
      "grad_norm": 0.16289867460727692,
      "learning_rate": 0.0009474530831099197,
      "loss": 0.4004,
      "step": 98
    },
    {
      "epoch": 0.26541554959785524,
      "grad_norm": 0.2458774447441101,
      "learning_rate": 0.0009469168900804291,
      "loss": 0.4141,
      "step": 99
    },
    {
      "epoch": 0.2680965147453083,
      "grad_norm": 0.12051112949848175,
      "learning_rate": 0.0009463806970509384,
      "loss": 0.3965,
      "step": 100
    },
    {
      "epoch": 0.2707774798927614,
      "grad_norm": 0.14612841606140137,
      "learning_rate": 0.0009458445040214478,
      "loss": 0.3574,
      "step": 101
    },
    {
      "epoch": 0.2734584450402145,
      "grad_norm": 0.1700150966644287,
      "learning_rate": 0.0009453083109919572,
      "loss": 0.3516,
      "step": 102
    },
    {
      "epoch": 0.2761394101876676,
      "grad_norm": 0.20658668875694275,
      "learning_rate": 0.0009447721179624666,
      "loss": 0.3262,
      "step": 103
    },
    {
      "epoch": 0.27882037533512066,
      "grad_norm": 0.3899141252040863,
      "learning_rate": 0.000944235924932976,
      "loss": 0.3555,
      "step": 104
    },
    {
      "epoch": 0.28150134048257375,
      "grad_norm": 0.2859193980693817,
      "learning_rate": 0.0009436997319034852,
      "loss": 0.3105,
      "step": 105
    },
    {
      "epoch": 0.28418230563002683,
      "grad_norm": 0.25368359684944153,
      "learning_rate": 0.0009431635388739946,
      "loss": 0.3262,
      "step": 106
    },
    {
      "epoch": 0.2868632707774799,
      "grad_norm": 0.17001894116401672,
      "learning_rate": 0.000942627345844504,
      "loss": 0.3242,
      "step": 107
    },
    {
      "epoch": 0.289544235924933,
      "grad_norm": 0.15162426233291626,
      "learning_rate": 0.0009420911528150134,
      "loss": 0.3555,
      "step": 108
    },
    {
      "epoch": 0.29222520107238603,
      "grad_norm": 0.20448242127895355,
      "learning_rate": 0.0009415549597855228,
      "loss": 0.3262,
      "step": 109
    },
    {
      "epoch": 0.2949061662198391,
      "grad_norm": 0.23639830946922302,
      "learning_rate": 0.0009410187667560321,
      "loss": 0.3203,
      "step": 110
    },
    {
      "epoch": 0.2975871313672922,
      "grad_norm": 0.1214347779750824,
      "learning_rate": 0.0009404825737265415,
      "loss": 0.3066,
      "step": 111
    },
    {
      "epoch": 0.3002680965147453,
      "grad_norm": 0.12820422649383545,
      "learning_rate": 0.0009399463806970509,
      "loss": 0.293,
      "step": 112
    },
    {
      "epoch": 0.30294906166219837,
      "grad_norm": 0.18421916663646698,
      "learning_rate": 0.0009394101876675603,
      "loss": 0.3223,
      "step": 113
    },
    {
      "epoch": 0.30563002680965146,
      "grad_norm": 0.12301234155893326,
      "learning_rate": 0.0009388739946380697,
      "loss": 0.2656,
      "step": 114
    },
    {
      "epoch": 0.30831099195710454,
      "grad_norm": 0.2842372953891754,
      "learning_rate": 0.000938337801608579,
      "loss": 0.3105,
      "step": 115
    },
    {
      "epoch": 0.3109919571045576,
      "grad_norm": 0.1376074105501175,
      "learning_rate": 0.0009378016085790884,
      "loss": 0.2715,
      "step": 116
    },
    {
      "epoch": 0.3136729222520107,
      "grad_norm": 0.14928561449050903,
      "learning_rate": 0.0009372654155495978,
      "loss": 0.3086,
      "step": 117
    },
    {
      "epoch": 0.3163538873994638,
      "grad_norm": 0.1665962040424347,
      "learning_rate": 0.0009367292225201072,
      "loss": 0.2637,
      "step": 118
    },
    {
      "epoch": 0.3190348525469169,
      "grad_norm": 0.1384596973657608,
      "learning_rate": 0.0009361930294906166,
      "loss": 0.293,
      "step": 119
    },
    {
      "epoch": 0.32171581769436997,
      "grad_norm": 0.11943553388118744,
      "learning_rate": 0.0009356568364611259,
      "loss": 0.2793,
      "step": 120
    },
    {
      "epoch": 0.32439678284182305,
      "grad_norm": 0.15297870337963104,
      "learning_rate": 0.0009351206434316353,
      "loss": 0.2617,
      "step": 121
    },
    {
      "epoch": 0.32707774798927614,
      "grad_norm": 0.1746274083852768,
      "learning_rate": 0.0009345844504021447,
      "loss": 0.2578,
      "step": 122
    },
    {
      "epoch": 0.3297587131367292,
      "grad_norm": 0.23644663393497467,
      "learning_rate": 0.0009340482573726542,
      "loss": 0.2715,
      "step": 123
    },
    {
      "epoch": 0.3324396782841823,
      "grad_norm": 0.23075322806835175,
      "learning_rate": 0.0009335120643431636,
      "loss": 0.25,
      "step": 124
    },
    {
      "epoch": 0.3351206434316354,
      "grad_norm": 0.11434713006019592,
      "learning_rate": 0.0009329758713136729,
      "loss": 0.3145,
      "step": 125
    },
    {
      "epoch": 0.3378016085790885,
      "grad_norm": 0.2182014435529709,
      "learning_rate": 0.0009324396782841823,
      "loss": 0.2832,
      "step": 126
    },
    {
      "epoch": 0.34048257372654156,
      "grad_norm": 0.1463240683078766,
      "learning_rate": 0.0009319034852546917,
      "loss": 0.2617,
      "step": 127
    },
    {
      "epoch": 0.34316353887399464,
      "grad_norm": 0.16704979538917542,
      "learning_rate": 0.0009313672922252011,
      "loss": 0.2656,
      "step": 128
    },
    {
      "epoch": 0.34584450402144773,
      "grad_norm": 0.10418020188808441,
      "learning_rate": 0.0009308310991957105,
      "loss": 0.248,
      "step": 129
    },
    {
      "epoch": 0.3485254691689008,
      "grad_norm": 0.10229937732219696,
      "learning_rate": 0.0009302949061662198,
      "loss": 0.2266,
      "step": 130
    },
    {
      "epoch": 0.3512064343163539,
      "grad_norm": 0.12001902610063553,
      "learning_rate": 0.0009297587131367292,
      "loss": 0.2412,
      "step": 131
    },
    {
      "epoch": 0.353887399463807,
      "grad_norm": 0.09072788059711456,
      "learning_rate": 0.0009292225201072386,
      "loss": 0.2354,
      "step": 132
    },
    {
      "epoch": 0.35656836461126007,
      "grad_norm": 0.09861829876899719,
      "learning_rate": 0.000928686327077748,
      "loss": 0.2578,
      "step": 133
    },
    {
      "epoch": 0.35924932975871315,
      "grad_norm": 0.1684478521347046,
      "learning_rate": 0.0009281501340482574,
      "loss": 0.293,
      "step": 134
    },
    {
      "epoch": 0.36193029490616624,
      "grad_norm": 0.14725051820278168,
      "learning_rate": 0.0009276139410187667,
      "loss": 0.2539,
      "step": 135
    },
    {
      "epoch": 0.3646112600536193,
      "grad_norm": 0.12352783977985382,
      "learning_rate": 0.0009270777479892761,
      "loss": 0.2188,
      "step": 136
    },
    {
      "epoch": 0.3672922252010724,
      "grad_norm": 0.1417471468448639,
      "learning_rate": 0.0009265415549597855,
      "loss": 0.2354,
      "step": 137
    },
    {
      "epoch": 0.3699731903485255,
      "grad_norm": 0.08479699492454529,
      "learning_rate": 0.0009260053619302949,
      "loss": 0.2598,
      "step": 138
    },
    {
      "epoch": 0.3726541554959786,
      "grad_norm": 0.20304365456104279,
      "learning_rate": 0.0009254691689008043,
      "loss": 0.2266,
      "step": 139
    },
    {
      "epoch": 0.3753351206434316,
      "grad_norm": 0.22194866836071014,
      "learning_rate": 0.0009249329758713136,
      "loss": 0.2285,
      "step": 140
    },
    {
      "epoch": 0.3780160857908847,
      "grad_norm": 0.11341596394777298,
      "learning_rate": 0.000924396782841823,
      "loss": 0.2363,
      "step": 141
    },
    {
      "epoch": 0.3806970509383378,
      "grad_norm": 0.13468405604362488,
      "learning_rate": 0.0009238605898123324,
      "loss": 0.2197,
      "step": 142
    },
    {
      "epoch": 0.38337801608579086,
      "grad_norm": 0.076375313103199,
      "learning_rate": 0.0009233243967828418,
      "loss": 0.2031,
      "step": 143
    },
    {
      "epoch": 0.38605898123324395,
      "grad_norm": 0.10989118367433548,
      "learning_rate": 0.0009227882037533512,
      "loss": 0.2119,
      "step": 144
    },
    {
      "epoch": 0.38873994638069703,
      "grad_norm": 0.1874612271785736,
      "learning_rate": 0.0009222520107238605,
      "loss": 0.2285,
      "step": 145
    },
    {
      "epoch": 0.3914209115281501,
      "grad_norm": 0.10381874442100525,
      "learning_rate": 0.0009217158176943699,
      "loss": 0.2129,
      "step": 146
    },
    {
      "epoch": 0.3941018766756032,
      "grad_norm": 0.0737760066986084,
      "learning_rate": 0.0009211796246648793,
      "loss": 0.1992,
      "step": 147
    },
    {
      "epoch": 0.3967828418230563,
      "grad_norm": 0.13673646748065948,
      "learning_rate": 0.0009206434316353887,
      "loss": 0.2002,
      "step": 148
    },
    {
      "epoch": 0.39946380697050937,
      "grad_norm": 0.1284661889076233,
      "learning_rate": 0.0009201072386058981,
      "loss": 0.2002,
      "step": 149
    },
    {
      "epoch": 0.40214477211796246,
      "grad_norm": 0.07227208465337753,
      "learning_rate": 0.0009195710455764074,
      "loss": 0.2197,
      "step": 150
    },
    {
      "epoch": 0.40482573726541554,
      "grad_norm": 0.053542088717222214,
      "learning_rate": 0.0009190348525469168,
      "loss": 0.2178,
      "step": 151
    },
    {
      "epoch": 0.4075067024128686,
      "grad_norm": 0.0772889256477356,
      "learning_rate": 0.0009184986595174263,
      "loss": 0.2119,
      "step": 152
    },
    {
      "epoch": 0.4101876675603217,
      "grad_norm": 0.12116805464029312,
      "learning_rate": 0.0009179624664879357,
      "loss": 0.2363,
      "step": 153
    },
    {
      "epoch": 0.4128686327077748,
      "grad_norm": 0.12567327916622162,
      "learning_rate": 0.0009174262734584451,
      "loss": 0.2109,
      "step": 154
    },
    {
      "epoch": 0.4155495978552279,
      "grad_norm": 0.1039428859949112,
      "learning_rate": 0.0009168900804289544,
      "loss": 0.1963,
      "step": 155
    },
    {
      "epoch": 0.41823056300268097,
      "grad_norm": 0.06572925299406052,
      "learning_rate": 0.0009163538873994638,
      "loss": 0.1875,
      "step": 156
    },
    {
      "epoch": 0.42091152815013405,
      "grad_norm": 0.27020809054374695,
      "learning_rate": 0.0009158176943699732,
      "loss": 0.1943,
      "step": 157
    },
    {
      "epoch": 0.42359249329758714,
      "grad_norm": 0.09034383296966553,
      "learning_rate": 0.0009152815013404826,
      "loss": 0.207,
      "step": 158
    },
    {
      "epoch": 0.4262734584450402,
      "grad_norm": 0.083586186170578,
      "learning_rate": 0.000914745308310992,
      "loss": 0.2041,
      "step": 159
    },
    {
      "epoch": 0.4289544235924933,
      "grad_norm": 0.07210909575223923,
      "learning_rate": 0.0009142091152815014,
      "loss": 0.1797,
      "step": 160
    },
    {
      "epoch": 0.4316353887399464,
      "grad_norm": 0.17327412962913513,
      "learning_rate": 0.0009136729222520107,
      "loss": 0.2002,
      "step": 161
    },
    {
      "epoch": 0.4343163538873995,
      "grad_norm": 0.1793481856584549,
      "learning_rate": 0.0009131367292225201,
      "loss": 0.2109,
      "step": 162
    },
    {
      "epoch": 0.43699731903485256,
      "grad_norm": 0.10437821596860886,
      "learning_rate": 0.0009126005361930295,
      "loss": 0.2002,
      "step": 163
    },
    {
      "epoch": 0.43967828418230565,
      "grad_norm": 0.059607233852148056,
      "learning_rate": 0.0009120643431635389,
      "loss": 0.1943,
      "step": 164
    },
    {
      "epoch": 0.44235924932975873,
      "grad_norm": 0.14320355653762817,
      "learning_rate": 0.0009115281501340483,
      "loss": 0.2061,
      "step": 165
    },
    {
      "epoch": 0.4450402144772118,
      "grad_norm": 0.09005400538444519,
      "learning_rate": 0.0009109919571045576,
      "loss": 0.1816,
      "step": 166
    },
    {
      "epoch": 0.4477211796246649,
      "grad_norm": 0.13945037126541138,
      "learning_rate": 0.000910455764075067,
      "loss": 0.1963,
      "step": 167
    },
    {
      "epoch": 0.450402144772118,
      "grad_norm": 0.11178182065486908,
      "learning_rate": 0.0009099195710455764,
      "loss": 0.1836,
      "step": 168
    },
    {
      "epoch": 0.45308310991957107,
      "grad_norm": 0.07875479757785797,
      "learning_rate": 0.0009093833780160858,
      "loss": 0.21,
      "step": 169
    },
    {
      "epoch": 0.45576407506702415,
      "grad_norm": 0.0738530308008194,
      "learning_rate": 0.0009088471849865952,
      "loss": 0.1992,
      "step": 170
    },
    {
      "epoch": 0.4584450402144772,
      "grad_norm": 0.10385853052139282,
      "learning_rate": 0.0009083109919571045,
      "loss": 0.2324,
      "step": 171
    },
    {
      "epoch": 0.46112600536193027,
      "grad_norm": 0.09368574619293213,
      "learning_rate": 0.0009077747989276139,
      "loss": 0.1934,
      "step": 172
    },
    {
      "epoch": 0.46380697050938335,
      "grad_norm": 0.12734414637088776,
      "learning_rate": 0.0009072386058981233,
      "loss": 0.1875,
      "step": 173
    },
    {
      "epoch": 0.46648793565683644,
      "grad_norm": 0.1611405313014984,
      "learning_rate": 0.0009067024128686327,
      "loss": 0.1924,
      "step": 174
    },
    {
      "epoch": 0.4691689008042895,
      "grad_norm": 0.061755552887916565,
      "learning_rate": 0.0009061662198391421,
      "loss": 0.1943,
      "step": 175
    },
    {
      "epoch": 0.4718498659517426,
      "grad_norm": 0.05461574345827103,
      "learning_rate": 0.0009056300268096514,
      "loss": 0.1797,
      "step": 176
    },
    {
      "epoch": 0.4745308310991957,
      "grad_norm": 0.1244334727525711,
      "learning_rate": 0.0009050938337801608,
      "loss": 0.1846,
      "step": 177
    },
    {
      "epoch": 0.4772117962466488,
      "grad_norm": 0.0824100449681282,
      "learning_rate": 0.0009045576407506702,
      "loss": 0.2139,
      "step": 178
    },
    {
      "epoch": 0.47989276139410186,
      "grad_norm": 0.05865325406193733,
      "learning_rate": 0.0009040214477211797,
      "loss": 0.1787,
      "step": 179
    },
    {
      "epoch": 0.48257372654155495,
      "grad_norm": 0.08235130459070206,
      "learning_rate": 0.0009034852546916891,
      "loss": 0.2383,
      "step": 180
    },
    {
      "epoch": 0.48525469168900803,
      "grad_norm": 0.10341799259185791,
      "learning_rate": 0.0009029490616621984,
      "loss": 0.1943,
      "step": 181
    },
    {
      "epoch": 0.4879356568364611,
      "grad_norm": 0.1217346340417862,
      "learning_rate": 0.0009024128686327078,
      "loss": 0.2021,
      "step": 182
    },
    {
      "epoch": 0.4906166219839142,
      "grad_norm": 0.1199263259768486,
      "learning_rate": 0.0009018766756032172,
      "loss": 0.2129,
      "step": 183
    },
    {
      "epoch": 0.4932975871313673,
      "grad_norm": 0.11809111386537552,
      "learning_rate": 0.0009013404825737266,
      "loss": 0.1875,
      "step": 184
    },
    {
      "epoch": 0.4959785522788204,
      "grad_norm": 0.0808391124010086,
      "learning_rate": 0.000900804289544236,
      "loss": 0.1572,
      "step": 185
    },
    {
      "epoch": 0.49865951742627346,
      "grad_norm": 0.07058607041835785,
      "learning_rate": 0.0009002680965147453,
      "loss": 0.1611,
      "step": 186
    },
    {
      "epoch": 0.5013404825737265,
      "grad_norm": 0.05178661644458771,
      "learning_rate": 0.0008997319034852547,
      "loss": 0.1758,
      "step": 187
    },
    {
      "epoch": 0.5040214477211796,
      "grad_norm": 0.05557653680443764,
      "learning_rate": 0.0008991957104557641,
      "loss": 0.1689,
      "step": 188
    },
    {
      "epoch": 0.5067024128686327,
      "grad_norm": 0.3273487389087677,
      "learning_rate": 0.0008986595174262735,
      "loss": 0.2178,
      "step": 189
    },
    {
      "epoch": 0.5093833780160858,
      "grad_norm": 0.045605387538671494,
      "learning_rate": 0.0008981233243967829,
      "loss": 0.1895,
      "step": 190
    },
    {
      "epoch": 0.5120643431635389,
      "grad_norm": 0.04935331642627716,
      "learning_rate": 0.0008975871313672922,
      "loss": 0.1729,
      "step": 191
    },
    {
      "epoch": 0.514745308310992,
      "grad_norm": 0.08618231862783432,
      "learning_rate": 0.0008970509383378016,
      "loss": 0.2012,
      "step": 192
    },
    {
      "epoch": 0.517426273458445,
      "grad_norm": 0.08344287425279617,
      "learning_rate": 0.000896514745308311,
      "loss": 0.1758,
      "step": 193
    },
    {
      "epoch": 0.5201072386058981,
      "grad_norm": 0.19316883385181427,
      "learning_rate": 0.0008959785522788204,
      "loss": 0.1455,
      "step": 194
    },
    {
      "epoch": 0.5227882037533512,
      "grad_norm": 0.09006964415311813,
      "learning_rate": 0.0008954423592493298,
      "loss": 0.1807,
      "step": 195
    },
    {
      "epoch": 0.5254691689008043,
      "grad_norm": 0.053801920264959335,
      "learning_rate": 0.0008949061662198391,
      "loss": 0.1797,
      "step": 196
    },
    {
      "epoch": 0.5281501340482574,
      "grad_norm": 0.08190446346998215,
      "learning_rate": 0.0008943699731903485,
      "loss": 0.1748,
      "step": 197
    },
    {
      "epoch": 0.5308310991957105,
      "grad_norm": 0.09411688148975372,
      "learning_rate": 0.0008938337801608579,
      "loss": 0.1787,
      "step": 198
    },
    {
      "epoch": 0.5335120643431636,
      "grad_norm": 0.1295836716890335,
      "learning_rate": 0.0008932975871313673,
      "loss": 0.1934,
      "step": 199
    },
    {
      "epoch": 0.5361930294906166,
      "grad_norm": 0.08562759310007095,
      "learning_rate": 0.0008927613941018767,
      "loss": 0.1709,
      "step": 200
    },
    {
      "epoch": 0.5388739946380697,
      "grad_norm": 0.20388098061084747,
      "learning_rate": 0.000892225201072386,
      "loss": 0.1621,
      "step": 201
    },
    {
      "epoch": 0.5415549597855228,
      "grad_norm": 0.10955978184938431,
      "learning_rate": 0.0008916890080428954,
      "loss": 0.1826,
      "step": 202
    },
    {
      "epoch": 0.5442359249329759,
      "grad_norm": 0.0738179013133049,
      "learning_rate": 0.0008911528150134048,
      "loss": 0.1865,
      "step": 203
    },
    {
      "epoch": 0.546916890080429,
      "grad_norm": 0.061613500118255615,
      "learning_rate": 0.0008906166219839142,
      "loss": 0.2031,
      "step": 204
    },
    {
      "epoch": 0.5495978552278821,
      "grad_norm": 0.06783407926559448,
      "learning_rate": 0.0008900804289544236,
      "loss": 0.1494,
      "step": 205
    },
    {
      "epoch": 0.5522788203753352,
      "grad_norm": 0.10419407486915588,
      "learning_rate": 0.0008895442359249329,
      "loss": 0.1973,
      "step": 206
    },
    {
      "epoch": 0.5549597855227882,
      "grad_norm": 0.07235944271087646,
      "learning_rate": 0.0008890080428954423,
      "loss": 0.1602,
      "step": 207
    },
    {
      "epoch": 0.5576407506702413,
      "grad_norm": 0.13933910429477692,
      "learning_rate": 0.0008884718498659518,
      "loss": 0.1533,
      "step": 208
    },
    {
      "epoch": 0.5603217158176944,
      "grad_norm": 0.11489327251911163,
      "learning_rate": 0.0008879356568364612,
      "loss": 0.1592,
      "step": 209
    },
    {
      "epoch": 0.5630026809651475,
      "grad_norm": 0.044777993112802505,
      "learning_rate": 0.0008873994638069706,
      "loss": 0.1533,
      "step": 210
    },
    {
      "epoch": 0.5656836461126006,
      "grad_norm": 0.19592058658599854,
      "learning_rate": 0.0008868632707774799,
      "loss": 0.2236,
      "step": 211
    },
    {
      "epoch": 0.5683646112600537,
      "grad_norm": 0.05544446036219597,
      "learning_rate": 0.0008863270777479893,
      "loss": 0.1641,
      "step": 212
    },
    {
      "epoch": 0.5710455764075067,
      "grad_norm": 0.043395090848207474,
      "learning_rate": 0.0008857908847184987,
      "loss": 0.1445,
      "step": 213
    },
    {
      "epoch": 0.5737265415549598,
      "grad_norm": 0.0692722275853157,
      "learning_rate": 0.0008852546916890081,
      "loss": 0.1553,
      "step": 214
    },
    {
      "epoch": 0.5764075067024129,
      "grad_norm": 0.06531191617250443,
      "learning_rate": 0.0008847184986595175,
      "loss": 0.1777,
      "step": 215
    },
    {
      "epoch": 0.579088471849866,
      "grad_norm": 0.1452934741973877,
      "learning_rate": 0.0008841823056300268,
      "loss": 0.1611,
      "step": 216
    },
    {
      "epoch": 0.5817694369973191,
      "grad_norm": 0.049949754029512405,
      "learning_rate": 0.0008836461126005362,
      "loss": 0.1689,
      "step": 217
    },
    {
      "epoch": 0.5844504021447721,
      "grad_norm": 0.19114215672016144,
      "learning_rate": 0.0008831099195710456,
      "loss": 0.1729,
      "step": 218
    },
    {
      "epoch": 0.5871313672922251,
      "grad_norm": 0.06373316049575806,
      "learning_rate": 0.000882573726541555,
      "loss": 0.1797,
      "step": 219
    },
    {
      "epoch": 0.5898123324396782,
      "grad_norm": 0.11046735942363739,
      "learning_rate": 0.0008820375335120644,
      "loss": 0.1885,
      "step": 220
    },
    {
      "epoch": 0.5924932975871313,
      "grad_norm": 0.0927487462759018,
      "learning_rate": 0.0008815013404825738,
      "loss": 0.1797,
      "step": 221
    },
    {
      "epoch": 0.5951742627345844,
      "grad_norm": 0.07419651746749878,
      "learning_rate": 0.0008809651474530831,
      "loss": 0.1514,
      "step": 222
    },
    {
      "epoch": 0.5978552278820375,
      "grad_norm": 0.09310788661241531,
      "learning_rate": 0.0008804289544235925,
      "loss": 0.1621,
      "step": 223
    },
    {
      "epoch": 0.6005361930294906,
      "grad_norm": 0.1004592627286911,
      "learning_rate": 0.0008798927613941019,
      "loss": 0.1748,
      "step": 224
    },
    {
      "epoch": 0.6032171581769437,
      "grad_norm": 0.10664056241512299,
      "learning_rate": 0.0008793565683646113,
      "loss": 0.1455,
      "step": 225
    },
    {
      "epoch": 0.6058981233243967,
      "grad_norm": 0.05210558697581291,
      "learning_rate": 0.0008788203753351207,
      "loss": 0.1426,
      "step": 226
    },
    {
      "epoch": 0.6085790884718498,
      "grad_norm": 0.1263301819562912,
      "learning_rate": 0.00087828418230563,
      "loss": 0.1699,
      "step": 227
    },
    {
      "epoch": 0.6112600536193029,
      "grad_norm": 0.06279142200946808,
      "learning_rate": 0.0008777479892761394,
      "loss": 0.1582,
      "step": 228
    },
    {
      "epoch": 0.613941018766756,
      "grad_norm": 0.054576706141233444,
      "learning_rate": 0.0008772117962466488,
      "loss": 0.1533,
      "step": 229
    },
    {
      "epoch": 0.6166219839142091,
      "grad_norm": 0.06367652118206024,
      "learning_rate": 0.0008766756032171582,
      "loss": 0.1602,
      "step": 230
    },
    {
      "epoch": 0.6193029490616622,
      "grad_norm": 0.07885829359292984,
      "learning_rate": 0.0008761394101876676,
      "loss": 0.1494,
      "step": 231
    },
    {
      "epoch": 0.6219839142091153,
      "grad_norm": 0.06232517212629318,
      "learning_rate": 0.0008756032171581769,
      "loss": 0.1592,
      "step": 232
    },
    {
      "epoch": 0.6246648793565683,
      "grad_norm": 0.049303747713565826,
      "learning_rate": 0.0008750670241286863,
      "loss": 0.167,
      "step": 233
    },
    {
      "epoch": 0.6273458445040214,
      "grad_norm": 0.06622445583343506,
      "learning_rate": 0.0008745308310991957,
      "loss": 0.1426,
      "step": 234
    },
    {
      "epoch": 0.6300268096514745,
      "grad_norm": 0.05847148224711418,
      "learning_rate": 0.0008739946380697052,
      "loss": 0.167,
      "step": 235
    },
    {
      "epoch": 0.6327077747989276,
      "grad_norm": 0.06749963760375977,
      "learning_rate": 0.0008734584450402146,
      "loss": 0.1514,
      "step": 236
    },
    {
      "epoch": 0.6353887399463807,
      "grad_norm": 0.07203942537307739,
      "learning_rate": 0.0008729222520107239,
      "loss": 0.1523,
      "step": 237
    },
    {
      "epoch": 0.6380697050938338,
      "grad_norm": 0.05947582423686981,
      "learning_rate": 0.0008723860589812333,
      "loss": 0.1885,
      "step": 238
    },
    {
      "epoch": 0.6407506702412868,
      "grad_norm": 0.0655047819018364,
      "learning_rate": 0.0008718498659517427,
      "loss": 0.1445,
      "step": 239
    },
    {
      "epoch": 0.6434316353887399,
      "grad_norm": 0.06399145722389221,
      "learning_rate": 0.0008713136729222521,
      "loss": 0.1504,
      "step": 240
    },
    {
      "epoch": 0.646112600536193,
      "grad_norm": 0.06266233325004578,
      "learning_rate": 0.0008707774798927615,
      "loss": 0.1787,
      "step": 241
    },
    {
      "epoch": 0.6487935656836461,
      "grad_norm": 0.051591433584690094,
      "learning_rate": 0.0008702412868632708,
      "loss": 0.1621,
      "step": 242
    },
    {
      "epoch": 0.6514745308310992,
      "grad_norm": 0.043494343757629395,
      "learning_rate": 0.0008697050938337802,
      "loss": 0.1367,
      "step": 243
    },
    {
      "epoch": 0.6541554959785523,
      "grad_norm": 0.08618957549333572,
      "learning_rate": 0.0008691689008042896,
      "loss": 0.1514,
      "step": 244
    },
    {
      "epoch": 0.6568364611260054,
      "grad_norm": 0.03784509748220444,
      "learning_rate": 0.000868632707774799,
      "loss": 0.1514,
      "step": 245
    },
    {
      "epoch": 0.6595174262734584,
      "grad_norm": 0.04608543962240219,
      "learning_rate": 0.0008680965147453084,
      "loss": 0.1719,
      "step": 246
    },
    {
      "epoch": 0.6621983914209115,
      "grad_norm": 0.04573862627148628,
      "learning_rate": 0.0008675603217158177,
      "loss": 0.1514,
      "step": 247
    },
    {
      "epoch": 0.6648793565683646,
      "grad_norm": 0.06628686934709549,
      "learning_rate": 0.0008670241286863271,
      "loss": 0.1387,
      "step": 248
    },
    {
      "epoch": 0.6675603217158177,
      "grad_norm": 0.42227041721343994,
      "learning_rate": 0.0008664879356568365,
      "loss": 0.1611,
      "step": 249
    },
    {
      "epoch": 0.6702412868632708,
      "grad_norm": 0.06363939493894577,
      "learning_rate": 0.0008659517426273459,
      "loss": 0.1543,
      "step": 250
    },
    {
      "epoch": 0.6729222520107239,
      "grad_norm": 0.1291762739419937,
      "learning_rate": 0.0008654155495978553,
      "loss": 0.1592,
      "step": 251
    },
    {
      "epoch": 0.675603217158177,
      "grad_norm": 0.08465231955051422,
      "learning_rate": 0.0008648793565683646,
      "loss": 0.1289,
      "step": 252
    },
    {
      "epoch": 0.67828418230563,
      "grad_norm": 0.07102254033088684,
      "learning_rate": 0.000864343163538874,
      "loss": 0.1245,
      "step": 253
    },
    {
      "epoch": 0.6809651474530831,
      "grad_norm": 0.04320219159126282,
      "learning_rate": 0.0008638069705093834,
      "loss": 0.1572,
      "step": 254
    },
    {
      "epoch": 0.6836461126005362,
      "grad_norm": 0.06793917715549469,
      "learning_rate": 0.0008632707774798928,
      "loss": 0.1494,
      "step": 255
    },
    {
      "epoch": 0.6863270777479893,
      "grad_norm": 0.0851428210735321,
      "learning_rate": 0.0008627345844504022,
      "loss": 0.1396,
      "step": 256
    },
    {
      "epoch": 0.6890080428954424,
      "grad_norm": 0.08463411778211594,
      "learning_rate": 0.0008621983914209115,
      "loss": 0.1475,
      "step": 257
    },
    {
      "epoch": 0.6916890080428955,
      "grad_norm": 0.060161739587783813,
      "learning_rate": 0.0008616621983914209,
      "loss": 0.1396,
      "step": 258
    },
    {
      "epoch": 0.6943699731903485,
      "grad_norm": 0.2213987112045288,
      "learning_rate": 0.0008611260053619303,
      "loss": 0.1641,
      "step": 259
    },
    {
      "epoch": 0.6970509383378016,
      "grad_norm": 0.10599017888307571,
      "learning_rate": 0.0008605898123324397,
      "loss": 0.165,
      "step": 260
    },
    {
      "epoch": 0.6997319034852547,
      "grad_norm": 0.062436603009700775,
      "learning_rate": 0.0008600536193029491,
      "loss": 0.1147,
      "step": 261
    },
    {
      "epoch": 0.7024128686327078,
      "grad_norm": 0.06163270026445389,
      "learning_rate": 0.0008595174262734584,
      "loss": 0.1348,
      "step": 262
    },
    {
      "epoch": 0.7050938337801609,
      "grad_norm": 0.0510433204472065,
      "learning_rate": 0.0008589812332439678,
      "loss": 0.1475,
      "step": 263
    },
    {
      "epoch": 0.707774798927614,
      "grad_norm": 0.08835092186927795,
      "learning_rate": 0.0008584450402144773,
      "loss": 0.1543,
      "step": 264
    },
    {
      "epoch": 0.710455764075067,
      "grad_norm": 0.05050614848732948,
      "learning_rate": 0.0008579088471849867,
      "loss": 0.1533,
      "step": 265
    },
    {
      "epoch": 0.7131367292225201,
      "grad_norm": 0.06976881623268127,
      "learning_rate": 0.0008573726541554961,
      "loss": 0.1504,
      "step": 266
    },
    {
      "epoch": 0.7158176943699732,
      "grad_norm": 0.05639524757862091,
      "learning_rate": 0.0008568364611260054,
      "loss": 0.1455,
      "step": 267
    },
    {
      "epoch": 0.7184986595174263,
      "grad_norm": 0.08397939056158066,
      "learning_rate": 0.0008563002680965148,
      "loss": 0.1553,
      "step": 268
    },
    {
      "epoch": 0.7211796246648794,
      "grad_norm": 0.052056312561035156,
      "learning_rate": 0.0008557640750670242,
      "loss": 0.1592,
      "step": 269
    },
    {
      "epoch": 0.7238605898123325,
      "grad_norm": 0.09511702507734299,
      "learning_rate": 0.0008552278820375336,
      "loss": 0.1357,
      "step": 270
    },
    {
      "epoch": 0.7265415549597856,
      "grad_norm": 0.09636112302541733,
      "learning_rate": 0.000854691689008043,
      "loss": 0.1396,
      "step": 271
    },
    {
      "epoch": 0.7292225201072386,
      "grad_norm": 0.04150195047259331,
      "learning_rate": 0.0008541554959785523,
      "loss": 0.1621,
      "step": 272
    },
    {
      "epoch": 0.7319034852546917,
      "grad_norm": 0.05346078798174858,
      "learning_rate": 0.0008536193029490617,
      "loss": 0.1543,
      "step": 273
    },
    {
      "epoch": 0.7345844504021448,
      "grad_norm": 0.05552854761481285,
      "learning_rate": 0.0008530831099195711,
      "loss": 0.1445,
      "step": 274
    },
    {
      "epoch": 0.7372654155495979,
      "grad_norm": 0.062398068606853485,
      "learning_rate": 0.0008525469168900805,
      "loss": 0.1338,
      "step": 275
    },
    {
      "epoch": 0.739946380697051,
      "grad_norm": 0.03844363987445831,
      "learning_rate": 0.0008520107238605899,
      "loss": 0.1475,
      "step": 276
    },
    {
      "epoch": 0.7426273458445041,
      "grad_norm": 0.052530135959386826,
      "learning_rate": 0.0008514745308310992,
      "loss": 0.1338,
      "step": 277
    },
    {
      "epoch": 0.7453083109919572,
      "grad_norm": 0.041506487876176834,
      "learning_rate": 0.0008509383378016086,
      "loss": 0.1187,
      "step": 278
    },
    {
      "epoch": 0.7479892761394102,
      "grad_norm": 0.08080233633518219,
      "learning_rate": 0.000850402144772118,
      "loss": 0.1177,
      "step": 279
    },
    {
      "epoch": 0.7506702412868632,
      "grad_norm": 0.0633867159485817,
      "learning_rate": 0.0008498659517426274,
      "loss": 0.1309,
      "step": 280
    },
    {
      "epoch": 0.7533512064343163,
      "grad_norm": 0.08870905637741089,
      "learning_rate": 0.0008493297587131368,
      "loss": 0.127,
      "step": 281
    },
    {
      "epoch": 0.7560321715817694,
      "grad_norm": 0.03916940093040466,
      "learning_rate": 0.0008487935656836462,
      "loss": 0.1279,
      "step": 282
    },
    {
      "epoch": 0.7587131367292225,
      "grad_norm": 0.10012105107307434,
      "learning_rate": 0.0008482573726541555,
      "loss": 0.1226,
      "step": 283
    },
    {
      "epoch": 0.7613941018766756,
      "grad_norm": 0.046535003930330276,
      "learning_rate": 0.0008477211796246649,
      "loss": 0.127,
      "step": 284
    },
    {
      "epoch": 0.7640750670241286,
      "grad_norm": 0.111423060297966,
      "learning_rate": 0.0008471849865951743,
      "loss": 0.1196,
      "step": 285
    },
    {
      "epoch": 0.7667560321715817,
      "grad_norm": 0.08397562056779861,
      "learning_rate": 0.0008466487935656837,
      "loss": 0.125,
      "step": 286
    },
    {
      "epoch": 0.7694369973190348,
      "grad_norm": 0.044796377420425415,
      "learning_rate": 0.0008461126005361931,
      "loss": 0.1216,
      "step": 287
    },
    {
      "epoch": 0.7721179624664879,
      "grad_norm": 0.12027771025896072,
      "learning_rate": 0.0008455764075067024,
      "loss": 0.1191,
      "step": 288
    },
    {
      "epoch": 0.774798927613941,
      "grad_norm": 0.10016563534736633,
      "learning_rate": 0.0008450402144772118,
      "loss": 0.124,
      "step": 289
    },
    {
      "epoch": 0.7774798927613941,
      "grad_norm": 0.42838335037231445,
      "learning_rate": 0.0008445040214477212,
      "loss": 0.1729,
      "step": 290
    },
    {
      "epoch": 0.7801608579088471,
      "grad_norm": 0.052443262189626694,
      "learning_rate": 0.0008439678284182307,
      "loss": 0.125,
      "step": 291
    },
    {
      "epoch": 0.7828418230563002,
      "grad_norm": 0.08069542795419693,
      "learning_rate": 0.0008434316353887401,
      "loss": 0.1504,
      "step": 292
    },
    {
      "epoch": 0.7855227882037533,
      "grad_norm": 0.03764799237251282,
      "learning_rate": 0.0008428954423592494,
      "loss": 0.1279,
      "step": 293
    },
    {
      "epoch": 0.7882037533512064,
      "grad_norm": 0.1475696861743927,
      "learning_rate": 0.0008423592493297588,
      "loss": 0.1445,
      "step": 294
    },
    {
      "epoch": 0.7908847184986595,
      "grad_norm": 0.07482592761516571,
      "learning_rate": 0.0008418230563002682,
      "loss": 0.1318,
      "step": 295
    },
    {
      "epoch": 0.7935656836461126,
      "grad_norm": 0.06922169774770737,
      "learning_rate": 0.0008412868632707776,
      "loss": 0.1123,
      "step": 296
    },
    {
      "epoch": 0.7962466487935657,
      "grad_norm": 0.05724015459418297,
      "learning_rate": 0.000840750670241287,
      "loss": 0.1211,
      "step": 297
    },
    {
      "epoch": 0.7989276139410187,
      "grad_norm": 0.05015747249126434,
      "learning_rate": 0.0008402144772117963,
      "loss": 0.1436,
      "step": 298
    },
    {
      "epoch": 0.8016085790884718,
      "grad_norm": 0.08031390607357025,
      "learning_rate": 0.0008396782841823057,
      "loss": 0.1328,
      "step": 299
    },
    {
      "epoch": 0.8042895442359249,
      "grad_norm": 0.03802446275949478,
      "learning_rate": 0.0008391420911528151,
      "loss": 0.1216,
      "step": 300
    },
    {
      "epoch": 0.806970509383378,
      "grad_norm": 0.1009891927242279,
      "learning_rate": 0.0008386058981233245,
      "loss": 0.1338,
      "step": 301
    },
    {
      "epoch": 0.8096514745308311,
      "grad_norm": 0.055782414972782135,
      "learning_rate": 0.0008380697050938339,
      "loss": 0.1533,
      "step": 302
    },
    {
      "epoch": 0.8123324396782842,
      "grad_norm": 0.0549689382314682,
      "learning_rate": 0.0008375335120643432,
      "loss": 0.1211,
      "step": 303
    },
    {
      "epoch": 0.8150134048257373,
      "grad_norm": 0.05988747254014015,
      "learning_rate": 0.0008369973190348526,
      "loss": 0.1201,
      "step": 304
    },
    {
      "epoch": 0.8176943699731903,
      "grad_norm": 0.05380743369460106,
      "learning_rate": 0.000836461126005362,
      "loss": 0.1533,
      "step": 305
    },
    {
      "epoch": 0.8203753351206434,
      "grad_norm": 0.06101953983306885,
      "learning_rate": 0.0008359249329758714,
      "loss": 0.1196,
      "step": 306
    },
    {
      "epoch": 0.8230563002680965,
      "grad_norm": 0.0857744961977005,
      "learning_rate": 0.0008353887399463808,
      "loss": 0.1436,
      "step": 307
    },
    {
      "epoch": 0.8257372654155496,
      "grad_norm": 0.04452688992023468,
      "learning_rate": 0.0008348525469168901,
      "loss": 0.127,
      "step": 308
    },
    {
      "epoch": 0.8284182305630027,
      "grad_norm": 0.0668758824467659,
      "learning_rate": 0.0008343163538873995,
      "loss": 0.1377,
      "step": 309
    },
    {
      "epoch": 0.8310991957104558,
      "grad_norm": 0.05803262069821358,
      "learning_rate": 0.0008337801608579089,
      "loss": 0.1465,
      "step": 310
    },
    {
      "epoch": 0.8337801608579088,
      "grad_norm": 0.14284424483776093,
      "learning_rate": 0.0008332439678284183,
      "loss": 0.1367,
      "step": 311
    },
    {
      "epoch": 0.8364611260053619,
      "grad_norm": 0.043850041925907135,
      "learning_rate": 0.0008327077747989277,
      "loss": 0.127,
      "step": 312
    },
    {
      "epoch": 0.839142091152815,
      "grad_norm": 0.05412169173359871,
      "learning_rate": 0.0008321715817694369,
      "loss": 0.1211,
      "step": 313
    },
    {
      "epoch": 0.8418230563002681,
      "grad_norm": 0.05096200853586197,
      "learning_rate": 0.0008316353887399463,
      "loss": 0.1113,
      "step": 314
    },
    {
      "epoch": 0.8445040214477212,
      "grad_norm": 0.043587151914834976,
      "learning_rate": 0.0008310991957104557,
      "loss": 0.1484,
      "step": 315
    },
    {
      "epoch": 0.8471849865951743,
      "grad_norm": 0.06496094167232513,
      "learning_rate": 0.0008305630026809651,
      "loss": 0.105,
      "step": 316
    },
    {
      "epoch": 0.8498659517426274,
      "grad_norm": 0.05088473856449127,
      "learning_rate": 0.0008300268096514745,
      "loss": 0.1338,
      "step": 317
    },
    {
      "epoch": 0.8525469168900804,
      "grad_norm": 0.03873397037386894,
      "learning_rate": 0.0008294906166219838,
      "loss": 0.126,
      "step": 318
    },
    {
      "epoch": 0.8552278820375335,
      "grad_norm": 0.044868286699056625,
      "learning_rate": 0.0008289544235924932,
      "loss": 0.1152,
      "step": 319
    },
    {
      "epoch": 0.8579088471849866,
      "grad_norm": 0.05869198963046074,
      "learning_rate": 0.0008284182305630026,
      "loss": 0.1152,
      "step": 320
    },
    {
      "epoch": 0.8605898123324397,
      "grad_norm": 0.040986642241477966,
      "learning_rate": 0.000827882037533512,
      "loss": 0.1426,
      "step": 321
    },
    {
      "epoch": 0.8632707774798928,
      "grad_norm": 0.05536036565899849,
      "learning_rate": 0.0008273458445040215,
      "loss": 0.1299,
      "step": 322
    },
    {
      "epoch": 0.8659517426273459,
      "grad_norm": 0.04869738593697548,
      "learning_rate": 0.0008268096514745308,
      "loss": 0.1328,
      "step": 323
    },
    {
      "epoch": 0.868632707774799,
      "grad_norm": 0.052299272269010544,
      "learning_rate": 0.0008262734584450402,
      "loss": 0.1226,
      "step": 324
    },
    {
      "epoch": 0.871313672922252,
      "grad_norm": 0.060558613389730453,
      "learning_rate": 0.0008257372654155496,
      "loss": 0.1108,
      "step": 325
    },
    {
      "epoch": 0.8739946380697051,
      "grad_norm": 0.04293038323521614,
      "learning_rate": 0.000825201072386059,
      "loss": 0.0933,
      "step": 326
    },
    {
      "epoch": 0.8766756032171582,
      "grad_norm": 0.07904618978500366,
      "learning_rate": 0.0008246648793565684,
      "loss": 0.1338,
      "step": 327
    },
    {
      "epoch": 0.8793565683646113,
      "grad_norm": 0.06765228509902954,
      "learning_rate": 0.0008241286863270777,
      "loss": 0.1504,
      "step": 328
    },
    {
      "epoch": 0.8820375335120644,
      "grad_norm": 0.08700711280107498,
      "learning_rate": 0.0008235924932975871,
      "loss": 0.1445,
      "step": 329
    },
    {
      "epoch": 0.8847184986595175,
      "grad_norm": 0.12562793493270874,
      "learning_rate": 0.0008230563002680965,
      "loss": 0.1123,
      "step": 330
    },
    {
      "epoch": 0.8873994638069705,
      "grad_norm": 0.044028472155332565,
      "learning_rate": 0.0008225201072386059,
      "loss": 0.1094,
      "step": 331
    },
    {
      "epoch": 0.8900804289544236,
      "grad_norm": 0.039789676666259766,
      "learning_rate": 0.0008219839142091153,
      "loss": 0.1206,
      "step": 332
    },
    {
      "epoch": 0.8927613941018767,
      "grad_norm": 0.05778714641928673,
      "learning_rate": 0.0008214477211796246,
      "loss": 0.1108,
      "step": 333
    },
    {
      "epoch": 0.8954423592493298,
      "grad_norm": 0.03794701024889946,
      "learning_rate": 0.000820911528150134,
      "loss": 0.1182,
      "step": 334
    },
    {
      "epoch": 0.8981233243967829,
      "grad_norm": 0.039037540555000305,
      "learning_rate": 0.0008203753351206434,
      "loss": 0.1104,
      "step": 335
    },
    {
      "epoch": 0.900804289544236,
      "grad_norm": 0.06138968467712402,
      "learning_rate": 0.0008198391420911528,
      "loss": 0.125,
      "step": 336
    },
    {
      "epoch": 0.903485254691689,
      "grad_norm": 0.0952741801738739,
      "learning_rate": 0.0008193029490616622,
      "loss": 0.1133,
      "step": 337
    },
    {
      "epoch": 0.9061662198391421,
      "grad_norm": 0.0911797434091568,
      "learning_rate": 0.0008187667560321715,
      "loss": 0.1216,
      "step": 338
    },
    {
      "epoch": 0.9088471849865952,
      "grad_norm": 0.04408690705895424,
      "learning_rate": 0.0008182305630026809,
      "loss": 0.1416,
      "step": 339
    },
    {
      "epoch": 0.9115281501340483,
      "grad_norm": 0.04354093596339226,
      "learning_rate": 0.0008176943699731903,
      "loss": 0.1328,
      "step": 340
    },
    {
      "epoch": 0.9142091152815014,
      "grad_norm": 0.04691474512219429,
      "learning_rate": 0.0008171581769436997,
      "loss": 0.1191,
      "step": 341
    },
    {
      "epoch": 0.9168900804289544,
      "grad_norm": 0.03107368014752865,
      "learning_rate": 0.0008166219839142091,
      "loss": 0.1089,
      "step": 342
    },
    {
      "epoch": 0.9195710455764075,
      "grad_norm": 0.07571236044168472,
      "learning_rate": 0.0008160857908847185,
      "loss": 0.1328,
      "step": 343
    },
    {
      "epoch": 0.9222520107238605,
      "grad_norm": 0.06326805800199509,
      "learning_rate": 0.0008155495978552278,
      "loss": 0.125,
      "step": 344
    },
    {
      "epoch": 0.9249329758713136,
      "grad_norm": 0.03810399770736694,
      "learning_rate": 0.0008150134048257372,
      "loss": 0.1289,
      "step": 345
    },
    {
      "epoch": 0.9276139410187667,
      "grad_norm": 0.04458208382129669,
      "learning_rate": 0.0008144772117962466,
      "loss": 0.126,
      "step": 346
    },
    {
      "epoch": 0.9302949061662198,
      "grad_norm": 0.04252784326672554,
      "learning_rate": 0.000813941018766756,
      "loss": 0.1089,
      "step": 347
    },
    {
      "epoch": 0.9329758713136729,
      "grad_norm": 0.04729359596967697,
      "learning_rate": 0.0008134048257372654,
      "loss": 0.1055,
      "step": 348
    },
    {
      "epoch": 0.935656836461126,
      "grad_norm": 0.07467181235551834,
      "learning_rate": 0.0008128686327077747,
      "loss": 0.1318,
      "step": 349
    },
    {
      "epoch": 0.938337801608579,
      "grad_norm": 0.03208956494927406,
      "learning_rate": 0.0008123324396782842,
      "loss": 0.1216,
      "step": 350
    },
    {
      "epoch": 0.9410187667560321,
      "grad_norm": 0.03872699663043022,
      "learning_rate": 0.0008117962466487936,
      "loss": 0.1118,
      "step": 351
    },
    {
      "epoch": 0.9436997319034852,
      "grad_norm": 0.11080144345760345,
      "learning_rate": 0.000811260053619303,
      "loss": 0.1514,
      "step": 352
    },
    {
      "epoch": 0.9463806970509383,
      "grad_norm": 0.050729699432849884,
      "learning_rate": 0.0008107238605898124,
      "loss": 0.1182,
      "step": 353
    },
    {
      "epoch": 0.9490616621983914,
      "grad_norm": 0.03472788259387016,
      "learning_rate": 0.0008101876675603217,
      "loss": 0.125,
      "step": 354
    },
    {
      "epoch": 0.9517426273458445,
      "grad_norm": 0.03542723506689072,
      "learning_rate": 0.0008096514745308311,
      "loss": 0.0947,
      "step": 355
    },
    {
      "epoch": 0.9544235924932976,
      "grad_norm": 0.03658240661025047,
      "learning_rate": 0.0008091152815013405,
      "loss": 0.1299,
      "step": 356
    },
    {
      "epoch": 0.9571045576407506,
      "grad_norm": 0.042796600610017776,
      "learning_rate": 0.0008085790884718499,
      "loss": 0.1167,
      "step": 357
    },
    {
      "epoch": 0.9597855227882037,
      "grad_norm": 0.045325081795454025,
      "learning_rate": 0.0008080428954423593,
      "loss": 0.1289,
      "step": 358
    },
    {
      "epoch": 0.9624664879356568,
      "grad_norm": 0.04898901283740997,
      "learning_rate": 0.0008075067024128686,
      "loss": 0.1226,
      "step": 359
    },
    {
      "epoch": 0.9651474530831099,
      "grad_norm": 0.05655749887228012,
      "learning_rate": 0.000806970509383378,
      "loss": 0.0986,
      "step": 360
    },
    {
      "epoch": 0.967828418230563,
      "grad_norm": 0.11963232606649399,
      "learning_rate": 0.0008064343163538874,
      "loss": 0.1396,
      "step": 361
    },
    {
      "epoch": 0.9705093833780161,
      "grad_norm": 0.08576340228319168,
      "learning_rate": 0.0008058981233243968,
      "loss": 0.1475,
      "step": 362
    },
    {
      "epoch": 0.9731903485254692,
      "grad_norm": 0.19159342348575592,
      "learning_rate": 0.0008053619302949062,
      "loss": 0.1035,
      "step": 363
    },
    {
      "epoch": 0.9758713136729222,
      "grad_norm": 0.08273222297430038,
      "learning_rate": 0.0008048257372654155,
      "loss": 0.1177,
      "step": 364
    },
    {
      "epoch": 0.9785522788203753,
      "grad_norm": 0.06683661788702011,
      "learning_rate": 0.0008042895442359249,
      "loss": 0.1113,
      "step": 365
    },
    {
      "epoch": 0.9812332439678284,
      "grad_norm": 0.05177105590701103,
      "learning_rate": 0.0008037533512064343,
      "loss": 0.1289,
      "step": 366
    },
    {
      "epoch": 0.9839142091152815,
      "grad_norm": 0.10126637667417526,
      "learning_rate": 0.0008032171581769437,
      "loss": 0.1309,
      "step": 367
    },
    {
      "epoch": 0.9865951742627346,
      "grad_norm": 0.039314959198236465,
      "learning_rate": 0.0008026809651474531,
      "loss": 0.126,
      "step": 368
    },
    {
      "epoch": 0.9892761394101877,
      "grad_norm": 0.05990242585539818,
      "learning_rate": 0.0008021447721179624,
      "loss": 0.1069,
      "step": 369
    },
    {
      "epoch": 0.9919571045576407,
      "grad_norm": 0.06147317588329315,
      "learning_rate": 0.0008016085790884718,
      "loss": 0.1157,
      "step": 370
    },
    {
      "epoch": 0.9946380697050938,
      "grad_norm": 0.07681682705879211,
      "learning_rate": 0.0008010723860589812,
      "loss": 0.1113,
      "step": 371
    },
    {
      "epoch": 0.9973190348525469,
      "grad_norm": 0.11717309802770615,
      "learning_rate": 0.0008005361930294906,
      "loss": 0.126,
      "step": 372
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.0514214001595974,
      "learning_rate": 0.0008,
      "loss": 0.1245,
      "step": 373
    },
    {
      "epoch": 1.002680965147453,
      "grad_norm": 0.09134218841791153,
      "learning_rate": 0.0007994638069705093,
      "loss": 0.126,
      "step": 374
    },
    {
      "epoch": 1.0053619302949062,
      "grad_norm": 0.04214397445321083,
      "learning_rate": 0.0007989276139410187,
      "loss": 0.1089,
      "step": 375
    },
    {
      "epoch": 1.0080428954423593,
      "grad_norm": 0.046443305909633636,
      "learning_rate": 0.0007983914209115281,
      "loss": 0.1348,
      "step": 376
    },
    {
      "epoch": 1.0107238605898123,
      "grad_norm": 0.053925734013319016,
      "learning_rate": 0.0007978552278820376,
      "loss": 0.106,
      "step": 377
    },
    {
      "epoch": 1.0134048257372654,
      "grad_norm": 0.04247036948800087,
      "learning_rate": 0.000797319034852547,
      "loss": 0.1318,
      "step": 378
    },
    {
      "epoch": 1.0160857908847185,
      "grad_norm": 0.04120238497853279,
      "learning_rate": 0.0007967828418230563,
      "loss": 0.1128,
      "step": 379
    },
    {
      "epoch": 1.0187667560321716,
      "grad_norm": 0.04126602038741112,
      "learning_rate": 0.0007962466487935657,
      "loss": 0.1377,
      "step": 380
    },
    {
      "epoch": 1.0214477211796247,
      "grad_norm": 0.03885405510663986,
      "learning_rate": 0.0007957104557640751,
      "loss": 0.0952,
      "step": 381
    },
    {
      "epoch": 1.0241286863270778,
      "grad_norm": 0.04503221809864044,
      "learning_rate": 0.0007951742627345845,
      "loss": 0.1152,
      "step": 382
    },
    {
      "epoch": 1.0268096514745308,
      "grad_norm": 0.054775744676589966,
      "learning_rate": 0.0007946380697050939,
      "loss": 0.1338,
      "step": 383
    },
    {
      "epoch": 1.029490616621984,
      "grad_norm": 0.031669747084379196,
      "learning_rate": 0.0007941018766756032,
      "loss": 0.0918,
      "step": 384
    },
    {
      "epoch": 1.032171581769437,
      "grad_norm": 0.08022021502256393,
      "learning_rate": 0.0007935656836461126,
      "loss": 0.1157,
      "step": 385
    },
    {
      "epoch": 1.03485254691689,
      "grad_norm": 0.045495133846998215,
      "learning_rate": 0.000793029490616622,
      "loss": 0.1045,
      "step": 386
    },
    {
      "epoch": 1.0375335120643432,
      "grad_norm": 0.07396295666694641,
      "learning_rate": 0.0007924932975871314,
      "loss": 0.1436,
      "step": 387
    },
    {
      "epoch": 1.0402144772117963,
      "grad_norm": 0.06620173901319504,
      "learning_rate": 0.0007919571045576408,
      "loss": 0.1367,
      "step": 388
    },
    {
      "epoch": 1.0428954423592494,
      "grad_norm": 0.05925356596708298,
      "learning_rate": 0.0007914209115281501,
      "loss": 0.1099,
      "step": 389
    },
    {
      "epoch": 1.0455764075067024,
      "grad_norm": 0.049868494272232056,
      "learning_rate": 0.0007908847184986595,
      "loss": 0.125,
      "step": 390
    },
    {
      "epoch": 1.0482573726541555,
      "grad_norm": 0.036422017961740494,
      "learning_rate": 0.0007903485254691689,
      "loss": 0.1318,
      "step": 391
    },
    {
      "epoch": 1.0509383378016086,
      "grad_norm": 0.04537982866168022,
      "learning_rate": 0.0007898123324396783,
      "loss": 0.1172,
      "step": 392
    },
    {
      "epoch": 1.0536193029490617,
      "grad_norm": 0.05305970087647438,
      "learning_rate": 0.0007892761394101877,
      "loss": 0.1152,
      "step": 393
    },
    {
      "epoch": 1.0563002680965148,
      "grad_norm": 0.16633930802345276,
      "learning_rate": 0.000788739946380697,
      "loss": 0.1152,
      "step": 394
    },
    {
      "epoch": 1.0589812332439679,
      "grad_norm": 0.06072550639510155,
      "learning_rate": 0.0007882037533512064,
      "loss": 0.1152,
      "step": 395
    },
    {
      "epoch": 1.061662198391421,
      "grad_norm": 0.04000595957040787,
      "learning_rate": 0.0007876675603217158,
      "loss": 0.0933,
      "step": 396
    },
    {
      "epoch": 1.064343163538874,
      "grad_norm": 0.05180668830871582,
      "learning_rate": 0.0007871313672922252,
      "loss": 0.1377,
      "step": 397
    },
    {
      "epoch": 1.0670241286863271,
      "grad_norm": 0.04465843364596367,
      "learning_rate": 0.0007865951742627346,
      "loss": 0.1118,
      "step": 398
    },
    {
      "epoch": 1.0697050938337802,
      "grad_norm": 0.051155298948287964,
      "learning_rate": 0.0007860589812332439,
      "loss": 0.0977,
      "step": 399
    },
    {
      "epoch": 1.0723860589812333,
      "grad_norm": 0.049843739718198776,
      "learning_rate": 0.0007855227882037533,
      "loss": 0.1191,
      "step": 400
    },
    {
      "epoch": 1.0750670241286864,
      "grad_norm": 0.06626590341329575,
      "learning_rate": 0.0007849865951742627,
      "loss": 0.1108,
      "step": 401
    },
    {
      "epoch": 1.0777479892761395,
      "grad_norm": 0.040575362741947174,
      "learning_rate": 0.0007844504021447721,
      "loss": 0.1377,
      "step": 402
    },
    {
      "epoch": 1.0804289544235925,
      "grad_norm": 0.040420133620500565,
      "learning_rate": 0.0007839142091152815,
      "loss": 0.1396,
      "step": 403
    },
    {
      "epoch": 1.0831099195710456,
      "grad_norm": 0.034759487956762314,
      "learning_rate": 0.0007833780160857908,
      "loss": 0.1108,
      "step": 404
    },
    {
      "epoch": 1.0857908847184987,
      "grad_norm": 0.03712080791592598,
      "learning_rate": 0.0007828418230563002,
      "loss": 0.1045,
      "step": 405
    },
    {
      "epoch": 1.0884718498659518,
      "grad_norm": 0.06715606153011322,
      "learning_rate": 0.0007823056300268097,
      "loss": 0.0967,
      "step": 406
    },
    {
      "epoch": 1.0911528150134049,
      "grad_norm": 0.03446543961763382,
      "learning_rate": 0.0007817694369973191,
      "loss": 0.1001,
      "step": 407
    },
    {
      "epoch": 1.093833780160858,
      "grad_norm": 0.10375390201807022,
      "learning_rate": 0.0007812332439678285,
      "loss": 0.1128,
      "step": 408
    },
    {
      "epoch": 1.096514745308311,
      "grad_norm": 0.051837362349033356,
      "learning_rate": 0.0007806970509383379,
      "loss": 0.1064,
      "step": 409
    },
    {
      "epoch": 1.0991957104557641,
      "grad_norm": 0.12197627872228622,
      "learning_rate": 0.0007801608579088472,
      "loss": 0.1177,
      "step": 410
    },
    {
      "epoch": 1.1018766756032172,
      "grad_norm": 0.3301435112953186,
      "learning_rate": 0.0007796246648793566,
      "loss": 0.1157,
      "step": 411
    },
    {
      "epoch": 1.1045576407506703,
      "grad_norm": 0.06505794078111649,
      "learning_rate": 0.000779088471849866,
      "loss": 0.0903,
      "step": 412
    },
    {
      "epoch": 1.1072386058981234,
      "grad_norm": 0.04316914454102516,
      "learning_rate": 0.0007785522788203754,
      "loss": 0.1104,
      "step": 413
    },
    {
      "epoch": 1.1099195710455765,
      "grad_norm": 0.05794845148921013,
      "learning_rate": 0.0007780160857908848,
      "loss": 0.0938,
      "step": 414
    },
    {
      "epoch": 1.1126005361930296,
      "grad_norm": 0.042976390570402145,
      "learning_rate": 0.0007774798927613941,
      "loss": 0.104,
      "step": 415
    },
    {
      "epoch": 1.1152815013404827,
      "grad_norm": 0.04087655991315842,
      "learning_rate": 0.0007769436997319035,
      "loss": 0.1216,
      "step": 416
    },
    {
      "epoch": 1.1179624664879357,
      "grad_norm": 0.04101721569895744,
      "learning_rate": 0.0007764075067024129,
      "loss": 0.1104,
      "step": 417
    },
    {
      "epoch": 1.1206434316353888,
      "grad_norm": 0.0990547388792038,
      "learning_rate": 0.0007758713136729223,
      "loss": 0.1147,
      "step": 418
    },
    {
      "epoch": 1.123324396782842,
      "grad_norm": 0.06716539710760117,
      "learning_rate": 0.0007753351206434317,
      "loss": 0.0996,
      "step": 419
    },
    {
      "epoch": 1.126005361930295,
      "grad_norm": 0.08820836991071701,
      "learning_rate": 0.000774798927613941,
      "loss": 0.1387,
      "step": 420
    },
    {
      "epoch": 1.128686327077748,
      "grad_norm": 0.06045705825090408,
      "learning_rate": 0.0007742627345844504,
      "loss": 0.127,
      "step": 421
    },
    {
      "epoch": 1.1313672922252012,
      "grad_norm": 0.0752866119146347,
      "learning_rate": 0.0007737265415549598,
      "loss": 0.104,
      "step": 422
    },
    {
      "epoch": 1.1340482573726542,
      "grad_norm": 0.11385703831911087,
      "learning_rate": 0.0007731903485254692,
      "loss": 0.1128,
      "step": 423
    },
    {
      "epoch": 1.1367292225201073,
      "grad_norm": 0.04446065425872803,
      "learning_rate": 0.0007726541554959786,
      "loss": 0.124,
      "step": 424
    },
    {
      "epoch": 1.1394101876675604,
      "grad_norm": 0.10648512095212936,
      "learning_rate": 0.0007721179624664879,
      "loss": 0.1104,
      "step": 425
    },
    {
      "epoch": 1.1420911528150135,
      "grad_norm": 0.0577947162091732,
      "learning_rate": 0.0007715817694369973,
      "loss": 0.1211,
      "step": 426
    },
    {
      "epoch": 1.1447721179624666,
      "grad_norm": 0.07637406140565872,
      "learning_rate": 0.0007710455764075067,
      "loss": 0.1309,
      "step": 427
    },
    {
      "epoch": 1.1474530831099194,
      "grad_norm": 0.13297788798809052,
      "learning_rate": 0.0007705093833780161,
      "loss": 0.1094,
      "step": 428
    },
    {
      "epoch": 1.1501340482573728,
      "grad_norm": 0.03967242315411568,
      "learning_rate": 0.0007699731903485255,
      "loss": 0.1196,
      "step": 429
    },
    {
      "epoch": 1.1528150134048256,
      "grad_norm": 0.05673348531126976,
      "learning_rate": 0.0007694369973190348,
      "loss": 0.1289,
      "step": 430
    },
    {
      "epoch": 1.155495978552279,
      "grad_norm": 0.08055753260850906,
      "learning_rate": 0.0007689008042895442,
      "loss": 0.1079,
      "step": 431
    },
    {
      "epoch": 1.1581769436997318,
      "grad_norm": 0.061955470591783524,
      "learning_rate": 0.0007683646112600536,
      "loss": 0.1128,
      "step": 432
    },
    {
      "epoch": 1.160857908847185,
      "grad_norm": 0.04938064143061638,
      "learning_rate": 0.000767828418230563,
      "loss": 0.1396,
      "step": 433
    },
    {
      "epoch": 1.163538873994638,
      "grad_norm": 0.03912496194243431,
      "learning_rate": 0.0007672922252010725,
      "loss": 0.0845,
      "step": 434
    },
    {
      "epoch": 1.1662198391420913,
      "grad_norm": 0.06021473929286003,
      "learning_rate": 0.0007667560321715818,
      "loss": 0.1211,
      "step": 435
    },
    {
      "epoch": 1.1689008042895441,
      "grad_norm": 0.09824032336473465,
      "learning_rate": 0.0007662198391420912,
      "loss": 0.105,
      "step": 436
    },
    {
      "epoch": 1.1715817694369974,
      "grad_norm": 0.04695698246359825,
      "learning_rate": 0.0007656836461126006,
      "loss": 0.1074,
      "step": 437
    },
    {
      "epoch": 1.1742627345844503,
      "grad_norm": 0.03620569407939911,
      "learning_rate": 0.00076514745308311,
      "loss": 0.1025,
      "step": 438
    },
    {
      "epoch": 1.1769436997319036,
      "grad_norm": 0.048227984458208084,
      "learning_rate": 0.0007646112600536194,
      "loss": 0.0894,
      "step": 439
    },
    {
      "epoch": 1.1796246648793565,
      "grad_norm": 0.05152279511094093,
      "learning_rate": 0.0007640750670241287,
      "loss": 0.1138,
      "step": 440
    },
    {
      "epoch": 1.1823056300268098,
      "grad_norm": 0.052527353167533875,
      "learning_rate": 0.0007635388739946381,
      "loss": 0.1011,
      "step": 441
    },
    {
      "epoch": 1.1849865951742626,
      "grad_norm": 0.05657774582505226,
      "learning_rate": 0.0007630026809651475,
      "loss": 0.1147,
      "step": 442
    },
    {
      "epoch": 1.1876675603217157,
      "grad_norm": 0.06153494119644165,
      "learning_rate": 0.0007624664879356569,
      "loss": 0.1138,
      "step": 443
    },
    {
      "epoch": 1.1903485254691688,
      "grad_norm": 0.036200109869241714,
      "learning_rate": 0.0007619302949061663,
      "loss": 0.1079,
      "step": 444
    },
    {
      "epoch": 1.193029490616622,
      "grad_norm": 0.06986283510923386,
      "learning_rate": 0.0007613941018766756,
      "loss": 0.1094,
      "step": 445
    },
    {
      "epoch": 1.195710455764075,
      "grad_norm": 0.046245086938142776,
      "learning_rate": 0.000760857908847185,
      "loss": 0.0947,
      "step": 446
    },
    {
      "epoch": 1.198391420911528,
      "grad_norm": 0.03321978449821472,
      "learning_rate": 0.0007603217158176944,
      "loss": 0.1118,
      "step": 447
    },
    {
      "epoch": 1.2010723860589811,
      "grad_norm": 0.03489430621266365,
      "learning_rate": 0.0007597855227882038,
      "loss": 0.1133,
      "step": 448
    },
    {
      "epoch": 1.2037533512064342,
      "grad_norm": 0.040769703686237335,
      "learning_rate": 0.0007592493297587132,
      "loss": 0.1094,
      "step": 449
    },
    {
      "epoch": 1.2064343163538873,
      "grad_norm": 0.07447294890880585,
      "learning_rate": 0.0007587131367292225,
      "loss": 0.1367,
      "step": 450
    },
    {
      "epoch": 1.2091152815013404,
      "grad_norm": 0.035061147063970566,
      "learning_rate": 0.0007581769436997319,
      "loss": 0.1289,
      "step": 451
    },
    {
      "epoch": 1.2117962466487935,
      "grad_norm": 0.04936068132519722,
      "learning_rate": 0.0007576407506702413,
      "loss": 0.127,
      "step": 452
    },
    {
      "epoch": 1.2144772117962466,
      "grad_norm": 0.06961347162723541,
      "learning_rate": 0.0007571045576407507,
      "loss": 0.1157,
      "step": 453
    },
    {
      "epoch": 1.2171581769436997,
      "grad_norm": 0.06500349193811417,
      "learning_rate": 0.0007565683646112601,
      "loss": 0.1123,
      "step": 454
    },
    {
      "epoch": 1.2198391420911527,
      "grad_norm": 0.05681437999010086,
      "learning_rate": 0.0007560321715817694,
      "loss": 0.1064,
      "step": 455
    },
    {
      "epoch": 1.2225201072386058,
      "grad_norm": 0.042462870478630066,
      "learning_rate": 0.0007554959785522788,
      "loss": 0.1089,
      "step": 456
    },
    {
      "epoch": 1.225201072386059,
      "grad_norm": 0.031834717839956284,
      "learning_rate": 0.0007549597855227882,
      "loss": 0.0928,
      "step": 457
    },
    {
      "epoch": 1.227882037533512,
      "grad_norm": 0.058806296437978745,
      "learning_rate": 0.0007544235924932976,
      "loss": 0.0796,
      "step": 458
    },
    {
      "epoch": 1.230563002680965,
      "grad_norm": 0.11173803359270096,
      "learning_rate": 0.000753887399463807,
      "loss": 0.1089,
      "step": 459
    },
    {
      "epoch": 1.2332439678284182,
      "grad_norm": 0.05673281103372574,
      "learning_rate": 0.0007533512064343163,
      "loss": 0.1299,
      "step": 460
    },
    {
      "epoch": 1.2359249329758712,
      "grad_norm": 0.03993743285536766,
      "learning_rate": 0.0007528150134048257,
      "loss": 0.0933,
      "step": 461
    },
    {
      "epoch": 1.2386058981233243,
      "grad_norm": 0.03976181149482727,
      "learning_rate": 0.0007522788203753352,
      "loss": 0.1113,
      "step": 462
    },
    {
      "epoch": 1.2412868632707774,
      "grad_norm": 0.060153573751449585,
      "learning_rate": 0.0007517426273458446,
      "loss": 0.1084,
      "step": 463
    },
    {
      "epoch": 1.2439678284182305,
      "grad_norm": 0.0652385801076889,
      "learning_rate": 0.000751206434316354,
      "loss": 0.0962,
      "step": 464
    },
    {
      "epoch": 1.2466487935656836,
      "grad_norm": 0.04295577481389046,
      "learning_rate": 0.0007506702412868633,
      "loss": 0.0957,
      "step": 465
    },
    {
      "epoch": 1.2493297587131367,
      "grad_norm": 0.03654337674379349,
      "learning_rate": 0.0007501340482573727,
      "loss": 0.1064,
      "step": 466
    },
    {
      "epoch": 1.2520107238605898,
      "grad_norm": 0.03914329409599304,
      "learning_rate": 0.0007495978552278821,
      "loss": 0.1011,
      "step": 467
    },
    {
      "epoch": 1.2546916890080428,
      "grad_norm": 0.055491309612989426,
      "learning_rate": 0.0007490616621983915,
      "loss": 0.0991,
      "step": 468
    },
    {
      "epoch": 1.257372654155496,
      "grad_norm": 0.14385013282299042,
      "learning_rate": 0.0007485254691689009,
      "loss": 0.1104,
      "step": 469
    },
    {
      "epoch": 1.260053619302949,
      "grad_norm": 0.041834477335214615,
      "learning_rate": 0.0007479892761394103,
      "loss": 0.1348,
      "step": 470
    },
    {
      "epoch": 1.262734584450402,
      "grad_norm": 0.04362652078270912,
      "learning_rate": 0.0007474530831099196,
      "loss": 0.1133,
      "step": 471
    },
    {
      "epoch": 1.2654155495978552,
      "grad_norm": 0.03786700591444969,
      "learning_rate": 0.000746916890080429,
      "loss": 0.0776,
      "step": 472
    },
    {
      "epoch": 1.2680965147453083,
      "grad_norm": 0.05023612454533577,
      "learning_rate": 0.0007463806970509384,
      "loss": 0.1035,
      "step": 473
    },
    {
      "epoch": 1.2707774798927614,
      "grad_norm": 0.06704123318195343,
      "learning_rate": 0.0007458445040214478,
      "loss": 0.1045,
      "step": 474
    },
    {
      "epoch": 1.2734584450402144,
      "grad_norm": 0.03324117138981819,
      "learning_rate": 0.0007453083109919572,
      "loss": 0.0981,
      "step": 475
    },
    {
      "epoch": 1.2761394101876675,
      "grad_norm": 0.08028421550989151,
      "learning_rate": 0.0007447721179624665,
      "loss": 0.1128,
      "step": 476
    },
    {
      "epoch": 1.2788203753351206,
      "grad_norm": 0.04832794889807701,
      "learning_rate": 0.0007442359249329759,
      "loss": 0.0981,
      "step": 477
    },
    {
      "epoch": 1.2815013404825737,
      "grad_norm": 0.058108240365982056,
      "learning_rate": 0.0007436997319034853,
      "loss": 0.0815,
      "step": 478
    },
    {
      "epoch": 1.2841823056300268,
      "grad_norm": 0.04811503365635872,
      "learning_rate": 0.0007431635388739947,
      "loss": 0.127,
      "step": 479
    },
    {
      "epoch": 1.2868632707774799,
      "grad_norm": 0.041951317340135574,
      "learning_rate": 0.0007426273458445041,
      "loss": 0.1021,
      "step": 480
    },
    {
      "epoch": 1.289544235924933,
      "grad_norm": 0.0333263985812664,
      "learning_rate": 0.0007420911528150134,
      "loss": 0.0981,
      "step": 481
    },
    {
      "epoch": 1.292225201072386,
      "grad_norm": 0.04993249475955963,
      "learning_rate": 0.0007415549597855228,
      "loss": 0.1001,
      "step": 482
    },
    {
      "epoch": 1.2949061662198391,
      "grad_norm": 0.0402793362736702,
      "learning_rate": 0.0007410187667560322,
      "loss": 0.0864,
      "step": 483
    },
    {
      "epoch": 1.2975871313672922,
      "grad_norm": 0.04187599569559097,
      "learning_rate": 0.0007404825737265416,
      "loss": 0.0781,
      "step": 484
    },
    {
      "epoch": 1.3002680965147453,
      "grad_norm": 0.032300300896167755,
      "learning_rate": 0.000739946380697051,
      "loss": 0.1094,
      "step": 485
    },
    {
      "epoch": 1.3029490616621984,
      "grad_norm": 0.0479496531188488,
      "learning_rate": 0.0007394101876675603,
      "loss": 0.0952,
      "step": 486
    },
    {
      "epoch": 1.3056300268096515,
      "grad_norm": 0.1377042680978775,
      "learning_rate": 0.0007388739946380697,
      "loss": 0.1216,
      "step": 487
    },
    {
      "epoch": 1.3083109919571045,
      "grad_norm": 0.0718507319688797,
      "learning_rate": 0.0007383378016085791,
      "loss": 0.0933,
      "step": 488
    },
    {
      "epoch": 1.3109919571045576,
      "grad_norm": 0.06362006068229675,
      "learning_rate": 0.0007378016085790886,
      "loss": 0.1143,
      "step": 489
    },
    {
      "epoch": 1.3136729222520107,
      "grad_norm": 0.059259604662656784,
      "learning_rate": 0.000737265415549598,
      "loss": 0.0952,
      "step": 490
    },
    {
      "epoch": 1.3163538873994638,
      "grad_norm": 0.048544492572546005,
      "learning_rate": 0.0007367292225201073,
      "loss": 0.1187,
      "step": 491
    },
    {
      "epoch": 1.3190348525469169,
      "grad_norm": 0.054381363093853,
      "learning_rate": 0.0007361930294906167,
      "loss": 0.0947,
      "step": 492
    },
    {
      "epoch": 1.32171581769437,
      "grad_norm": 0.06608201563358307,
      "learning_rate": 0.0007356568364611261,
      "loss": 0.1069,
      "step": 493
    },
    {
      "epoch": 1.324396782841823,
      "grad_norm": 0.04577617347240448,
      "learning_rate": 0.0007351206434316355,
      "loss": 0.1045,
      "step": 494
    },
    {
      "epoch": 1.3270777479892761,
      "grad_norm": 0.04107692092657089,
      "learning_rate": 0.0007345844504021449,
      "loss": 0.123,
      "step": 495
    },
    {
      "epoch": 1.3297587131367292,
      "grad_norm": 0.04668464511632919,
      "learning_rate": 0.0007340482573726542,
      "loss": 0.0806,
      "step": 496
    },
    {
      "epoch": 1.3324396782841823,
      "grad_norm": 0.04721759259700775,
      "learning_rate": 0.0007335120643431636,
      "loss": 0.0933,
      "step": 497
    },
    {
      "epoch": 1.3351206434316354,
      "grad_norm": 0.0504222996532917,
      "learning_rate": 0.000732975871313673,
      "loss": 0.1099,
      "step": 498
    },
    {
      "epoch": 1.3378016085790885,
      "grad_norm": 0.07776026427745819,
      "learning_rate": 0.0007324396782841824,
      "loss": 0.1011,
      "step": 499
    },
    {
      "epoch": 1.3404825737265416,
      "grad_norm": 0.04930930212140083,
      "learning_rate": 0.0007319034852546918,
      "loss": 0.1138,
      "step": 500
    },
    {
      "epoch": 1.3431635388739946,
      "grad_norm": 0.05139215290546417,
      "learning_rate": 0.0007313672922252011,
      "loss": 0.1016,
      "step": 501
    },
    {
      "epoch": 1.3458445040214477,
      "grad_norm": 0.055083103477954865,
      "learning_rate": 0.0007308310991957105,
      "loss": 0.1118,
      "step": 502
    },
    {
      "epoch": 1.3485254691689008,
      "grad_norm": 0.03755289316177368,
      "learning_rate": 0.0007302949061662199,
      "loss": 0.1055,
      "step": 503
    },
    {
      "epoch": 1.351206434316354,
      "grad_norm": 0.03425239399075508,
      "learning_rate": 0.0007297587131367293,
      "loss": 0.1147,
      "step": 504
    },
    {
      "epoch": 1.353887399463807,
      "grad_norm": 0.037353742867708206,
      "learning_rate": 0.0007292225201072387,
      "loss": 0.1094,
      "step": 505
    },
    {
      "epoch": 1.35656836461126,
      "grad_norm": 0.061036910861730576,
      "learning_rate": 0.000728686327077748,
      "loss": 0.1113,
      "step": 506
    },
    {
      "epoch": 1.3592493297587132,
      "grad_norm": 0.27556315064430237,
      "learning_rate": 0.0007281501340482574,
      "loss": 0.1113,
      "step": 507
    },
    {
      "epoch": 1.3619302949061662,
      "grad_norm": 0.046286195516586304,
      "learning_rate": 0.0007276139410187668,
      "loss": 0.1074,
      "step": 508
    },
    {
      "epoch": 1.3646112600536193,
      "grad_norm": 0.043877772986888885,
      "learning_rate": 0.0007270777479892762,
      "loss": 0.1006,
      "step": 509
    },
    {
      "epoch": 1.3672922252010724,
      "grad_norm": 0.034168899059295654,
      "learning_rate": 0.0007265415549597856,
      "loss": 0.0977,
      "step": 510
    },
    {
      "epoch": 1.3699731903485255,
      "grad_norm": 0.07602671533823013,
      "learning_rate": 0.0007260053619302949,
      "loss": 0.0923,
      "step": 511
    },
    {
      "epoch": 1.3726541554959786,
      "grad_norm": 0.04076689854264259,
      "learning_rate": 0.0007254691689008043,
      "loss": 0.105,
      "step": 512
    },
    {
      "epoch": 1.3753351206434317,
      "grad_norm": 0.03930553421378136,
      "learning_rate": 0.0007249329758713137,
      "loss": 0.1191,
      "step": 513
    },
    {
      "epoch": 1.3780160857908847,
      "grad_norm": 0.0356135219335556,
      "learning_rate": 0.0007243967828418231,
      "loss": 0.1064,
      "step": 514
    },
    {
      "epoch": 1.3806970509383378,
      "grad_norm": 0.03488024324178696,
      "learning_rate": 0.0007238605898123325,
      "loss": 0.0967,
      "step": 515
    },
    {
      "epoch": 1.383378016085791,
      "grad_norm": 0.03902392089366913,
      "learning_rate": 0.0007233243967828417,
      "loss": 0.1147,
      "step": 516
    },
    {
      "epoch": 1.386058981233244,
      "grad_norm": 0.06332410871982574,
      "learning_rate": 0.0007227882037533511,
      "loss": 0.0938,
      "step": 517
    },
    {
      "epoch": 1.388739946380697,
      "grad_norm": 0.039852239191532135,
      "learning_rate": 0.0007222520107238605,
      "loss": 0.0947,
      "step": 518
    },
    {
      "epoch": 1.3914209115281502,
      "grad_norm": 0.03509104996919632,
      "learning_rate": 0.00072171581769437,
      "loss": 0.1011,
      "step": 519
    },
    {
      "epoch": 1.3941018766756033,
      "grad_norm": 0.04925445467233658,
      "learning_rate": 0.0007211796246648794,
      "loss": 0.0957,
      "step": 520
    },
    {
      "epoch": 1.3967828418230563,
      "grad_norm": 0.05072880908846855,
      "learning_rate": 0.0007206434316353887,
      "loss": 0.0962,
      "step": 521
    },
    {
      "epoch": 1.3994638069705094,
      "grad_norm": 0.032136112451553345,
      "learning_rate": 0.0007201072386058981,
      "loss": 0.0874,
      "step": 522
    },
    {
      "epoch": 1.4021447721179625,
      "grad_norm": 0.06137721240520477,
      "learning_rate": 0.0007195710455764075,
      "loss": 0.1226,
      "step": 523
    },
    {
      "epoch": 1.4048257372654156,
      "grad_norm": 0.09033665060997009,
      "learning_rate": 0.0007190348525469169,
      "loss": 0.1196,
      "step": 524
    },
    {
      "epoch": 1.4075067024128687,
      "grad_norm": 0.05129444971680641,
      "learning_rate": 0.0007184986595174263,
      "loss": 0.0869,
      "step": 525
    },
    {
      "epoch": 1.4101876675603218,
      "grad_norm": 0.04301593452692032,
      "learning_rate": 0.0007179624664879356,
      "loss": 0.1079,
      "step": 526
    },
    {
      "epoch": 1.4128686327077749,
      "grad_norm": 0.030942073091864586,
      "learning_rate": 0.000717426273458445,
      "loss": 0.0967,
      "step": 527
    },
    {
      "epoch": 1.415549597855228,
      "grad_norm": 0.03687169402837753,
      "learning_rate": 0.0007168900804289544,
      "loss": 0.106,
      "step": 528
    },
    {
      "epoch": 1.418230563002681,
      "grad_norm": 0.035381801426410675,
      "learning_rate": 0.0007163538873994638,
      "loss": 0.1064,
      "step": 529
    },
    {
      "epoch": 1.420911528150134,
      "grad_norm": 0.05853291228413582,
      "learning_rate": 0.0007158176943699732,
      "loss": 0.1094,
      "step": 530
    },
    {
      "epoch": 1.4235924932975872,
      "grad_norm": 0.06287603080272675,
      "learning_rate": 0.0007152815013404826,
      "loss": 0.1001,
      "step": 531
    },
    {
      "epoch": 1.4262734584450403,
      "grad_norm": 0.03775661438703537,
      "learning_rate": 0.0007147453083109919,
      "loss": 0.1133,
      "step": 532
    },
    {
      "epoch": 1.4289544235924934,
      "grad_norm": 0.05955362692475319,
      "learning_rate": 0.0007142091152815013,
      "loss": 0.0991,
      "step": 533
    },
    {
      "epoch": 1.4316353887399464,
      "grad_norm": 0.06330487877130508,
      "learning_rate": 0.0007136729222520107,
      "loss": 0.0986,
      "step": 534
    },
    {
      "epoch": 1.4343163538873995,
      "grad_norm": 0.053505487740039825,
      "learning_rate": 0.0007131367292225201,
      "loss": 0.083,
      "step": 535
    },
    {
      "epoch": 1.4369973190348526,
      "grad_norm": 0.029733123257756233,
      "learning_rate": 0.0007126005361930295,
      "loss": 0.084,
      "step": 536
    },
    {
      "epoch": 1.4396782841823057,
      "grad_norm": 0.036740802228450775,
      "learning_rate": 0.0007120643431635388,
      "loss": 0.1113,
      "step": 537
    },
    {
      "epoch": 1.4423592493297588,
      "grad_norm": 0.08972163498401642,
      "learning_rate": 0.0007115281501340482,
      "loss": 0.1123,
      "step": 538
    },
    {
      "epoch": 1.4450402144772119,
      "grad_norm": 0.03469543159008026,
      "learning_rate": 0.0007109919571045576,
      "loss": 0.0894,
      "step": 539
    },
    {
      "epoch": 1.447721179624665,
      "grad_norm": 0.03413958102464676,
      "learning_rate": 0.000710455764075067,
      "loss": 0.0918,
      "step": 540
    },
    {
      "epoch": 1.450402144772118,
      "grad_norm": 0.04605596885085106,
      "learning_rate": 0.0007099195710455764,
      "loss": 0.1162,
      "step": 541
    },
    {
      "epoch": 1.4530831099195711,
      "grad_norm": 0.04342813789844513,
      "learning_rate": 0.0007093833780160857,
      "loss": 0.0913,
      "step": 542
    },
    {
      "epoch": 1.4557640750670242,
      "grad_norm": 0.03513413295149803,
      "learning_rate": 0.0007088471849865951,
      "loss": 0.0791,
      "step": 543
    },
    {
      "epoch": 1.4584450402144773,
      "grad_norm": 0.041823577135801315,
      "learning_rate": 0.0007083109919571045,
      "loss": 0.0908,
      "step": 544
    },
    {
      "epoch": 1.4611260053619302,
      "grad_norm": 0.07622428238391876,
      "learning_rate": 0.0007077747989276139,
      "loss": 0.1182,
      "step": 545
    },
    {
      "epoch": 1.4638069705093835,
      "grad_norm": 0.02944333665072918,
      "learning_rate": 0.0007072386058981233,
      "loss": 0.0981,
      "step": 546
    },
    {
      "epoch": 1.4664879356568363,
      "grad_norm": 0.05987898260354996,
      "learning_rate": 0.0007067024128686326,
      "loss": 0.1206,
      "step": 547
    },
    {
      "epoch": 1.4691689008042896,
      "grad_norm": 0.0388108566403389,
      "learning_rate": 0.000706166219839142,
      "loss": 0.085,
      "step": 548
    },
    {
      "epoch": 1.4718498659517425,
      "grad_norm": 0.10303395986557007,
      "learning_rate": 0.0007056300268096515,
      "loss": 0.0845,
      "step": 549
    },
    {
      "epoch": 1.4745308310991958,
      "grad_norm": 0.03565900772809982,
      "learning_rate": 0.0007050938337801609,
      "loss": 0.0908,
      "step": 550
    },
    {
      "epoch": 1.4772117962466487,
      "grad_norm": 0.09259187430143356,
      "learning_rate": 0.0007045576407506703,
      "loss": 0.1011,
      "step": 551
    },
    {
      "epoch": 1.479892761394102,
      "grad_norm": 0.049213260412216187,
      "learning_rate": 0.0007040214477211796,
      "loss": 0.1064,
      "step": 552
    },
    {
      "epoch": 1.4825737265415548,
      "grad_norm": 0.046823132783174515,
      "learning_rate": 0.000703485254691689,
      "loss": 0.1377,
      "step": 553
    },
    {
      "epoch": 1.4852546916890081,
      "grad_norm": 0.041788045316934586,
      "learning_rate": 0.0007029490616621984,
      "loss": 0.1064,
      "step": 554
    },
    {
      "epoch": 1.487935656836461,
      "grad_norm": 0.06347786635160446,
      "learning_rate": 0.0007024128686327078,
      "loss": 0.0981,
      "step": 555
    },
    {
      "epoch": 1.4906166219839143,
      "grad_norm": 0.04448361322283745,
      "learning_rate": 0.0007018766756032172,
      "loss": 0.0869,
      "step": 556
    },
    {
      "epoch": 1.4932975871313672,
      "grad_norm": 0.04716790094971657,
      "learning_rate": 0.0007013404825737265,
      "loss": 0.1089,
      "step": 557
    },
    {
      "epoch": 1.4959785522788205,
      "grad_norm": 0.03406123071908951,
      "learning_rate": 0.0007008042895442359,
      "loss": 0.084,
      "step": 558
    },
    {
      "epoch": 1.4986595174262733,
      "grad_norm": 0.08022797852754593,
      "learning_rate": 0.0007002680965147453,
      "loss": 0.0996,
      "step": 559
    },
    {
      "epoch": 1.5013404825737267,
      "grad_norm": 0.035018082708120346,
      "learning_rate": 0.0006997319034852547,
      "loss": 0.0952,
      "step": 560
    },
    {
      "epoch": 1.5040214477211795,
      "grad_norm": 0.0382620133459568,
      "learning_rate": 0.0006991957104557641,
      "loss": 0.1074,
      "step": 561
    },
    {
      "epoch": 1.5067024128686328,
      "grad_norm": 0.032989244908094406,
      "learning_rate": 0.0006986595174262734,
      "loss": 0.0894,
      "step": 562
    },
    {
      "epoch": 1.5093833780160857,
      "grad_norm": 0.04077911004424095,
      "learning_rate": 0.0006981233243967828,
      "loss": 0.1025,
      "step": 563
    },
    {
      "epoch": 1.512064343163539,
      "grad_norm": 0.03434010595083237,
      "learning_rate": 0.0006975871313672922,
      "loss": 0.0874,
      "step": 564
    },
    {
      "epoch": 1.5147453083109919,
      "grad_norm": 0.0420994758605957,
      "learning_rate": 0.0006970509383378016,
      "loss": 0.0986,
      "step": 565
    },
    {
      "epoch": 1.5174262734584452,
      "grad_norm": 0.061451442539691925,
      "learning_rate": 0.000696514745308311,
      "loss": 0.0894,
      "step": 566
    },
    {
      "epoch": 1.520107238605898,
      "grad_norm": 0.048312947154045105,
      "learning_rate": 0.0006959785522788203,
      "loss": 0.0903,
      "step": 567
    },
    {
      "epoch": 1.5227882037533513,
      "grad_norm": 0.04021066799759865,
      "learning_rate": 0.0006954423592493297,
      "loss": 0.1021,
      "step": 568
    },
    {
      "epoch": 1.5254691689008042,
      "grad_norm": 0.04300690442323685,
      "learning_rate": 0.0006949061662198391,
      "loss": 0.0889,
      "step": 569
    },
    {
      "epoch": 1.5281501340482575,
      "grad_norm": 0.03647777810692787,
      "learning_rate": 0.0006943699731903485,
      "loss": 0.0942,
      "step": 570
    },
    {
      "epoch": 1.5308310991957104,
      "grad_norm": 0.06660760194063187,
      "learning_rate": 0.0006938337801608579,
      "loss": 0.0903,
      "step": 571
    },
    {
      "epoch": 1.5335120643431637,
      "grad_norm": 0.03831843286752701,
      "learning_rate": 0.0006932975871313672,
      "loss": 0.085,
      "step": 572
    },
    {
      "epoch": 1.5361930294906165,
      "grad_norm": 0.03509709611535072,
      "learning_rate": 0.0006927613941018766,
      "loss": 0.1133,
      "step": 573
    },
    {
      "epoch": 1.5388739946380698,
      "grad_norm": 0.03339695930480957,
      "learning_rate": 0.000692225201072386,
      "loss": 0.0854,
      "step": 574
    },
    {
      "epoch": 1.5415549597855227,
      "grad_norm": 0.029331503435969353,
      "learning_rate": 0.0006916890080428954,
      "loss": 0.0835,
      "step": 575
    },
    {
      "epoch": 1.544235924932976,
      "grad_norm": 0.044259533286094666,
      "learning_rate": 0.0006911528150134049,
      "loss": 0.1123,
      "step": 576
    },
    {
      "epoch": 1.5469168900804289,
      "grad_norm": 0.1104215458035469,
      "learning_rate": 0.0006906166219839142,
      "loss": 0.085,
      "step": 577
    },
    {
      "epoch": 1.5495978552278822,
      "grad_norm": 0.03132126107811928,
      "learning_rate": 0.0006900804289544236,
      "loss": 0.0889,
      "step": 578
    },
    {
      "epoch": 1.552278820375335,
      "grad_norm": 0.03774578496813774,
      "learning_rate": 0.000689544235924933,
      "loss": 0.105,
      "step": 579
    },
    {
      "epoch": 1.5549597855227884,
      "grad_norm": 0.03556313365697861,
      "learning_rate": 0.0006890080428954424,
      "loss": 0.0938,
      "step": 580
    },
    {
      "epoch": 1.5576407506702412,
      "grad_norm": 0.060462772846221924,
      "learning_rate": 0.0006884718498659518,
      "loss": 0.104,
      "step": 581
    },
    {
      "epoch": 1.5603217158176945,
      "grad_norm": 0.06803324818611145,
      "learning_rate": 0.0006879356568364611,
      "loss": 0.1045,
      "step": 582
    },
    {
      "epoch": 1.5630026809651474,
      "grad_norm": 0.03435233235359192,
      "learning_rate": 0.0006873994638069705,
      "loss": 0.0967,
      "step": 583
    },
    {
      "epoch": 1.5656836461126007,
      "grad_norm": 0.06937245279550552,
      "learning_rate": 0.0006868632707774799,
      "loss": 0.0967,
      "step": 584
    },
    {
      "epoch": 1.5683646112600536,
      "grad_norm": 0.058249812573194504,
      "learning_rate": 0.0006863270777479893,
      "loss": 0.1006,
      "step": 585
    },
    {
      "epoch": 1.5710455764075069,
      "grad_norm": 0.04290178790688515,
      "learning_rate": 0.0006857908847184987,
      "loss": 0.1011,
      "step": 586
    },
    {
      "epoch": 1.5737265415549597,
      "grad_norm": 0.031614307314157486,
      "learning_rate": 0.000685254691689008,
      "loss": 0.0732,
      "step": 587
    },
    {
      "epoch": 1.576407506702413,
      "grad_norm": 0.06755483150482178,
      "learning_rate": 0.0006847184986595174,
      "loss": 0.0962,
      "step": 588
    },
    {
      "epoch": 1.579088471849866,
      "grad_norm": 0.043806977570056915,
      "learning_rate": 0.0006841823056300268,
      "loss": 0.0947,
      "step": 589
    },
    {
      "epoch": 1.5817694369973192,
      "grad_norm": 0.04403859004378319,
      "learning_rate": 0.0006836461126005362,
      "loss": 0.0913,
      "step": 590
    },
    {
      "epoch": 1.584450402144772,
      "grad_norm": 0.05758955702185631,
      "learning_rate": 0.0006831099195710456,
      "loss": 0.1152,
      "step": 591
    },
    {
      "epoch": 1.5871313672922251,
      "grad_norm": 0.09294259548187256,
      "learning_rate": 0.000682573726541555,
      "loss": 0.0991,
      "step": 592
    },
    {
      "epoch": 1.5898123324396782,
      "grad_norm": 0.05314783751964569,
      "learning_rate": 0.0006820375335120643,
      "loss": 0.1309,
      "step": 593
    },
    {
      "epoch": 1.5924932975871313,
      "grad_norm": 0.03911600634455681,
      "learning_rate": 0.0006815013404825737,
      "loss": 0.1045,
      "step": 594
    },
    {
      "epoch": 1.5951742627345844,
      "grad_norm": 0.04651249572634697,
      "learning_rate": 0.0006809651474530831,
      "loss": 0.1157,
      "step": 595
    },
    {
      "epoch": 1.5978552278820375,
      "grad_norm": 0.045887503772974014,
      "learning_rate": 0.0006804289544235925,
      "loss": 0.084,
      "step": 596
    },
    {
      "epoch": 1.6005361930294906,
      "grad_norm": 0.04048018530011177,
      "learning_rate": 0.0006798927613941019,
      "loss": 0.1016,
      "step": 597
    },
    {
      "epoch": 1.6032171581769437,
      "grad_norm": 0.037670742720365524,
      "learning_rate": 0.0006793565683646112,
      "loss": 0.0967,
      "step": 598
    },
    {
      "epoch": 1.6058981233243967,
      "grad_norm": 0.029867587611079216,
      "learning_rate": 0.0006788203753351206,
      "loss": 0.084,
      "step": 599
    },
    {
      "epoch": 1.6085790884718498,
      "grad_norm": 0.05724148824810982,
      "learning_rate": 0.00067828418230563,
      "loss": 0.0947,
      "step": 600
    },
    {
      "epoch": 1.611260053619303,
      "grad_norm": 0.040982212871313095,
      "learning_rate": 0.0006777479892761394,
      "loss": 0.1201,
      "step": 601
    },
    {
      "epoch": 1.613941018766756,
      "grad_norm": 0.03761983662843704,
      "learning_rate": 0.0006772117962466488,
      "loss": 0.0811,
      "step": 602
    },
    {
      "epoch": 1.616621983914209,
      "grad_norm": 0.06299537420272827,
      "learning_rate": 0.0006766756032171581,
      "loss": 0.1094,
      "step": 603
    },
    {
      "epoch": 1.6193029490616622,
      "grad_norm": 0.07088592648506165,
      "learning_rate": 0.0006761394101876675,
      "loss": 0.103,
      "step": 604
    },
    {
      "epoch": 1.6219839142091153,
      "grad_norm": 0.03746599704027176,
      "learning_rate": 0.000675603217158177,
      "loss": 0.1079,
      "step": 605
    },
    {
      "epoch": 1.6246648793565683,
      "grad_norm": 0.04793906211853027,
      "learning_rate": 0.0006750670241286864,
      "loss": 0.0786,
      "step": 606
    },
    {
      "epoch": 1.6273458445040214,
      "grad_norm": 0.0485326424241066,
      "learning_rate": 0.0006745308310991958,
      "loss": 0.0894,
      "step": 607
    },
    {
      "epoch": 1.6300268096514745,
      "grad_norm": 0.04811655357480049,
      "learning_rate": 0.0006739946380697051,
      "loss": 0.0957,
      "step": 608
    },
    {
      "epoch": 1.6327077747989276,
      "grad_norm": 0.037618160247802734,
      "learning_rate": 0.0006734584450402145,
      "loss": 0.084,
      "step": 609
    },
    {
      "epoch": 1.6353887399463807,
      "grad_norm": 0.059875261038541794,
      "learning_rate": 0.0006729222520107239,
      "loss": 0.0776,
      "step": 610
    },
    {
      "epoch": 1.6380697050938338,
      "grad_norm": 0.03591162711381912,
      "learning_rate": 0.0006723860589812333,
      "loss": 0.0938,
      "step": 611
    },
    {
      "epoch": 1.6407506702412868,
      "grad_norm": 0.039722394198179245,
      "learning_rate": 0.0006718498659517427,
      "loss": 0.1069,
      "step": 612
    },
    {
      "epoch": 1.64343163538874,
      "grad_norm": 0.032908614724874496,
      "learning_rate": 0.000671313672922252,
      "loss": 0.0728,
      "step": 613
    },
    {
      "epoch": 1.646112600536193,
      "grad_norm": 0.03397935628890991,
      "learning_rate": 0.0006707774798927614,
      "loss": 0.1011,
      "step": 614
    },
    {
      "epoch": 1.648793565683646,
      "grad_norm": 0.07602355629205704,
      "learning_rate": 0.0006702412868632708,
      "loss": 0.0952,
      "step": 615
    },
    {
      "epoch": 1.6514745308310992,
      "grad_norm": 0.04519004747271538,
      "learning_rate": 0.0006697050938337802,
      "loss": 0.104,
      "step": 616
    },
    {
      "epoch": 1.6541554959785523,
      "grad_norm": 0.07860317826271057,
      "learning_rate": 0.0006691689008042896,
      "loss": 0.1079,
      "step": 617
    },
    {
      "epoch": 1.6568364611260054,
      "grad_norm": 0.043541934341192245,
      "learning_rate": 0.0006686327077747989,
      "loss": 0.0947,
      "step": 618
    },
    {
      "epoch": 1.6595174262734584,
      "grad_norm": 0.03397178649902344,
      "learning_rate": 0.0006680965147453083,
      "loss": 0.083,
      "step": 619
    },
    {
      "epoch": 1.6621983914209115,
      "grad_norm": 0.031535740941762924,
      "learning_rate": 0.0006675603217158177,
      "loss": 0.0728,
      "step": 620
    },
    {
      "epoch": 1.6648793565683646,
      "grad_norm": 0.07710004597902298,
      "learning_rate": 0.0006670241286863271,
      "loss": 0.1011,
      "step": 621
    },
    {
      "epoch": 1.6675603217158177,
      "grad_norm": 0.03302523493766785,
      "learning_rate": 0.0006664879356568365,
      "loss": 0.0835,
      "step": 622
    },
    {
      "epoch": 1.6702412868632708,
      "grad_norm": 0.05227351561188698,
      "learning_rate": 0.0006659517426273458,
      "loss": 0.124,
      "step": 623
    },
    {
      "epoch": 1.6729222520107239,
      "grad_norm": 0.058933693915605545,
      "learning_rate": 0.0006654155495978552,
      "loss": 0.0947,
      "step": 624
    },
    {
      "epoch": 1.675603217158177,
      "grad_norm": 0.04874420166015625,
      "learning_rate": 0.0006648793565683646,
      "loss": 0.1016,
      "step": 625
    },
    {
      "epoch": 1.67828418230563,
      "grad_norm": 0.03734862431883812,
      "learning_rate": 0.000664343163538874,
      "loss": 0.0874,
      "step": 626
    },
    {
      "epoch": 1.6809651474530831,
      "grad_norm": 0.03725391998887062,
      "learning_rate": 0.0006638069705093834,
      "loss": 0.1108,
      "step": 627
    },
    {
      "epoch": 1.6836461126005362,
      "grad_norm": 0.03566949442028999,
      "learning_rate": 0.0006632707774798927,
      "loss": 0.0967,
      "step": 628
    },
    {
      "epoch": 1.6863270777479893,
      "grad_norm": 0.04120577499270439,
      "learning_rate": 0.0006627345844504021,
      "loss": 0.0776,
      "step": 629
    },
    {
      "epoch": 1.6890080428954424,
      "grad_norm": 0.03609228506684303,
      "learning_rate": 0.0006621983914209115,
      "loss": 0.0889,
      "step": 630
    },
    {
      "epoch": 1.6916890080428955,
      "grad_norm": 0.034433916211128235,
      "learning_rate": 0.000661662198391421,
      "loss": 0.0918,
      "step": 631
    },
    {
      "epoch": 1.6943699731903485,
      "grad_norm": 0.03521991893649101,
      "learning_rate": 0.0006611260053619304,
      "loss": 0.1006,
      "step": 632
    },
    {
      "epoch": 1.6970509383378016,
      "grad_norm": 0.037951648235321045,
      "learning_rate": 0.0006605898123324397,
      "loss": 0.1074,
      "step": 633
    },
    {
      "epoch": 1.6997319034852547,
      "grad_norm": 0.05361508950591087,
      "learning_rate": 0.0006600536193029491,
      "loss": 0.0952,
      "step": 634
    },
    {
      "epoch": 1.7024128686327078,
      "grad_norm": 0.04750974103808403,
      "learning_rate": 0.0006595174262734585,
      "loss": 0.0781,
      "step": 635
    },
    {
      "epoch": 1.7050938337801609,
      "grad_norm": 0.046655938029289246,
      "learning_rate": 0.0006589812332439679,
      "loss": 0.0806,
      "step": 636
    },
    {
      "epoch": 1.707774798927614,
      "grad_norm": 0.03164898604154587,
      "learning_rate": 0.0006584450402144773,
      "loss": 0.0977,
      "step": 637
    },
    {
      "epoch": 1.710455764075067,
      "grad_norm": 0.0662396252155304,
      "learning_rate": 0.0006579088471849866,
      "loss": 0.0869,
      "step": 638
    },
    {
      "epoch": 1.7131367292225201,
      "grad_norm": 0.03163400664925575,
      "learning_rate": 0.000657372654155496,
      "loss": 0.084,
      "step": 639
    },
    {
      "epoch": 1.7158176943699732,
      "grad_norm": 0.07160817831754684,
      "learning_rate": 0.0006568364611260054,
      "loss": 0.0874,
      "step": 640
    },
    {
      "epoch": 1.7184986595174263,
      "grad_norm": 0.03731532022356987,
      "learning_rate": 0.0006563002680965148,
      "loss": 0.0845,
      "step": 641
    },
    {
      "epoch": 1.7211796246648794,
      "grad_norm": 0.07238434255123138,
      "learning_rate": 0.0006557640750670242,
      "loss": 0.1172,
      "step": 642
    },
    {
      "epoch": 1.7238605898123325,
      "grad_norm": 0.03840181604027748,
      "learning_rate": 0.0006552278820375335,
      "loss": 0.0928,
      "step": 643
    },
    {
      "epoch": 1.7265415549597856,
      "grad_norm": 0.034525465220212936,
      "learning_rate": 0.0006546916890080429,
      "loss": 0.0781,
      "step": 644
    },
    {
      "epoch": 1.7292225201072386,
      "grad_norm": 0.03352326154708862,
      "learning_rate": 0.0006541554959785523,
      "loss": 0.0854,
      "step": 645
    },
    {
      "epoch": 1.7319034852546917,
      "grad_norm": 0.04455629736185074,
      "learning_rate": 0.0006536193029490617,
      "loss": 0.0996,
      "step": 646
    },
    {
      "epoch": 1.7345844504021448,
      "grad_norm": 0.0432487428188324,
      "learning_rate": 0.0006530831099195711,
      "loss": 0.103,
      "step": 647
    },
    {
      "epoch": 1.737265415549598,
      "grad_norm": 0.03995256498456001,
      "learning_rate": 0.0006525469168900804,
      "loss": 0.0889,
      "step": 648
    },
    {
      "epoch": 1.739946380697051,
      "grad_norm": 0.03333340585231781,
      "learning_rate": 0.0006520107238605898,
      "loss": 0.082,
      "step": 649
    },
    {
      "epoch": 1.742627345844504,
      "grad_norm": 0.04562292620539665,
      "learning_rate": 0.0006514745308310992,
      "loss": 0.1152,
      "step": 650
    },
    {
      "epoch": 1.7453083109919572,
      "grad_norm": 0.031180959194898605,
      "learning_rate": 0.0006509383378016086,
      "loss": 0.0742,
      "step": 651
    },
    {
      "epoch": 1.7479892761394102,
      "grad_norm": 0.051821913570165634,
      "learning_rate": 0.000650402144772118,
      "loss": 0.0991,
      "step": 652
    },
    {
      "epoch": 1.7506702412868633,
      "grad_norm": 0.03943241387605667,
      "learning_rate": 0.0006498659517426274,
      "loss": 0.0879,
      "step": 653
    },
    {
      "epoch": 1.7533512064343162,
      "grad_norm": 0.04481467977166176,
      "learning_rate": 0.0006493297587131367,
      "loss": 0.0889,
      "step": 654
    },
    {
      "epoch": 1.7560321715817695,
      "grad_norm": 0.06367837637662888,
      "learning_rate": 0.0006487935656836461,
      "loss": 0.104,
      "step": 655
    },
    {
      "epoch": 1.7587131367292224,
      "grad_norm": 0.03282579034566879,
      "learning_rate": 0.0006482573726541555,
      "loss": 0.103,
      "step": 656
    },
    {
      "epoch": 1.7613941018766757,
      "grad_norm": 0.03364764526486397,
      "learning_rate": 0.0006477211796246649,
      "loss": 0.106,
      "step": 657
    },
    {
      "epoch": 1.7640750670241285,
      "grad_norm": 0.09112652391195297,
      "learning_rate": 0.0006471849865951743,
      "loss": 0.0811,
      "step": 658
    },
    {
      "epoch": 1.7667560321715818,
      "grad_norm": 0.03631673380732536,
      "learning_rate": 0.0006466487935656836,
      "loss": 0.0894,
      "step": 659
    },
    {
      "epoch": 1.7694369973190347,
      "grad_norm": 0.2668725848197937,
      "learning_rate": 0.000646112600536193,
      "loss": 0.1006,
      "step": 660
    },
    {
      "epoch": 1.772117962466488,
      "grad_norm": 0.05296935886144638,
      "learning_rate": 0.0006455764075067025,
      "loss": 0.0835,
      "step": 661
    },
    {
      "epoch": 1.7747989276139409,
      "grad_norm": 0.03983982652425766,
      "learning_rate": 0.0006450402144772119,
      "loss": 0.0986,
      "step": 662
    },
    {
      "epoch": 1.7774798927613942,
      "grad_norm": 0.03526284918189049,
      "learning_rate": 0.0006445040214477213,
      "loss": 0.082,
      "step": 663
    },
    {
      "epoch": 1.780160857908847,
      "grad_norm": 0.05491136386990547,
      "learning_rate": 0.0006439678284182306,
      "loss": 0.1143,
      "step": 664
    },
    {
      "epoch": 1.7828418230563003,
      "grad_norm": 0.06086675077676773,
      "learning_rate": 0.00064343163538874,
      "loss": 0.1211,
      "step": 665
    },
    {
      "epoch": 1.7855227882037532,
      "grad_norm": 0.03084493614733219,
      "learning_rate": 0.0006428954423592494,
      "loss": 0.0654,
      "step": 666
    },
    {
      "epoch": 1.7882037533512065,
      "grad_norm": 0.03004429303109646,
      "learning_rate": 0.0006423592493297588,
      "loss": 0.0884,
      "step": 667
    },
    {
      "epoch": 1.7908847184986594,
      "grad_norm": 0.02999439835548401,
      "learning_rate": 0.0006418230563002682,
      "loss": 0.0903,
      "step": 668
    },
    {
      "epoch": 1.7935656836461127,
      "grad_norm": 0.08487444370985031,
      "learning_rate": 0.0006412868632707775,
      "loss": 0.0859,
      "step": 669
    },
    {
      "epoch": 1.7962466487935655,
      "grad_norm": 0.05282389000058174,
      "learning_rate": 0.0006407506702412869,
      "loss": 0.1157,
      "step": 670
    },
    {
      "epoch": 1.7989276139410189,
      "grad_norm": 0.05163947865366936,
      "learning_rate": 0.0006402144772117963,
      "loss": 0.0811,
      "step": 671
    },
    {
      "epoch": 1.8016085790884717,
      "grad_norm": 0.03628304600715637,
      "learning_rate": 0.0006396782841823057,
      "loss": 0.0879,
      "step": 672
    },
    {
      "epoch": 1.804289544235925,
      "grad_norm": 0.03398411348462105,
      "learning_rate": 0.0006391420911528151,
      "loss": 0.0811,
      "step": 673
    },
    {
      "epoch": 1.8069705093833779,
      "grad_norm": 0.03489488735795021,
      "learning_rate": 0.0006386058981233244,
      "loss": 0.0859,
      "step": 674
    },
    {
      "epoch": 1.8096514745308312,
      "grad_norm": 0.10462342947721481,
      "learning_rate": 0.0006380697050938338,
      "loss": 0.0996,
      "step": 675
    },
    {
      "epoch": 1.812332439678284,
      "grad_norm": 0.04418081417679787,
      "learning_rate": 0.0006375335120643432,
      "loss": 0.0952,
      "step": 676
    },
    {
      "epoch": 1.8150134048257374,
      "grad_norm": 0.04059178754687309,
      "learning_rate": 0.0006369973190348526,
      "loss": 0.0898,
      "step": 677
    },
    {
      "epoch": 1.8176943699731902,
      "grad_norm": 0.039256852120161057,
      "learning_rate": 0.000636461126005362,
      "loss": 0.0923,
      "step": 678
    },
    {
      "epoch": 1.8203753351206435,
      "grad_norm": 0.04564589262008667,
      "learning_rate": 0.0006359249329758713,
      "loss": 0.106,
      "step": 679
    },
    {
      "epoch": 1.8230563002680964,
      "grad_norm": 0.03321092948317528,
      "learning_rate": 0.0006353887399463807,
      "loss": 0.0947,
      "step": 680
    },
    {
      "epoch": 1.8257372654155497,
      "grad_norm": 0.04797625541687012,
      "learning_rate": 0.0006348525469168901,
      "loss": 0.0962,
      "step": 681
    },
    {
      "epoch": 1.8284182305630026,
      "grad_norm": 0.030749857425689697,
      "learning_rate": 0.0006343163538873995,
      "loss": 0.0962,
      "step": 682
    },
    {
      "epoch": 1.8310991957104559,
      "grad_norm": 0.05395966395735741,
      "learning_rate": 0.0006337801608579089,
      "loss": 0.0889,
      "step": 683
    },
    {
      "epoch": 1.8337801608579087,
      "grad_norm": 0.07870075106620789,
      "learning_rate": 0.0006332439678284182,
      "loss": 0.0903,
      "step": 684
    },
    {
      "epoch": 1.836461126005362,
      "grad_norm": 0.04637882858514786,
      "learning_rate": 0.0006327077747989276,
      "loss": 0.0815,
      "step": 685
    },
    {
      "epoch": 1.839142091152815,
      "grad_norm": 0.05572907626628876,
      "learning_rate": 0.000632171581769437,
      "loss": 0.0806,
      "step": 686
    },
    {
      "epoch": 1.8418230563002682,
      "grad_norm": 0.0728142186999321,
      "learning_rate": 0.0006316353887399464,
      "loss": 0.0977,
      "step": 687
    },
    {
      "epoch": 1.844504021447721,
      "grad_norm": 0.04549106955528259,
      "learning_rate": 0.0006310991957104559,
      "loss": 0.0986,
      "step": 688
    },
    {
      "epoch": 1.8471849865951744,
      "grad_norm": 0.045741572976112366,
      "learning_rate": 0.0006305630026809652,
      "loss": 0.1138,
      "step": 689
    },
    {
      "epoch": 1.8498659517426272,
      "grad_norm": 0.03676038980484009,
      "learning_rate": 0.0006300268096514746,
      "loss": 0.0854,
      "step": 690
    },
    {
      "epoch": 1.8525469168900806,
      "grad_norm": 0.03530310094356537,
      "learning_rate": 0.000629490616621984,
      "loss": 0.1025,
      "step": 691
    },
    {
      "epoch": 1.8552278820375334,
      "grad_norm": 0.08328412473201752,
      "learning_rate": 0.0006289544235924934,
      "loss": 0.0811,
      "step": 692
    },
    {
      "epoch": 1.8579088471849867,
      "grad_norm": 0.03384043648838997,
      "learning_rate": 0.0006284182305630028,
      "loss": 0.106,
      "step": 693
    },
    {
      "epoch": 1.8605898123324396,
      "grad_norm": 0.03660734370350838,
      "learning_rate": 0.0006278820375335121,
      "loss": 0.0981,
      "step": 694
    },
    {
      "epoch": 1.863270777479893,
      "grad_norm": 0.03478599712252617,
      "learning_rate": 0.0006273458445040215,
      "loss": 0.0747,
      "step": 695
    },
    {
      "epoch": 1.8659517426273458,
      "grad_norm": 0.044746220111846924,
      "learning_rate": 0.0006268096514745309,
      "loss": 0.1021,
      "step": 696
    },
    {
      "epoch": 1.868632707774799,
      "grad_norm": 0.03477809950709343,
      "learning_rate": 0.0006262734584450403,
      "loss": 0.0811,
      "step": 697
    },
    {
      "epoch": 1.871313672922252,
      "grad_norm": 0.042874209582805634,
      "learning_rate": 0.0006257372654155497,
      "loss": 0.0996,
      "step": 698
    },
    {
      "epoch": 1.8739946380697052,
      "grad_norm": 0.04167487844824791,
      "learning_rate": 0.000625201072386059,
      "loss": 0.0869,
      "step": 699
    },
    {
      "epoch": 1.876675603217158,
      "grad_norm": 0.03343311697244644,
      "learning_rate": 0.0006246648793565684,
      "loss": 0.0947,
      "step": 700
    },
    {
      "epoch": 1.8793565683646114,
      "grad_norm": 0.03143227472901344,
      "learning_rate": 0.0006241286863270778,
      "loss": 0.083,
      "step": 701
    },
    {
      "epoch": 1.8820375335120643,
      "grad_norm": 0.03336033970117569,
      "learning_rate": 0.0006235924932975872,
      "loss": 0.0845,
      "step": 702
    },
    {
      "epoch": 1.8847184986595176,
      "grad_norm": 0.049359869211912155,
      "learning_rate": 0.0006230563002680966,
      "loss": 0.0801,
      "step": 703
    },
    {
      "epoch": 1.8873994638069704,
      "grad_norm": 0.05212642252445221,
      "learning_rate": 0.0006225201072386059,
      "loss": 0.0845,
      "step": 704
    },
    {
      "epoch": 1.8900804289544237,
      "grad_norm": 0.034898541867733,
      "learning_rate": 0.0006219839142091153,
      "loss": 0.0806,
      "step": 705
    },
    {
      "epoch": 1.8927613941018766,
      "grad_norm": 0.06740362197160721,
      "learning_rate": 0.0006214477211796247,
      "loss": 0.0859,
      "step": 706
    },
    {
      "epoch": 1.89544235924933,
      "grad_norm": 0.034282781183719635,
      "learning_rate": 0.0006209115281501341,
      "loss": 0.0908,
      "step": 707
    },
    {
      "epoch": 1.8981233243967828,
      "grad_norm": 0.0332472026348114,
      "learning_rate": 0.0006203753351206435,
      "loss": 0.0859,
      "step": 708
    },
    {
      "epoch": 1.900804289544236,
      "grad_norm": 0.04226452484726906,
      "learning_rate": 0.0006198391420911528,
      "loss": 0.0952,
      "step": 709
    },
    {
      "epoch": 1.903485254691689,
      "grad_norm": 0.04115695506334305,
      "learning_rate": 0.0006193029490616622,
      "loss": 0.1099,
      "step": 710
    },
    {
      "epoch": 1.9061662198391423,
      "grad_norm": 0.03898955509066582,
      "learning_rate": 0.0006187667560321716,
      "loss": 0.0815,
      "step": 711
    },
    {
      "epoch": 1.9088471849865951,
      "grad_norm": 0.05828113481402397,
      "learning_rate": 0.000618230563002681,
      "loss": 0.0947,
      "step": 712
    },
    {
      "epoch": 1.9115281501340484,
      "grad_norm": 0.0349823422729969,
      "learning_rate": 0.0006176943699731904,
      "loss": 0.0874,
      "step": 713
    },
    {
      "epoch": 1.9142091152815013,
      "grad_norm": 0.03551993891596794,
      "learning_rate": 0.0006171581769436997,
      "loss": 0.0938,
      "step": 714
    },
    {
      "epoch": 1.9168900804289544,
      "grad_norm": 0.05521970987319946,
      "learning_rate": 0.0006166219839142091,
      "loss": 0.0913,
      "step": 715
    },
    {
      "epoch": 1.9195710455764075,
      "grad_norm": 0.05087597295641899,
      "learning_rate": 0.0006160857908847185,
      "loss": 0.0962,
      "step": 716
    },
    {
      "epoch": 1.9222520107238605,
      "grad_norm": 0.059400126338005066,
      "learning_rate": 0.000615549597855228,
      "loss": 0.0894,
      "step": 717
    },
    {
      "epoch": 1.9249329758713136,
      "grad_norm": 0.03561253100633621,
      "learning_rate": 0.0006150134048257374,
      "loss": 0.0889,
      "step": 718
    },
    {
      "epoch": 1.9276139410187667,
      "grad_norm": 0.029261864721775055,
      "learning_rate": 0.0006144772117962468,
      "loss": 0.0698,
      "step": 719
    },
    {
      "epoch": 1.9302949061662198,
      "grad_norm": 0.07530271261930466,
      "learning_rate": 0.0006139410187667561,
      "loss": 0.082,
      "step": 720
    },
    {
      "epoch": 1.9329758713136729,
      "grad_norm": 0.02858825773000717,
      "learning_rate": 0.0006134048257372655,
      "loss": 0.0762,
      "step": 721
    },
    {
      "epoch": 1.935656836461126,
      "grad_norm": 0.031396981328725815,
      "learning_rate": 0.0006128686327077749,
      "loss": 0.1006,
      "step": 722
    },
    {
      "epoch": 1.938337801608579,
      "grad_norm": 0.046662043780088425,
      "learning_rate": 0.0006123324396782843,
      "loss": 0.1064,
      "step": 723
    },
    {
      "epoch": 1.9410187667560321,
      "grad_norm": 0.07535764575004578,
      "learning_rate": 0.0006117962466487937,
      "loss": 0.0825,
      "step": 724
    },
    {
      "epoch": 1.9436997319034852,
      "grad_norm": 0.060698263347148895,
      "learning_rate": 0.0006112600536193029,
      "loss": 0.0903,
      "step": 725
    },
    {
      "epoch": 1.9463806970509383,
      "grad_norm": 0.03282945603132248,
      "learning_rate": 0.0006107238605898123,
      "loss": 0.0806,
      "step": 726
    },
    {
      "epoch": 1.9490616621983914,
      "grad_norm": 0.0497354120016098,
      "learning_rate": 0.0006101876675603217,
      "loss": 0.0918,
      "step": 727
    },
    {
      "epoch": 1.9517426273458445,
      "grad_norm": 0.05658402666449547,
      "learning_rate": 0.0006096514745308311,
      "loss": 0.0913,
      "step": 728
    },
    {
      "epoch": 1.9544235924932976,
      "grad_norm": 0.05371883884072304,
      "learning_rate": 0.0006091152815013405,
      "loss": 0.1025,
      "step": 729
    },
    {
      "epoch": 1.9571045576407506,
      "grad_norm": 0.049372974783182144,
      "learning_rate": 0.0006085790884718498,
      "loss": 0.0825,
      "step": 730
    },
    {
      "epoch": 1.9597855227882037,
      "grad_norm": 0.03492110222578049,
      "learning_rate": 0.0006080428954423592,
      "loss": 0.0698,
      "step": 731
    },
    {
      "epoch": 1.9624664879356568,
      "grad_norm": 0.03575868904590607,
      "learning_rate": 0.0006075067024128686,
      "loss": 0.0977,
      "step": 732
    },
    {
      "epoch": 1.96514745308311,
      "grad_norm": 0.032048679888248444,
      "learning_rate": 0.000606970509383378,
      "loss": 0.0713,
      "step": 733
    },
    {
      "epoch": 1.967828418230563,
      "grad_norm": 0.03137800097465515,
      "learning_rate": 0.0006064343163538874,
      "loss": 0.0913,
      "step": 734
    },
    {
      "epoch": 1.970509383378016,
      "grad_norm": 0.03848005831241608,
      "learning_rate": 0.0006058981233243967,
      "loss": 0.1089,
      "step": 735
    },
    {
      "epoch": 1.9731903485254692,
      "grad_norm": 0.03390797600150108,
      "learning_rate": 0.0006053619302949061,
      "loss": 0.0967,
      "step": 736
    },
    {
      "epoch": 1.9758713136729222,
      "grad_norm": 0.043038297444581985,
      "learning_rate": 0.0006048257372654155,
      "loss": 0.0879,
      "step": 737
    },
    {
      "epoch": 1.9785522788203753,
      "grad_norm": 0.0331210121512413,
      "learning_rate": 0.0006042895442359249,
      "loss": 0.0996,
      "step": 738
    },
    {
      "epoch": 1.9812332439678284,
      "grad_norm": 0.048743851482868195,
      "learning_rate": 0.0006037533512064343,
      "loss": 0.0884,
      "step": 739
    },
    {
      "epoch": 1.9839142091152815,
      "grad_norm": 0.03731883689761162,
      "learning_rate": 0.0006032171581769436,
      "loss": 0.0918,
      "step": 740
    },
    {
      "epoch": 1.9865951742627346,
      "grad_norm": 0.03445059433579445,
      "learning_rate": 0.000602680965147453,
      "loss": 0.0693,
      "step": 741
    },
    {
      "epoch": 1.9892761394101877,
      "grad_norm": 0.04463675618171692,
      "learning_rate": 0.0006021447721179624,
      "loss": 0.0854,
      "step": 742
    },
    {
      "epoch": 1.9919571045576407,
      "grad_norm": 0.06414873152971268,
      "learning_rate": 0.0006016085790884718,
      "loss": 0.0845,
      "step": 743
    },
    {
      "epoch": 1.9946380697050938,
      "grad_norm": 0.04409773275256157,
      "learning_rate": 0.0006010723860589812,
      "loss": 0.1045,
      "step": 744
    },
    {
      "epoch": 1.997319034852547,
      "grad_norm": 0.033543091267347336,
      "learning_rate": 0.0006005361930294905,
      "loss": 0.0737,
      "step": 745
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.10569846630096436,
      "learning_rate": 0.0006,
      "loss": 0.0923,
      "step": 746
    },
    {
      "epoch": 2.002680965147453,
      "grad_norm": 0.03430378809571266,
      "learning_rate": 0.0005994638069705094,
      "loss": 0.0806,
      "step": 747
    },
    {
      "epoch": 2.005361930294906,
      "grad_norm": 0.08312736451625824,
      "learning_rate": 0.0005989276139410188,
      "loss": 0.0894,
      "step": 748
    },
    {
      "epoch": 2.008042895442359,
      "grad_norm": 0.03626048192381859,
      "learning_rate": 0.0005983914209115282,
      "loss": 0.0923,
      "step": 749
    },
    {
      "epoch": 2.0107238605898123,
      "grad_norm": 0.09385457634925842,
      "learning_rate": 0.0005978552278820375,
      "loss": 0.1011,
      "step": 750
    },
    {
      "epoch": 2.013404825737265,
      "grad_norm": 0.03677622601389885,
      "learning_rate": 0.0005973190348525469,
      "loss": 0.0864,
      "step": 751
    },
    {
      "epoch": 2.0160857908847185,
      "grad_norm": 0.034742023795843124,
      "learning_rate": 0.0005967828418230563,
      "loss": 0.064,
      "step": 752
    },
    {
      "epoch": 2.0187667560321714,
      "grad_norm": 0.03367893397808075,
      "learning_rate": 0.0005962466487935657,
      "loss": 0.0845,
      "step": 753
    },
    {
      "epoch": 2.0214477211796247,
      "grad_norm": 0.04199318587779999,
      "learning_rate": 0.0005957104557640751,
      "loss": 0.0884,
      "step": 754
    },
    {
      "epoch": 2.0241286863270775,
      "grad_norm": 0.03277599439024925,
      "learning_rate": 0.0005951742627345844,
      "loss": 0.0845,
      "step": 755
    },
    {
      "epoch": 2.026809651474531,
      "grad_norm": 0.04273903742432594,
      "learning_rate": 0.0005946380697050938,
      "loss": 0.1021,
      "step": 756
    },
    {
      "epoch": 2.0294906166219837,
      "grad_norm": 0.0347498282790184,
      "learning_rate": 0.0005941018766756032,
      "loss": 0.0869,
      "step": 757
    },
    {
      "epoch": 2.032171581769437,
      "grad_norm": 0.026582347229123116,
      "learning_rate": 0.0005935656836461126,
      "loss": 0.0776,
      "step": 758
    },
    {
      "epoch": 2.03485254691689,
      "grad_norm": 0.038381777703762054,
      "learning_rate": 0.000593029490616622,
      "loss": 0.0869,
      "step": 759
    },
    {
      "epoch": 2.037533512064343,
      "grad_norm": 0.05644490569829941,
      "learning_rate": 0.0005924932975871313,
      "loss": 0.0713,
      "step": 760
    },
    {
      "epoch": 2.040214477211796,
      "grad_norm": 0.0436444953083992,
      "learning_rate": 0.0005919571045576407,
      "loss": 0.0889,
      "step": 761
    },
    {
      "epoch": 2.0428954423592494,
      "grad_norm": 0.030998896807432175,
      "learning_rate": 0.0005914209115281501,
      "loss": 0.0598,
      "step": 762
    },
    {
      "epoch": 2.045576407506702,
      "grad_norm": 0.04905093088746071,
      "learning_rate": 0.0005908847184986595,
      "loss": 0.0918,
      "step": 763
    },
    {
      "epoch": 2.0482573726541555,
      "grad_norm": 0.0583963617682457,
      "learning_rate": 0.0005903485254691689,
      "loss": 0.0684,
      "step": 764
    },
    {
      "epoch": 2.0509383378016084,
      "grad_norm": 0.03368112072348595,
      "learning_rate": 0.0005898123324396782,
      "loss": 0.0879,
      "step": 765
    },
    {
      "epoch": 2.0536193029490617,
      "grad_norm": 0.03272809460759163,
      "learning_rate": 0.0005892761394101876,
      "loss": 0.0977,
      "step": 766
    },
    {
      "epoch": 2.0563002680965146,
      "grad_norm": 0.0349864698946476,
      "learning_rate": 0.000588739946380697,
      "loss": 0.0718,
      "step": 767
    },
    {
      "epoch": 2.058981233243968,
      "grad_norm": 0.05612928047776222,
      "learning_rate": 0.0005882037533512064,
      "loss": 0.0854,
      "step": 768
    },
    {
      "epoch": 2.0616621983914207,
      "grad_norm": 0.05394940450787544,
      "learning_rate": 0.0005876675603217158,
      "loss": 0.0859,
      "step": 769
    },
    {
      "epoch": 2.064343163538874,
      "grad_norm": 0.042839981615543365,
      "learning_rate": 0.0005871313672922251,
      "loss": 0.0825,
      "step": 770
    },
    {
      "epoch": 2.067024128686327,
      "grad_norm": 0.038818251341581345,
      "learning_rate": 0.0005865951742627345,
      "loss": 0.0957,
      "step": 771
    },
    {
      "epoch": 2.06970509383378,
      "grad_norm": 0.035749390721321106,
      "learning_rate": 0.0005860589812332439,
      "loss": 0.0776,
      "step": 772
    },
    {
      "epoch": 2.072386058981233,
      "grad_norm": 0.03299698978662491,
      "learning_rate": 0.0005855227882037533,
      "loss": 0.0786,
      "step": 773
    },
    {
      "epoch": 2.0750670241286864,
      "grad_norm": 0.036197513341903687,
      "learning_rate": 0.0005849865951742628,
      "loss": 0.0747,
      "step": 774
    },
    {
      "epoch": 2.0777479892761392,
      "grad_norm": 0.03660275414586067,
      "learning_rate": 0.000584450402144772,
      "loss": 0.0942,
      "step": 775
    },
    {
      "epoch": 2.0804289544235925,
      "grad_norm": 0.04527246579527855,
      "learning_rate": 0.0005839142091152815,
      "loss": 0.0996,
      "step": 776
    },
    {
      "epoch": 2.0831099195710454,
      "grad_norm": 0.040299467742443085,
      "learning_rate": 0.0005833780160857909,
      "loss": 0.0757,
      "step": 777
    },
    {
      "epoch": 2.0857908847184987,
      "grad_norm": 0.05915040895342827,
      "learning_rate": 0.0005828418230563003,
      "loss": 0.0898,
      "step": 778
    },
    {
      "epoch": 2.0884718498659516,
      "grad_norm": 0.03083060123026371,
      "learning_rate": 0.0005823056300268097,
      "loss": 0.0815,
      "step": 779
    },
    {
      "epoch": 2.091152815013405,
      "grad_norm": 0.03326166048645973,
      "learning_rate": 0.0005817694369973191,
      "loss": 0.0942,
      "step": 780
    },
    {
      "epoch": 2.0938337801608577,
      "grad_norm": 0.05566205084323883,
      "learning_rate": 0.0005812332439678284,
      "loss": 0.0864,
      "step": 781
    },
    {
      "epoch": 2.096514745308311,
      "grad_norm": 0.05487316474318504,
      "learning_rate": 0.0005806970509383378,
      "loss": 0.0942,
      "step": 782
    },
    {
      "epoch": 2.099195710455764,
      "grad_norm": 0.03324287757277489,
      "learning_rate": 0.0005801608579088472,
      "loss": 0.0781,
      "step": 783
    },
    {
      "epoch": 2.1018766756032172,
      "grad_norm": 0.031884972006082535,
      "learning_rate": 0.0005796246648793566,
      "loss": 0.084,
      "step": 784
    },
    {
      "epoch": 2.10455764075067,
      "grad_norm": 0.0285988487303257,
      "learning_rate": 0.000579088471849866,
      "loss": 0.0957,
      "step": 785
    },
    {
      "epoch": 2.1072386058981234,
      "grad_norm": 0.05139920487999916,
      "learning_rate": 0.0005785522788203753,
      "loss": 0.0806,
      "step": 786
    },
    {
      "epoch": 2.1099195710455763,
      "grad_norm": 0.031143812462687492,
      "learning_rate": 0.0005780160857908847,
      "loss": 0.0811,
      "step": 787
    },
    {
      "epoch": 2.1126005361930296,
      "grad_norm": 0.03788631781935692,
      "learning_rate": 0.0005774798927613941,
      "loss": 0.1206,
      "step": 788
    },
    {
      "epoch": 2.1152815013404824,
      "grad_norm": 0.07246872037649155,
      "learning_rate": 0.0005769436997319035,
      "loss": 0.105,
      "step": 789
    },
    {
      "epoch": 2.1179624664879357,
      "grad_norm": 0.043266043066978455,
      "learning_rate": 0.0005764075067024129,
      "loss": 0.0957,
      "step": 790
    },
    {
      "epoch": 2.1206434316353886,
      "grad_norm": 0.032101962715387344,
      "learning_rate": 0.0005758713136729222,
      "loss": 0.0928,
      "step": 791
    },
    {
      "epoch": 2.123324396782842,
      "grad_norm": 0.03318500146269798,
      "learning_rate": 0.0005753351206434316,
      "loss": 0.0654,
      "step": 792
    },
    {
      "epoch": 2.1260053619302948,
      "grad_norm": 0.03730318695306778,
      "learning_rate": 0.000574798927613941,
      "loss": 0.1011,
      "step": 793
    },
    {
      "epoch": 2.128686327077748,
      "grad_norm": 0.031890664249658585,
      "learning_rate": 0.0005742627345844504,
      "loss": 0.0806,
      "step": 794
    },
    {
      "epoch": 2.131367292225201,
      "grad_norm": 0.042605917900800705,
      "learning_rate": 0.0005737265415549598,
      "loss": 0.0918,
      "step": 795
    },
    {
      "epoch": 2.1340482573726542,
      "grad_norm": 0.07057476043701172,
      "learning_rate": 0.0005731903485254691,
      "loss": 0.0977,
      "step": 796
    },
    {
      "epoch": 2.136729222520107,
      "grad_norm": 0.04181036725640297,
      "learning_rate": 0.0005726541554959785,
      "loss": 0.0864,
      "step": 797
    },
    {
      "epoch": 2.1394101876675604,
      "grad_norm": 0.04225916787981987,
      "learning_rate": 0.0005721179624664879,
      "loss": 0.0854,
      "step": 798
    },
    {
      "epoch": 2.1420911528150133,
      "grad_norm": 0.04498101770877838,
      "learning_rate": 0.0005715817694369973,
      "loss": 0.083,
      "step": 799
    },
    {
      "epoch": 2.1447721179624666,
      "grad_norm": 0.054196715354919434,
      "learning_rate": 0.0005710455764075067,
      "loss": 0.0898,
      "step": 800
    },
    {
      "epoch": 2.1474530831099194,
      "grad_norm": 0.03904839977622032,
      "learning_rate": 0.000570509383378016,
      "loss": 0.0684,
      "step": 801
    },
    {
      "epoch": 2.1501340482573728,
      "grad_norm": 0.02953147515654564,
      "learning_rate": 0.0005699731903485254,
      "loss": 0.0879,
      "step": 802
    },
    {
      "epoch": 2.1528150134048256,
      "grad_norm": 0.034660082310438156,
      "learning_rate": 0.0005694369973190349,
      "loss": 0.105,
      "step": 803
    },
    {
      "epoch": 2.155495978552279,
      "grad_norm": 0.03498854488134384,
      "learning_rate": 0.0005689008042895443,
      "loss": 0.0952,
      "step": 804
    },
    {
      "epoch": 2.158176943699732,
      "grad_norm": 0.03896281123161316,
      "learning_rate": 0.0005683646112600537,
      "loss": 0.0781,
      "step": 805
    },
    {
      "epoch": 2.160857908847185,
      "grad_norm": 0.04157179221510887,
      "learning_rate": 0.000567828418230563,
      "loss": 0.0903,
      "step": 806
    },
    {
      "epoch": 2.163538873994638,
      "grad_norm": 0.032259151339530945,
      "learning_rate": 0.0005672922252010724,
      "loss": 0.1021,
      "step": 807
    },
    {
      "epoch": 2.1662198391420913,
      "grad_norm": 0.03174324333667755,
      "learning_rate": 0.0005667560321715818,
      "loss": 0.0771,
      "step": 808
    },
    {
      "epoch": 2.168900804289544,
      "grad_norm": 0.03328792005777359,
      "learning_rate": 0.0005662198391420912,
      "loss": 0.0991,
      "step": 809
    },
    {
      "epoch": 2.1715817694369974,
      "grad_norm": 0.033644065260887146,
      "learning_rate": 0.0005656836461126006,
      "loss": 0.1084,
      "step": 810
    },
    {
      "epoch": 2.1742627345844503,
      "grad_norm": 0.06733627617359161,
      "learning_rate": 0.0005651474530831099,
      "loss": 0.0854,
      "step": 811
    },
    {
      "epoch": 2.1769436997319036,
      "grad_norm": 0.04686626046895981,
      "learning_rate": 0.0005646112600536193,
      "loss": 0.0679,
      "step": 812
    },
    {
      "epoch": 2.1796246648793565,
      "grad_norm": 0.058080222457647324,
      "learning_rate": 0.0005640750670241287,
      "loss": 0.0776,
      "step": 813
    },
    {
      "epoch": 2.1823056300268098,
      "grad_norm": 0.048811398446559906,
      "learning_rate": 0.0005635388739946381,
      "loss": 0.0928,
      "step": 814
    },
    {
      "epoch": 2.1849865951742626,
      "grad_norm": 0.04923953488469124,
      "learning_rate": 0.0005630026809651475,
      "loss": 0.0913,
      "step": 815
    },
    {
      "epoch": 2.187667560321716,
      "grad_norm": 0.04501518979668617,
      "learning_rate": 0.0005624664879356568,
      "loss": 0.1006,
      "step": 816
    },
    {
      "epoch": 2.190348525469169,
      "grad_norm": 0.029153862968087196,
      "learning_rate": 0.0005619302949061662,
      "loss": 0.0752,
      "step": 817
    },
    {
      "epoch": 2.193029490616622,
      "grad_norm": 0.038546644151210785,
      "learning_rate": 0.0005613941018766756,
      "loss": 0.0869,
      "step": 818
    },
    {
      "epoch": 2.195710455764075,
      "grad_norm": 0.03335211053490639,
      "learning_rate": 0.000560857908847185,
      "loss": 0.1011,
      "step": 819
    },
    {
      "epoch": 2.1983914209115283,
      "grad_norm": 0.04229817911982536,
      "learning_rate": 0.0005603217158176944,
      "loss": 0.0791,
      "step": 820
    },
    {
      "epoch": 2.201072386058981,
      "grad_norm": 0.043795496225357056,
      "learning_rate": 0.0005597855227882037,
      "loss": 0.084,
      "step": 821
    },
    {
      "epoch": 2.2037533512064345,
      "grad_norm": 0.049313656985759735,
      "learning_rate": 0.0005592493297587131,
      "loss": 0.0806,
      "step": 822
    },
    {
      "epoch": 2.2064343163538873,
      "grad_norm": 0.0329517163336277,
      "learning_rate": 0.0005587131367292225,
      "loss": 0.082,
      "step": 823
    },
    {
      "epoch": 2.2091152815013406,
      "grad_norm": 0.10225845873355865,
      "learning_rate": 0.0005581769436997319,
      "loss": 0.1133,
      "step": 824
    },
    {
      "epoch": 2.2117962466487935,
      "grad_norm": 0.03261163830757141,
      "learning_rate": 0.0005576407506702413,
      "loss": 0.0796,
      "step": 825
    },
    {
      "epoch": 2.214477211796247,
      "grad_norm": 0.04693250730633736,
      "learning_rate": 0.0005571045576407506,
      "loss": 0.0752,
      "step": 826
    },
    {
      "epoch": 2.2171581769436997,
      "grad_norm": 0.03484063223004341,
      "learning_rate": 0.00055656836461126,
      "loss": 0.0811,
      "step": 827
    },
    {
      "epoch": 2.219839142091153,
      "grad_norm": 0.051512461155653,
      "learning_rate": 0.0005560321715817694,
      "loss": 0.1089,
      "step": 828
    },
    {
      "epoch": 2.222520107238606,
      "grad_norm": 0.03606724366545677,
      "learning_rate": 0.0005554959785522788,
      "loss": 0.0781,
      "step": 829
    },
    {
      "epoch": 2.225201072386059,
      "grad_norm": 0.042556263506412506,
      "learning_rate": 0.0005549597855227883,
      "loss": 0.0986,
      "step": 830
    },
    {
      "epoch": 2.227882037533512,
      "grad_norm": 0.0402558296918869,
      "learning_rate": 0.0005544235924932975,
      "loss": 0.124,
      "step": 831
    },
    {
      "epoch": 2.2305630026809653,
      "grad_norm": 0.032840028405189514,
      "learning_rate": 0.000553887399463807,
      "loss": 0.0889,
      "step": 832
    },
    {
      "epoch": 2.233243967828418,
      "grad_norm": 0.06358606368303299,
      "learning_rate": 0.0005533512064343164,
      "loss": 0.0767,
      "step": 833
    },
    {
      "epoch": 2.2359249329758715,
      "grad_norm": 0.07437573373317719,
      "learning_rate": 0.0005528150134048258,
      "loss": 0.0835,
      "step": 834
    },
    {
      "epoch": 2.2386058981233243,
      "grad_norm": 0.08423195034265518,
      "learning_rate": 0.0005522788203753352,
      "loss": 0.0757,
      "step": 835
    },
    {
      "epoch": 2.2412868632707776,
      "grad_norm": 0.03759710490703583,
      "learning_rate": 0.0005517426273458445,
      "loss": 0.0801,
      "step": 836
    },
    {
      "epoch": 2.2439678284182305,
      "grad_norm": 0.06274528801441193,
      "learning_rate": 0.0005512064343163539,
      "loss": 0.0728,
      "step": 837
    },
    {
      "epoch": 2.246648793565684,
      "grad_norm": 0.048998963087797165,
      "learning_rate": 0.0005506702412868633,
      "loss": 0.0698,
      "step": 838
    },
    {
      "epoch": 2.2493297587131367,
      "grad_norm": 0.055721841752529144,
      "learning_rate": 0.0005501340482573727,
      "loss": 0.0781,
      "step": 839
    },
    {
      "epoch": 2.25201072386059,
      "grad_norm": 0.03229285031557083,
      "learning_rate": 0.0005495978552278821,
      "loss": 0.0894,
      "step": 840
    },
    {
      "epoch": 2.254691689008043,
      "grad_norm": 0.03315029665827751,
      "learning_rate": 0.0005490616621983915,
      "loss": 0.0664,
      "step": 841
    },
    {
      "epoch": 2.257372654155496,
      "grad_norm": 0.08098700642585754,
      "learning_rate": 0.0005485254691689008,
      "loss": 0.083,
      "step": 842
    },
    {
      "epoch": 2.260053619302949,
      "grad_norm": 0.03128039464354515,
      "learning_rate": 0.0005479892761394102,
      "loss": 0.0898,
      "step": 843
    },
    {
      "epoch": 2.2627345844504023,
      "grad_norm": 0.03564932197332382,
      "learning_rate": 0.0005474530831099196,
      "loss": 0.084,
      "step": 844
    },
    {
      "epoch": 2.265415549597855,
      "grad_norm": 0.03421381860971451,
      "learning_rate": 0.000546916890080429,
      "loss": 0.0713,
      "step": 845
    },
    {
      "epoch": 2.2680965147453085,
      "grad_norm": 0.032776832580566406,
      "learning_rate": 0.0005463806970509384,
      "loss": 0.0747,
      "step": 846
    },
    {
      "epoch": 2.2707774798927614,
      "grad_norm": 0.029902128502726555,
      "learning_rate": 0.0005458445040214477,
      "loss": 0.0742,
      "step": 847
    },
    {
      "epoch": 2.2734584450402147,
      "grad_norm": 0.035277996212244034,
      "learning_rate": 0.0005453083109919571,
      "loss": 0.0825,
      "step": 848
    },
    {
      "epoch": 2.2761394101876675,
      "grad_norm": 0.03023644909262657,
      "learning_rate": 0.0005447721179624665,
      "loss": 0.0723,
      "step": 849
    },
    {
      "epoch": 2.278820375335121,
      "grad_norm": 0.06994642317295074,
      "learning_rate": 0.0005442359249329759,
      "loss": 0.0942,
      "step": 850
    },
    {
      "epoch": 2.2815013404825737,
      "grad_norm": 0.03942849859595299,
      "learning_rate": 0.0005436997319034853,
      "loss": 0.0825,
      "step": 851
    },
    {
      "epoch": 2.284182305630027,
      "grad_norm": 0.04563867673277855,
      "learning_rate": 0.0005431635388739946,
      "loss": 0.1187,
      "step": 852
    },
    {
      "epoch": 2.28686327077748,
      "grad_norm": 0.051533445715904236,
      "learning_rate": 0.000542627345844504,
      "loss": 0.0762,
      "step": 853
    },
    {
      "epoch": 2.289544235924933,
      "grad_norm": 0.03819504752755165,
      "learning_rate": 0.0005420911528150134,
      "loss": 0.0732,
      "step": 854
    },
    {
      "epoch": 2.292225201072386,
      "grad_norm": 0.0456085279583931,
      "learning_rate": 0.0005415549597855228,
      "loss": 0.0977,
      "step": 855
    },
    {
      "epoch": 2.294906166219839,
      "grad_norm": 0.06104753911495209,
      "learning_rate": 0.0005410187667560322,
      "loss": 0.0771,
      "step": 856
    },
    {
      "epoch": 2.297587131367292,
      "grad_norm": 0.0399252288043499,
      "learning_rate": 0.0005404825737265415,
      "loss": 0.0952,
      "step": 857
    },
    {
      "epoch": 2.3002680965147455,
      "grad_norm": 0.034094683825969696,
      "learning_rate": 0.000539946380697051,
      "loss": 0.0815,
      "step": 858
    },
    {
      "epoch": 2.3029490616621984,
      "grad_norm": 0.027968382462859154,
      "learning_rate": 0.0005394101876675604,
      "loss": 0.0786,
      "step": 859
    },
    {
      "epoch": 2.3056300268096512,
      "grad_norm": 0.03289773687720299,
      "learning_rate": 0.0005388739946380698,
      "loss": 0.0835,
      "step": 860
    },
    {
      "epoch": 2.3083109919571045,
      "grad_norm": 0.1652454137802124,
      "learning_rate": 0.0005383378016085792,
      "loss": 0.0952,
      "step": 861
    },
    {
      "epoch": 2.310991957104558,
      "grad_norm": 0.04320806637406349,
      "learning_rate": 0.0005378016085790885,
      "loss": 0.0957,
      "step": 862
    },
    {
      "epoch": 2.3136729222520107,
      "grad_norm": 0.04303862527012825,
      "learning_rate": 0.0005372654155495979,
      "loss": 0.0894,
      "step": 863
    },
    {
      "epoch": 2.3163538873994636,
      "grad_norm": 0.040943849831819534,
      "learning_rate": 0.0005367292225201073,
      "loss": 0.0825,
      "step": 864
    },
    {
      "epoch": 2.319034852546917,
      "grad_norm": 0.036241866648197174,
      "learning_rate": 0.0005361930294906167,
      "loss": 0.0845,
      "step": 865
    },
    {
      "epoch": 2.32171581769437,
      "grad_norm": 0.07375852763652802,
      "learning_rate": 0.0005356568364611261,
      "loss": 0.1079,
      "step": 866
    },
    {
      "epoch": 2.324396782841823,
      "grad_norm": 0.055429697036743164,
      "learning_rate": 0.0005351206434316354,
      "loss": 0.0757,
      "step": 867
    },
    {
      "epoch": 2.327077747989276,
      "grad_norm": 0.04170104116201401,
      "learning_rate": 0.0005345844504021448,
      "loss": 0.0928,
      "step": 868
    },
    {
      "epoch": 2.329758713136729,
      "grad_norm": 0.0324278324842453,
      "learning_rate": 0.0005340482573726542,
      "loss": 0.0815,
      "step": 869
    },
    {
      "epoch": 2.3324396782841825,
      "grad_norm": 0.04998921602964401,
      "learning_rate": 0.0005335120643431636,
      "loss": 0.0918,
      "step": 870
    },
    {
      "epoch": 2.3351206434316354,
      "grad_norm": 0.033685605973005295,
      "learning_rate": 0.000532975871313673,
      "loss": 0.0771,
      "step": 871
    },
    {
      "epoch": 2.3378016085790883,
      "grad_norm": 0.03649421036243439,
      "learning_rate": 0.0005324396782841823,
      "loss": 0.0845,
      "step": 872
    },
    {
      "epoch": 2.3404825737265416,
      "grad_norm": 0.04425225034356117,
      "learning_rate": 0.0005319034852546917,
      "loss": 0.0923,
      "step": 873
    },
    {
      "epoch": 2.343163538873995,
      "grad_norm": 0.034396544098854065,
      "learning_rate": 0.0005313672922252011,
      "loss": 0.084,
      "step": 874
    },
    {
      "epoch": 2.3458445040214477,
      "grad_norm": 0.04160553589463234,
      "learning_rate": 0.0005308310991957105,
      "loss": 0.0938,
      "step": 875
    },
    {
      "epoch": 2.3485254691689006,
      "grad_norm": 0.03773968666791916,
      "learning_rate": 0.0005302949061662199,
      "loss": 0.0918,
      "step": 876
    },
    {
      "epoch": 2.351206434316354,
      "grad_norm": 0.03922989219427109,
      "learning_rate": 0.0005297587131367292,
      "loss": 0.0669,
      "step": 877
    },
    {
      "epoch": 2.353887399463807,
      "grad_norm": 0.031548600643873215,
      "learning_rate": 0.0005292225201072386,
      "loss": 0.084,
      "step": 878
    },
    {
      "epoch": 2.35656836461126,
      "grad_norm": 0.03844838589429855,
      "learning_rate": 0.000528686327077748,
      "loss": 0.0908,
      "step": 879
    },
    {
      "epoch": 2.359249329758713,
      "grad_norm": 0.045294880867004395,
      "learning_rate": 0.0005281501340482574,
      "loss": 0.0747,
      "step": 880
    },
    {
      "epoch": 2.3619302949061662,
      "grad_norm": 0.03127607703208923,
      "learning_rate": 0.0005276139410187668,
      "loss": 0.0894,
      "step": 881
    },
    {
      "epoch": 2.3646112600536195,
      "grad_norm": 0.03715170919895172,
      "learning_rate": 0.0005270777479892761,
      "loss": 0.0801,
      "step": 882
    },
    {
      "epoch": 2.3672922252010724,
      "grad_norm": 0.03258066624403,
      "learning_rate": 0.0005265415549597855,
      "loss": 0.0854,
      "step": 883
    },
    {
      "epoch": 2.3699731903485253,
      "grad_norm": 0.036446329206228256,
      "learning_rate": 0.0005260053619302949,
      "loss": 0.0825,
      "step": 884
    },
    {
      "epoch": 2.3726541554959786,
      "grad_norm": 0.03712620213627815,
      "learning_rate": 0.0005254691689008043,
      "loss": 0.0723,
      "step": 885
    },
    {
      "epoch": 2.3753351206434314,
      "grad_norm": 0.0445614829659462,
      "learning_rate": 0.0005249329758713138,
      "loss": 0.083,
      "step": 886
    },
    {
      "epoch": 2.3780160857908847,
      "grad_norm": 0.06324391812086105,
      "learning_rate": 0.000524396782841823,
      "loss": 0.0889,
      "step": 887
    },
    {
      "epoch": 2.3806970509383376,
      "grad_norm": 0.04107050597667694,
      "learning_rate": 0.0005238605898123325,
      "loss": 0.0957,
      "step": 888
    },
    {
      "epoch": 2.383378016085791,
      "grad_norm": 0.03606544807553291,
      "learning_rate": 0.0005233243967828419,
      "loss": 0.0811,
      "step": 889
    },
    {
      "epoch": 2.386058981233244,
      "grad_norm": 0.03712540864944458,
      "learning_rate": 0.0005227882037533513,
      "loss": 0.085,
      "step": 890
    },
    {
      "epoch": 2.388739946380697,
      "grad_norm": 0.11446812748908997,
      "learning_rate": 0.0005222520107238607,
      "loss": 0.0874,
      "step": 891
    },
    {
      "epoch": 2.39142091152815,
      "grad_norm": 0.031547050923109055,
      "learning_rate": 0.00052171581769437,
      "loss": 0.0747,
      "step": 892
    },
    {
      "epoch": 2.3941018766756033,
      "grad_norm": 0.034981995820999146,
      "learning_rate": 0.0005211796246648794,
      "loss": 0.0596,
      "step": 893
    },
    {
      "epoch": 2.396782841823056,
      "grad_norm": 0.03033776767551899,
      "learning_rate": 0.0005206434316353888,
      "loss": 0.0762,
      "step": 894
    },
    {
      "epoch": 2.3994638069705094,
      "grad_norm": 0.056878089904785156,
      "learning_rate": 0.0005201072386058982,
      "loss": 0.0771,
      "step": 895
    },
    {
      "epoch": 2.4021447721179623,
      "grad_norm": 0.043199218809604645,
      "learning_rate": 0.0005195710455764076,
      "loss": 0.0703,
      "step": 896
    },
    {
      "epoch": 2.4048257372654156,
      "grad_norm": 0.029607243835926056,
      "learning_rate": 0.0005190348525469169,
      "loss": 0.0664,
      "step": 897
    },
    {
      "epoch": 2.4075067024128685,
      "grad_norm": 0.04446909576654434,
      "learning_rate": 0.0005184986595174263,
      "loss": 0.103,
      "step": 898
    },
    {
      "epoch": 2.4101876675603218,
      "grad_norm": 0.03762441873550415,
      "learning_rate": 0.0005179624664879357,
      "loss": 0.083,
      "step": 899
    },
    {
      "epoch": 2.4128686327077746,
      "grad_norm": 0.05300010368227959,
      "learning_rate": 0.0005174262734584451,
      "loss": 0.0806,
      "step": 900
    },
    {
      "epoch": 2.415549597855228,
      "grad_norm": 0.03600887954235077,
      "learning_rate": 0.0005168900804289545,
      "loss": 0.0791,
      "step": 901
    },
    {
      "epoch": 2.418230563002681,
      "grad_norm": 0.05163959413766861,
      "learning_rate": 0.0005163538873994639,
      "loss": 0.0859,
      "step": 902
    },
    {
      "epoch": 2.420911528150134,
      "grad_norm": 0.06449629366397858,
      "learning_rate": 0.0005158176943699732,
      "loss": 0.0864,
      "step": 903
    },
    {
      "epoch": 2.423592493297587,
      "grad_norm": 0.17042610049247742,
      "learning_rate": 0.0005152815013404826,
      "loss": 0.0757,
      "step": 904
    },
    {
      "epoch": 2.4262734584450403,
      "grad_norm": 0.030833955854177475,
      "learning_rate": 0.000514745308310992,
      "loss": 0.0801,
      "step": 905
    },
    {
      "epoch": 2.428954423592493,
      "grad_norm": 0.04969592019915581,
      "learning_rate": 0.0005142091152815014,
      "loss": 0.0942,
      "step": 906
    },
    {
      "epoch": 2.4316353887399464,
      "grad_norm": 0.033548422157764435,
      "learning_rate": 0.0005136729222520108,
      "loss": 0.0815,
      "step": 907
    },
    {
      "epoch": 2.4343163538873993,
      "grad_norm": 0.041085485368967056,
      "learning_rate": 0.0005131367292225201,
      "loss": 0.0967,
      "step": 908
    },
    {
      "epoch": 2.4369973190348526,
      "grad_norm": 0.030732743442058563,
      "learning_rate": 0.0005126005361930295,
      "loss": 0.0796,
      "step": 909
    },
    {
      "epoch": 2.4396782841823055,
      "grad_norm": 0.04744518920779228,
      "learning_rate": 0.0005120643431635389,
      "loss": 0.0864,
      "step": 910
    },
    {
      "epoch": 2.442359249329759,
      "grad_norm": 0.03321860730648041,
      "learning_rate": 0.0005115281501340483,
      "loss": 0.083,
      "step": 911
    },
    {
      "epoch": 2.4450402144772116,
      "grad_norm": 0.03522844985127449,
      "learning_rate": 0.0005109919571045577,
      "loss": 0.0762,
      "step": 912
    },
    {
      "epoch": 2.447721179624665,
      "grad_norm": 0.036423467099666595,
      "learning_rate": 0.000510455764075067,
      "loss": 0.0713,
      "step": 913
    },
    {
      "epoch": 2.450402144772118,
      "grad_norm": 0.03411766514182091,
      "learning_rate": 0.0005099195710455764,
      "loss": 0.0894,
      "step": 914
    },
    {
      "epoch": 2.453083109919571,
      "grad_norm": 0.032221630215644836,
      "learning_rate": 0.0005093833780160859,
      "loss": 0.0801,
      "step": 915
    },
    {
      "epoch": 2.455764075067024,
      "grad_norm": 0.0723690539598465,
      "learning_rate": 0.0005088471849865953,
      "loss": 0.0972,
      "step": 916
    },
    {
      "epoch": 2.4584450402144773,
      "grad_norm": 0.07299646735191345,
      "learning_rate": 0.0005083109919571047,
      "loss": 0.0713,
      "step": 917
    },
    {
      "epoch": 2.46112600536193,
      "grad_norm": 0.05316649377346039,
      "learning_rate": 0.000507774798927614,
      "loss": 0.0815,
      "step": 918
    },
    {
      "epoch": 2.4638069705093835,
      "grad_norm": 0.07026220858097076,
      "learning_rate": 0.0005072386058981234,
      "loss": 0.0776,
      "step": 919
    },
    {
      "epoch": 2.4664879356568363,
      "grad_norm": 0.042193662375211716,
      "learning_rate": 0.0005067024128686328,
      "loss": 0.0879,
      "step": 920
    },
    {
      "epoch": 2.4691689008042896,
      "grad_norm": 0.031097421422600746,
      "learning_rate": 0.0005061662198391422,
      "loss": 0.0859,
      "step": 921
    },
    {
      "epoch": 2.4718498659517425,
      "grad_norm": 0.0680055245757103,
      "learning_rate": 0.0005056300268096516,
      "loss": 0.0669,
      "step": 922
    },
    {
      "epoch": 2.474530831099196,
      "grad_norm": 0.03323804587125778,
      "learning_rate": 0.0005050938337801609,
      "loss": 0.0835,
      "step": 923
    },
    {
      "epoch": 2.4772117962466487,
      "grad_norm": 0.03791726008057594,
      "learning_rate": 0.0005045576407506703,
      "loss": 0.0859,
      "step": 924
    },
    {
      "epoch": 2.479892761394102,
      "grad_norm": 0.03308619186282158,
      "learning_rate": 0.0005040214477211797,
      "loss": 0.0791,
      "step": 925
    },
    {
      "epoch": 2.482573726541555,
      "grad_norm": 0.03673257306218147,
      "learning_rate": 0.0005034852546916891,
      "loss": 0.0938,
      "step": 926
    },
    {
      "epoch": 2.485254691689008,
      "grad_norm": 0.079349584877491,
      "learning_rate": 0.0005029490616621985,
      "loss": 0.0791,
      "step": 927
    },
    {
      "epoch": 2.487935656836461,
      "grad_norm": 0.03134139999747276,
      "learning_rate": 0.0005024128686327078,
      "loss": 0.0693,
      "step": 928
    },
    {
      "epoch": 2.4906166219839143,
      "grad_norm": 0.03280682861804962,
      "learning_rate": 0.0005018766756032172,
      "loss": 0.0747,
      "step": 929
    },
    {
      "epoch": 2.493297587131367,
      "grad_norm": 0.03331838920712471,
      "learning_rate": 0.0005013404825737266,
      "loss": 0.0845,
      "step": 930
    },
    {
      "epoch": 2.4959785522788205,
      "grad_norm": 0.030452143400907516,
      "learning_rate": 0.000500804289544236,
      "loss": 0.0884,
      "step": 931
    },
    {
      "epoch": 2.4986595174262733,
      "grad_norm": 0.07069593667984009,
      "learning_rate": 0.0005002680965147454,
      "loss": 0.0972,
      "step": 932
    },
    {
      "epoch": 2.5013404825737267,
      "grad_norm": 0.0656808540225029,
      "learning_rate": 0.0004997319034852547,
      "loss": 0.0894,
      "step": 933
    },
    {
      "epoch": 2.5040214477211795,
      "grad_norm": 0.04986843466758728,
      "learning_rate": 0.000499195710455764,
      "loss": 0.0898,
      "step": 934
    },
    {
      "epoch": 2.506702412868633,
      "grad_norm": 0.06059977412223816,
      "learning_rate": 0.0004986595174262734,
      "loss": 0.0781,
      "step": 935
    },
    {
      "epoch": 2.5093833780160857,
      "grad_norm": 0.03439248725771904,
      "learning_rate": 0.0004981233243967828,
      "loss": 0.0781,
      "step": 936
    },
    {
      "epoch": 2.512064343163539,
      "grad_norm": 0.052568960934877396,
      "learning_rate": 0.0004975871313672922,
      "loss": 0.0747,
      "step": 937
    },
    {
      "epoch": 2.514745308310992,
      "grad_norm": 0.04947372525930405,
      "learning_rate": 0.0004970509383378016,
      "loss": 0.0581,
      "step": 938
    },
    {
      "epoch": 2.517426273458445,
      "grad_norm": 0.031085053458809853,
      "learning_rate": 0.0004965147453083109,
      "loss": 0.0752,
      "step": 939
    },
    {
      "epoch": 2.520107238605898,
      "grad_norm": 0.07879003137350082,
      "learning_rate": 0.0004959785522788203,
      "loss": 0.062,
      "step": 940
    },
    {
      "epoch": 2.5227882037533513,
      "grad_norm": 0.03165212646126747,
      "learning_rate": 0.0004954423592493297,
      "loss": 0.0869,
      "step": 941
    },
    {
      "epoch": 2.525469168900804,
      "grad_norm": 0.035223327577114105,
      "learning_rate": 0.0004949061662198391,
      "loss": 0.0703,
      "step": 942
    },
    {
      "epoch": 2.5281501340482575,
      "grad_norm": 0.03868938982486725,
      "learning_rate": 0.0004943699731903485,
      "loss": 0.0864,
      "step": 943
    },
    {
      "epoch": 2.5308310991957104,
      "grad_norm": 0.031144825741648674,
      "learning_rate": 0.0004938337801608578,
      "loss": 0.0801,
      "step": 944
    },
    {
      "epoch": 2.5335120643431637,
      "grad_norm": 0.0464402511715889,
      "learning_rate": 0.0004932975871313673,
      "loss": 0.0737,
      "step": 945
    },
    {
      "epoch": 2.5361930294906165,
      "grad_norm": 0.04566359147429466,
      "learning_rate": 0.0004927613941018767,
      "loss": 0.0864,
      "step": 946
    },
    {
      "epoch": 2.53887399463807,
      "grad_norm": 0.03489857539534569,
      "learning_rate": 0.0004922252010723861,
      "loss": 0.0913,
      "step": 947
    },
    {
      "epoch": 2.5415549597855227,
      "grad_norm": 0.03278469666838646,
      "learning_rate": 0.0004916890080428955,
      "loss": 0.0913,
      "step": 948
    },
    {
      "epoch": 2.544235924932976,
      "grad_norm": 0.04365338385105133,
      "learning_rate": 0.0004911528150134049,
      "loss": 0.0898,
      "step": 949
    },
    {
      "epoch": 2.546916890080429,
      "grad_norm": 0.029206858947873116,
      "learning_rate": 0.0004906166219839142,
      "loss": 0.0879,
      "step": 950
    },
    {
      "epoch": 2.549597855227882,
      "grad_norm": 0.037212420254945755,
      "learning_rate": 0.0004900804289544236,
      "loss": 0.0854,
      "step": 951
    },
    {
      "epoch": 2.552278820375335,
      "grad_norm": 0.0352712981402874,
      "learning_rate": 0.000489544235924933,
      "loss": 0.0591,
      "step": 952
    },
    {
      "epoch": 2.5549597855227884,
      "grad_norm": 0.04692136123776436,
      "learning_rate": 0.0004890080428954424,
      "loss": 0.082,
      "step": 953
    },
    {
      "epoch": 2.557640750670241,
      "grad_norm": 0.02897484228014946,
      "learning_rate": 0.0004884718498659518,
      "loss": 0.0649,
      "step": 954
    },
    {
      "epoch": 2.5603217158176945,
      "grad_norm": 0.034926243126392365,
      "learning_rate": 0.00048793565683646114,
      "loss": 0.0869,
      "step": 955
    },
    {
      "epoch": 2.5630026809651474,
      "grad_norm": 0.22672568261623383,
      "learning_rate": 0.00048739946380697055,
      "loss": 0.1001,
      "step": 956
    },
    {
      "epoch": 2.5656836461126007,
      "grad_norm": 0.03080684132874012,
      "learning_rate": 0.0004868632707774799,
      "loss": 0.0752,
      "step": 957
    },
    {
      "epoch": 2.5683646112600536,
      "grad_norm": 0.03376643732190132,
      "learning_rate": 0.0004863270777479893,
      "loss": 0.0654,
      "step": 958
    },
    {
      "epoch": 2.571045576407507,
      "grad_norm": 0.0361662395298481,
      "learning_rate": 0.0004857908847184987,
      "loss": 0.084,
      "step": 959
    },
    {
      "epoch": 2.5737265415549597,
      "grad_norm": 0.05537430942058563,
      "learning_rate": 0.00048525469168900806,
      "loss": 0.0806,
      "step": 960
    },
    {
      "epoch": 2.576407506702413,
      "grad_norm": 0.038983382284641266,
      "learning_rate": 0.00048471849865951746,
      "loss": 0.0732,
      "step": 961
    },
    {
      "epoch": 2.579088471849866,
      "grad_norm": 0.040517695248126984,
      "learning_rate": 0.0004841823056300268,
      "loss": 0.0698,
      "step": 962
    },
    {
      "epoch": 2.581769436997319,
      "grad_norm": 0.05526214838027954,
      "learning_rate": 0.0004836461126005362,
      "loss": 0.0957,
      "step": 963
    },
    {
      "epoch": 2.584450402144772,
      "grad_norm": 0.043390434235334396,
      "learning_rate": 0.0004831099195710456,
      "loss": 0.0874,
      "step": 964
    },
    {
      "epoch": 2.5871313672922254,
      "grad_norm": 0.04111999273300171,
      "learning_rate": 0.000482573726541555,
      "loss": 0.0815,
      "step": 965
    },
    {
      "epoch": 2.5898123324396782,
      "grad_norm": 0.060075607150793076,
      "learning_rate": 0.0004820375335120644,
      "loss": 0.082,
      "step": 966
    },
    {
      "epoch": 2.592493297587131,
      "grad_norm": 0.04818848520517349,
      "learning_rate": 0.00048150134048257373,
      "loss": 0.0718,
      "step": 967
    },
    {
      "epoch": 2.5951742627345844,
      "grad_norm": 0.047483719885349274,
      "learning_rate": 0.00048096514745308314,
      "loss": 0.0752,
      "step": 968
    },
    {
      "epoch": 2.5978552278820377,
      "grad_norm": 0.04930832237005234,
      "learning_rate": 0.00048042895442359254,
      "loss": 0.0879,
      "step": 969
    },
    {
      "epoch": 2.6005361930294906,
      "grad_norm": 0.03939960151910782,
      "learning_rate": 0.0004798927613941019,
      "loss": 0.0898,
      "step": 970
    },
    {
      "epoch": 2.6032171581769434,
      "grad_norm": 0.035421498119831085,
      "learning_rate": 0.0004793565683646113,
      "loss": 0.1016,
      "step": 971
    },
    {
      "epoch": 2.6058981233243967,
      "grad_norm": 0.04112277179956436,
      "learning_rate": 0.00047882037533512065,
      "loss": 0.0952,
      "step": 972
    },
    {
      "epoch": 2.60857908847185,
      "grad_norm": 0.03933054953813553,
      "learning_rate": 0.00047828418230563005,
      "loss": 0.083,
      "step": 973
    },
    {
      "epoch": 2.611260053619303,
      "grad_norm": 0.03497800976037979,
      "learning_rate": 0.00047774798927613946,
      "loss": 0.0732,
      "step": 974
    },
    {
      "epoch": 2.6139410187667558,
      "grad_norm": 0.03845300152897835,
      "learning_rate": 0.0004772117962466488,
      "loss": 0.1001,
      "step": 975
    },
    {
      "epoch": 2.616621983914209,
      "grad_norm": 0.03700267896056175,
      "learning_rate": 0.0004766756032171582,
      "loss": 0.084,
      "step": 976
    },
    {
      "epoch": 2.6193029490616624,
      "grad_norm": 0.03986675664782524,
      "learning_rate": 0.00047613941018766757,
      "loss": 0.0767,
      "step": 977
    },
    {
      "epoch": 2.6219839142091153,
      "grad_norm": 0.031253695487976074,
      "learning_rate": 0.00047560321715817697,
      "loss": 0.0845,
      "step": 978
    },
    {
      "epoch": 2.624664879356568,
      "grad_norm": 0.02897724322974682,
      "learning_rate": 0.0004750670241286864,
      "loss": 0.0659,
      "step": 979
    },
    {
      "epoch": 2.6273458445040214,
      "grad_norm": 0.04062271490693092,
      "learning_rate": 0.00047453083109919573,
      "loss": 0.0869,
      "step": 980
    },
    {
      "epoch": 2.6300268096514747,
      "grad_norm": 0.11298771947622299,
      "learning_rate": 0.00047399463806970513,
      "loss": 0.0923,
      "step": 981
    },
    {
      "epoch": 2.6327077747989276,
      "grad_norm": 0.053234368562698364,
      "learning_rate": 0.00047345844504021454,
      "loss": 0.0776,
      "step": 982
    },
    {
      "epoch": 2.6353887399463805,
      "grad_norm": 0.07163837552070618,
      "learning_rate": 0.0004729222520107239,
      "loss": 0.0786,
      "step": 983
    },
    {
      "epoch": 2.6380697050938338,
      "grad_norm": 0.03232762962579727,
      "learning_rate": 0.0004723860589812333,
      "loss": 0.0859,
      "step": 984
    },
    {
      "epoch": 2.640750670241287,
      "grad_norm": 0.04475165531039238,
      "learning_rate": 0.0004718498659517426,
      "loss": 0.0623,
      "step": 985
    },
    {
      "epoch": 2.64343163538874,
      "grad_norm": 0.031020859256386757,
      "learning_rate": 0.000471313672922252,
      "loss": 0.0732,
      "step": 986
    },
    {
      "epoch": 2.646112600536193,
      "grad_norm": 0.034749507904052734,
      "learning_rate": 0.0004707774798927614,
      "loss": 0.0815,
      "step": 987
    },
    {
      "epoch": 2.648793565683646,
      "grad_norm": 0.03299165144562721,
      "learning_rate": 0.00047024128686327075,
      "loss": 0.0693,
      "step": 988
    },
    {
      "epoch": 2.6514745308310994,
      "grad_norm": 0.03954298049211502,
      "learning_rate": 0.00046970509383378016,
      "loss": 0.0786,
      "step": 989
    },
    {
      "epoch": 2.6541554959785523,
      "grad_norm": 0.03215185925364494,
      "learning_rate": 0.0004691689008042895,
      "loss": 0.0615,
      "step": 990
    },
    {
      "epoch": 2.656836461126005,
      "grad_norm": 0.03450610861182213,
      "learning_rate": 0.0004686327077747989,
      "loss": 0.083,
      "step": 991
    },
    {
      "epoch": 2.6595174262734584,
      "grad_norm": 0.039525460451841354,
      "learning_rate": 0.0004680965147453083,
      "loss": 0.0781,
      "step": 992
    },
    {
      "epoch": 2.6621983914209117,
      "grad_norm": 0.033783383667469025,
      "learning_rate": 0.00046756032171581767,
      "loss": 0.0684,
      "step": 993
    },
    {
      "epoch": 2.6648793565683646,
      "grad_norm": 0.07378853857517242,
      "learning_rate": 0.0004670241286863271,
      "loss": 0.0732,
      "step": 994
    },
    {
      "epoch": 2.6675603217158175,
      "grad_norm": 0.04803871363401413,
      "learning_rate": 0.00046648793565683643,
      "loss": 0.0806,
      "step": 995
    },
    {
      "epoch": 2.670241286863271,
      "grad_norm": 0.0473891943693161,
      "learning_rate": 0.00046595174262734583,
      "loss": 0.0608,
      "step": 996
    },
    {
      "epoch": 2.672922252010724,
      "grad_norm": 0.027387388050556183,
      "learning_rate": 0.00046541554959785524,
      "loss": 0.0781,
      "step": 997
    },
    {
      "epoch": 2.675603217158177,
      "grad_norm": 0.058801863342523575,
      "learning_rate": 0.0004648793565683646,
      "loss": 0.062,
      "step": 998
    },
    {
      "epoch": 2.67828418230563,
      "grad_norm": 0.06370819360017776,
      "learning_rate": 0.000464343163538874,
      "loss": 0.0801,
      "step": 999
    },
    {
      "epoch": 2.680965147453083,
      "grad_norm": 0.08050252497196198,
      "learning_rate": 0.00046380697050938335,
      "loss": 0.1025,
      "step": 1000
    },
    {
      "epoch": 2.6836461126005364,
      "grad_norm": 0.03231462836265564,
      "learning_rate": 0.00046327077747989275,
      "loss": 0.0737,
      "step": 1001
    },
    {
      "epoch": 2.6863270777479893,
      "grad_norm": 0.04999430477619171,
      "learning_rate": 0.00046273458445040216,
      "loss": 0.084,
      "step": 1002
    },
    {
      "epoch": 2.689008042895442,
      "grad_norm": 0.0440647229552269,
      "learning_rate": 0.0004621983914209115,
      "loss": 0.0859,
      "step": 1003
    },
    {
      "epoch": 2.6916890080428955,
      "grad_norm": 0.03151547536253929,
      "learning_rate": 0.0004616621983914209,
      "loss": 0.0957,
      "step": 1004
    },
    {
      "epoch": 2.6943699731903488,
      "grad_norm": 0.029061518609523773,
      "learning_rate": 0.00046112600536193026,
      "loss": 0.0601,
      "step": 1005
    },
    {
      "epoch": 2.6970509383378016,
      "grad_norm": 0.02973254770040512,
      "learning_rate": 0.00046058981233243967,
      "loss": 0.084,
      "step": 1006
    },
    {
      "epoch": 2.6997319034852545,
      "grad_norm": 0.039933279156684875,
      "learning_rate": 0.0004600536193029491,
      "loss": 0.0962,
      "step": 1007
    },
    {
      "epoch": 2.702412868632708,
      "grad_norm": 0.04637759178876877,
      "learning_rate": 0.0004595174262734584,
      "loss": 0.0869,
      "step": 1008
    },
    {
      "epoch": 2.705093833780161,
      "grad_norm": 0.037191569805145264,
      "learning_rate": 0.00045898123324396783,
      "loss": 0.1064,
      "step": 1009
    },
    {
      "epoch": 2.707774798927614,
      "grad_norm": 0.10644633322954178,
      "learning_rate": 0.0004584450402144772,
      "loss": 0.083,
      "step": 1010
    },
    {
      "epoch": 2.710455764075067,
      "grad_norm": 0.03369465097784996,
      "learning_rate": 0.0004579088471849866,
      "loss": 0.0815,
      "step": 1011
    },
    {
      "epoch": 2.71313672922252,
      "grad_norm": 0.03413769602775574,
      "learning_rate": 0.000457372654155496,
      "loss": 0.0859,
      "step": 1012
    },
    {
      "epoch": 2.7158176943699734,
      "grad_norm": 0.02774645946919918,
      "learning_rate": 0.00045683646112600534,
      "loss": 0.0747,
      "step": 1013
    },
    {
      "epoch": 2.7184986595174263,
      "grad_norm": 0.0507267601788044,
      "learning_rate": 0.00045630026809651475,
      "loss": 0.0659,
      "step": 1014
    },
    {
      "epoch": 2.721179624664879,
      "grad_norm": 0.03088841587305069,
      "learning_rate": 0.00045576407506702415,
      "loss": 0.0703,
      "step": 1015
    },
    {
      "epoch": 2.7238605898123325,
      "grad_norm": 0.03674368932843208,
      "learning_rate": 0.0004552278820375335,
      "loss": 0.0869,
      "step": 1016
    },
    {
      "epoch": 2.726541554959786,
      "grad_norm": 0.03926585614681244,
      "learning_rate": 0.0004546916890080429,
      "loss": 0.0688,
      "step": 1017
    },
    {
      "epoch": 2.7292225201072386,
      "grad_norm": 0.03078903630375862,
      "learning_rate": 0.00045415549597855226,
      "loss": 0.0742,
      "step": 1018
    },
    {
      "epoch": 2.7319034852546915,
      "grad_norm": 0.03132256492972374,
      "learning_rate": 0.00045361930294906167,
      "loss": 0.0903,
      "step": 1019
    },
    {
      "epoch": 2.734584450402145,
      "grad_norm": 0.03946792706847191,
      "learning_rate": 0.00045308310991957107,
      "loss": 0.0679,
      "step": 1020
    },
    {
      "epoch": 2.737265415549598,
      "grad_norm": 0.04852665215730667,
      "learning_rate": 0.0004525469168900804,
      "loss": 0.063,
      "step": 1021
    },
    {
      "epoch": 2.739946380697051,
      "grad_norm": 0.05886615812778473,
      "learning_rate": 0.0004520107238605898,
      "loss": 0.0859,
      "step": 1022
    },
    {
      "epoch": 2.742627345844504,
      "grad_norm": 0.042024776339530945,
      "learning_rate": 0.0004514745308310992,
      "loss": 0.1064,
      "step": 1023
    },
    {
      "epoch": 2.745308310991957,
      "grad_norm": 0.04712005704641342,
      "learning_rate": 0.0004509383378016086,
      "loss": 0.0723,
      "step": 1024
    },
    {
      "epoch": 2.7479892761394105,
      "grad_norm": 0.035773102194070816,
      "learning_rate": 0.000450402144772118,
      "loss": 0.0825,
      "step": 1025
    },
    {
      "epoch": 2.7506702412868633,
      "grad_norm": 0.040228888392448425,
      "learning_rate": 0.00044986595174262734,
      "loss": 0.0693,
      "step": 1026
    },
    {
      "epoch": 2.753351206434316,
      "grad_norm": 0.032590318471193314,
      "learning_rate": 0.00044932975871313674,
      "loss": 0.0859,
      "step": 1027
    },
    {
      "epoch": 2.7560321715817695,
      "grad_norm": 0.0362604558467865,
      "learning_rate": 0.0004487935656836461,
      "loss": 0.0933,
      "step": 1028
    },
    {
      "epoch": 2.7587131367292224,
      "grad_norm": 0.040936391800642014,
      "learning_rate": 0.0004482573726541555,
      "loss": 0.0801,
      "step": 1029
    },
    {
      "epoch": 2.7613941018766757,
      "grad_norm": 0.03555183857679367,
      "learning_rate": 0.0004477211796246649,
      "loss": 0.0928,
      "step": 1030
    },
    {
      "epoch": 2.7640750670241285,
      "grad_norm": 0.052926015108823776,
      "learning_rate": 0.00044718498659517426,
      "loss": 0.0889,
      "step": 1031
    },
    {
      "epoch": 2.766756032171582,
      "grad_norm": 0.06223062053322792,
      "learning_rate": 0.00044664879356568366,
      "loss": 0.0771,
      "step": 1032
    },
    {
      "epoch": 2.7694369973190347,
      "grad_norm": 0.05688551813364029,
      "learning_rate": 0.000446112600536193,
      "loss": 0.0605,
      "step": 1033
    },
    {
      "epoch": 2.772117962466488,
      "grad_norm": 0.0625266432762146,
      "learning_rate": 0.0004455764075067024,
      "loss": 0.0962,
      "step": 1034
    },
    {
      "epoch": 2.774798927613941,
      "grad_norm": 0.05045560002326965,
      "learning_rate": 0.0004450402144772118,
      "loss": 0.0903,
      "step": 1035
    },
    {
      "epoch": 2.777479892761394,
      "grad_norm": 0.036591432988643646,
      "learning_rate": 0.0004445040214477212,
      "loss": 0.0757,
      "step": 1036
    },
    {
      "epoch": 2.780160857908847,
      "grad_norm": 0.027004383504390717,
      "learning_rate": 0.0004439678284182306,
      "loss": 0.0654,
      "step": 1037
    },
    {
      "epoch": 2.7828418230563003,
      "grad_norm": 0.03382125869393349,
      "learning_rate": 0.00044343163538873993,
      "loss": 0.0884,
      "step": 1038
    },
    {
      "epoch": 2.785522788203753,
      "grad_norm": 0.059165533632040024,
      "learning_rate": 0.00044289544235924934,
      "loss": 0.0835,
      "step": 1039
    },
    {
      "epoch": 2.7882037533512065,
      "grad_norm": 0.02429327741265297,
      "learning_rate": 0.00044235924932975874,
      "loss": 0.0659,
      "step": 1040
    },
    {
      "epoch": 2.7908847184986594,
      "grad_norm": 0.08322098106145859,
      "learning_rate": 0.0004418230563002681,
      "loss": 0.1055,
      "step": 1041
    },
    {
      "epoch": 2.7935656836461127,
      "grad_norm": 0.0398312471807003,
      "learning_rate": 0.0004412868632707775,
      "loss": 0.0781,
      "step": 1042
    },
    {
      "epoch": 2.7962466487935655,
      "grad_norm": 0.03364887461066246,
      "learning_rate": 0.0004407506702412869,
      "loss": 0.0938,
      "step": 1043
    },
    {
      "epoch": 2.798927613941019,
      "grad_norm": 0.058709923177957535,
      "learning_rate": 0.00044021447721179625,
      "loss": 0.0889,
      "step": 1044
    },
    {
      "epoch": 2.8016085790884717,
      "grad_norm": 0.04049253463745117,
      "learning_rate": 0.00043967828418230566,
      "loss": 0.0889,
      "step": 1045
    },
    {
      "epoch": 2.804289544235925,
      "grad_norm": 0.03731603920459747,
      "learning_rate": 0.000439142091152815,
      "loss": 0.0786,
      "step": 1046
    },
    {
      "epoch": 2.806970509383378,
      "grad_norm": 0.03580477461218834,
      "learning_rate": 0.0004386058981233244,
      "loss": 0.0879,
      "step": 1047
    },
    {
      "epoch": 2.809651474530831,
      "grad_norm": 0.039729807525873184,
      "learning_rate": 0.0004380697050938338,
      "loss": 0.104,
      "step": 1048
    },
    {
      "epoch": 2.812332439678284,
      "grad_norm": 0.02945406176149845,
      "learning_rate": 0.00043753351206434317,
      "loss": 0.0762,
      "step": 1049
    },
    {
      "epoch": 2.8150134048257374,
      "grad_norm": 0.03572060540318489,
      "learning_rate": 0.0004369973190348526,
      "loss": 0.1064,
      "step": 1050
    },
    {
      "epoch": 2.8176943699731902,
      "grad_norm": 0.029210586100816727,
      "learning_rate": 0.00043646112600536193,
      "loss": 0.0659,
      "step": 1051
    },
    {
      "epoch": 2.8203753351206435,
      "grad_norm": 0.03101716935634613,
      "learning_rate": 0.00043592493297587133,
      "loss": 0.0579,
      "step": 1052
    },
    {
      "epoch": 2.8230563002680964,
      "grad_norm": 0.03780188038945198,
      "learning_rate": 0.00043538873994638074,
      "loss": 0.1045,
      "step": 1053
    },
    {
      "epoch": 2.8257372654155497,
      "grad_norm": 0.028615346178412437,
      "learning_rate": 0.0004348525469168901,
      "loss": 0.0679,
      "step": 1054
    },
    {
      "epoch": 2.8284182305630026,
      "grad_norm": 0.08167009055614471,
      "learning_rate": 0.0004343163538873995,
      "loss": 0.0879,
      "step": 1055
    },
    {
      "epoch": 2.831099195710456,
      "grad_norm": 0.04827873408794403,
      "learning_rate": 0.00043378016085790885,
      "loss": 0.0771,
      "step": 1056
    },
    {
      "epoch": 2.8337801608579087,
      "grad_norm": 0.04596373438835144,
      "learning_rate": 0.00043324396782841825,
      "loss": 0.0747,
      "step": 1057
    },
    {
      "epoch": 2.836461126005362,
      "grad_norm": 0.0463244654238224,
      "learning_rate": 0.00043270777479892766,
      "loss": 0.0767,
      "step": 1058
    },
    {
      "epoch": 2.839142091152815,
      "grad_norm": 0.05879902094602585,
      "learning_rate": 0.000432171581769437,
      "loss": 0.0781,
      "step": 1059
    },
    {
      "epoch": 2.841823056300268,
      "grad_norm": 0.04210151359438896,
      "learning_rate": 0.0004316353887399464,
      "loss": 0.0703,
      "step": 1060
    },
    {
      "epoch": 2.844504021447721,
      "grad_norm": 0.06502574682235718,
      "learning_rate": 0.00043109919571045576,
      "loss": 0.1001,
      "step": 1061
    },
    {
      "epoch": 2.8471849865951744,
      "grad_norm": 0.04148052632808685,
      "learning_rate": 0.00043056300268096517,
      "loss": 0.0874,
      "step": 1062
    },
    {
      "epoch": 2.8498659517426272,
      "grad_norm": 0.02800106629729271,
      "learning_rate": 0.0004300268096514746,
      "loss": 0.0776,
      "step": 1063
    },
    {
      "epoch": 2.8525469168900806,
      "grad_norm": 0.037974923849105835,
      "learning_rate": 0.0004294906166219839,
      "loss": 0.0942,
      "step": 1064
    },
    {
      "epoch": 2.8552278820375334,
      "grad_norm": 0.028500718995928764,
      "learning_rate": 0.00042895442359249333,
      "loss": 0.0781,
      "step": 1065
    },
    {
      "epoch": 2.8579088471849867,
      "grad_norm": 0.03200831264257431,
      "learning_rate": 0.0004284182305630027,
      "loss": 0.082,
      "step": 1066
    },
    {
      "epoch": 2.8605898123324396,
      "grad_norm": 0.05240514501929283,
      "learning_rate": 0.0004278820375335121,
      "loss": 0.0791,
      "step": 1067
    },
    {
      "epoch": 2.863270777479893,
      "grad_norm": 0.03158421441912651,
      "learning_rate": 0.0004273458445040215,
      "loss": 0.0776,
      "step": 1068
    },
    {
      "epoch": 2.8659517426273458,
      "grad_norm": 0.027602870017290115,
      "learning_rate": 0.00042680965147453084,
      "loss": 0.0786,
      "step": 1069
    },
    {
      "epoch": 2.868632707774799,
      "grad_norm": 0.038074661046266556,
      "learning_rate": 0.00042627345844504025,
      "loss": 0.0869,
      "step": 1070
    },
    {
      "epoch": 2.871313672922252,
      "grad_norm": 0.029944658279418945,
      "learning_rate": 0.0004257372654155496,
      "loss": 0.0747,
      "step": 1071
    },
    {
      "epoch": 2.8739946380697052,
      "grad_norm": 0.050761908292770386,
      "learning_rate": 0.000425201072386059,
      "loss": 0.0923,
      "step": 1072
    },
    {
      "epoch": 2.876675603217158,
      "grad_norm": 0.17589108645915985,
      "learning_rate": 0.0004246648793565684,
      "loss": 0.0757,
      "step": 1073
    },
    {
      "epoch": 2.8793565683646114,
      "grad_norm": 0.02960900217294693,
      "learning_rate": 0.00042412868632707776,
      "loss": 0.0752,
      "step": 1074
    },
    {
      "epoch": 2.8820375335120643,
      "grad_norm": 0.04145786166191101,
      "learning_rate": 0.00042359249329758717,
      "loss": 0.0742,
      "step": 1075
    },
    {
      "epoch": 2.8847184986595176,
      "grad_norm": 0.026367345824837685,
      "learning_rate": 0.00042305630026809657,
      "loss": 0.0718,
      "step": 1076
    },
    {
      "epoch": 2.8873994638069704,
      "grad_norm": 0.034391775727272034,
      "learning_rate": 0.0004225201072386059,
      "loss": 0.0718,
      "step": 1077
    },
    {
      "epoch": 2.8900804289544237,
      "grad_norm": 0.03386280685663223,
      "learning_rate": 0.0004219839142091153,
      "loss": 0.0752,
      "step": 1078
    },
    {
      "epoch": 2.8927613941018766,
      "grad_norm": 0.042457643896341324,
      "learning_rate": 0.0004214477211796247,
      "loss": 0.0776,
      "step": 1079
    },
    {
      "epoch": 2.89544235924933,
      "grad_norm": 0.047997619956731796,
      "learning_rate": 0.0004209115281501341,
      "loss": 0.0845,
      "step": 1080
    },
    {
      "epoch": 2.8981233243967828,
      "grad_norm": 0.06609110534191132,
      "learning_rate": 0.0004203753351206435,
      "loss": 0.0879,
      "step": 1081
    },
    {
      "epoch": 2.900804289544236,
      "grad_norm": 0.027180559933185577,
      "learning_rate": 0.00041983914209115284,
      "loss": 0.0786,
      "step": 1082
    },
    {
      "epoch": 2.903485254691689,
      "grad_norm": 0.03159182146191597,
      "learning_rate": 0.00041930294906166224,
      "loss": 0.0947,
      "step": 1083
    },
    {
      "epoch": 2.9061662198391423,
      "grad_norm": 0.050074514001607895,
      "learning_rate": 0.0004187667560321716,
      "loss": 0.0967,
      "step": 1084
    },
    {
      "epoch": 2.908847184986595,
      "grad_norm": 0.028459057211875916,
      "learning_rate": 0.000418230563002681,
      "loss": 0.0674,
      "step": 1085
    },
    {
      "epoch": 2.9115281501340484,
      "grad_norm": 0.07017023116350174,
      "learning_rate": 0.0004176943699731904,
      "loss": 0.0884,
      "step": 1086
    },
    {
      "epoch": 2.9142091152815013,
      "grad_norm": 0.052338212728500366,
      "learning_rate": 0.00041715817694369976,
      "loss": 0.0879,
      "step": 1087
    },
    {
      "epoch": 2.9168900804289546,
      "grad_norm": 0.05541515722870827,
      "learning_rate": 0.00041662198391420916,
      "loss": 0.0913,
      "step": 1088
    },
    {
      "epoch": 2.9195710455764075,
      "grad_norm": 0.040169961750507355,
      "learning_rate": 0.00041608579088471846,
      "loss": 0.1104,
      "step": 1089
    },
    {
      "epoch": 2.9222520107238603,
      "grad_norm": 0.09185714274644852,
      "learning_rate": 0.00041554959785522786,
      "loss": 0.0913,
      "step": 1090
    },
    {
      "epoch": 2.9249329758713136,
      "grad_norm": 0.03457530960440636,
      "learning_rate": 0.00041501340482573727,
      "loss": 0.0732,
      "step": 1091
    },
    {
      "epoch": 2.927613941018767,
      "grad_norm": 0.06300072371959686,
      "learning_rate": 0.0004144772117962466,
      "loss": 0.0811,
      "step": 1092
    },
    {
      "epoch": 2.93029490616622,
      "grad_norm": 0.0364382304251194,
      "learning_rate": 0.000413941018766756,
      "loss": 0.0835,
      "step": 1093
    },
    {
      "epoch": 2.9329758713136727,
      "grad_norm": 0.044789452105760574,
      "learning_rate": 0.0004134048257372654,
      "loss": 0.0928,
      "step": 1094
    },
    {
      "epoch": 2.935656836461126,
      "grad_norm": 0.03221514821052551,
      "learning_rate": 0.0004128686327077748,
      "loss": 0.0718,
      "step": 1095
    },
    {
      "epoch": 2.9383378016085793,
      "grad_norm": 0.029771551489830017,
      "learning_rate": 0.0004123324396782842,
      "loss": 0.0723,
      "step": 1096
    },
    {
      "epoch": 2.941018766756032,
      "grad_norm": 0.0368044376373291,
      "learning_rate": 0.00041179624664879354,
      "loss": 0.0825,
      "step": 1097
    },
    {
      "epoch": 2.943699731903485,
      "grad_norm": 0.030489442870020866,
      "learning_rate": 0.00041126005361930294,
      "loss": 0.0923,
      "step": 1098
    },
    {
      "epoch": 2.9463806970509383,
      "grad_norm": 0.024206336587667465,
      "learning_rate": 0.0004107238605898123,
      "loss": 0.0723,
      "step": 1099
    },
    {
      "epoch": 2.9490616621983916,
      "grad_norm": 0.03732249140739441,
      "learning_rate": 0.0004101876675603217,
      "loss": 0.0791,
      "step": 1100
    },
    {
      "epoch": 2.9517426273458445,
      "grad_norm": 0.05361761152744293,
      "learning_rate": 0.0004096514745308311,
      "loss": 0.0674,
      "step": 1101
    },
    {
      "epoch": 2.9544235924932973,
      "grad_norm": 0.10293260216712952,
      "learning_rate": 0.00040911528150134046,
      "loss": 0.0698,
      "step": 1102
    },
    {
      "epoch": 2.9571045576407506,
      "grad_norm": 0.03260527178645134,
      "learning_rate": 0.00040857908847184986,
      "loss": 0.083,
      "step": 1103
    },
    {
      "epoch": 2.959785522788204,
      "grad_norm": 0.029955795034766197,
      "learning_rate": 0.00040804289544235927,
      "loss": 0.0835,
      "step": 1104
    },
    {
      "epoch": 2.962466487935657,
      "grad_norm": 0.03247825801372528,
      "learning_rate": 0.0004075067024128686,
      "loss": 0.0623,
      "step": 1105
    },
    {
      "epoch": 2.9651474530831097,
      "grad_norm": 0.03552715480327606,
      "learning_rate": 0.000406970509383378,
      "loss": 0.0947,
      "step": 1106
    },
    {
      "epoch": 2.967828418230563,
      "grad_norm": 0.0394044928252697,
      "learning_rate": 0.0004064343163538874,
      "loss": 0.0669,
      "step": 1107
    },
    {
      "epoch": 2.9705093833780163,
      "grad_norm": 0.03406401723623276,
      "learning_rate": 0.0004058981233243968,
      "loss": 0.0889,
      "step": 1108
    },
    {
      "epoch": 2.973190348525469,
      "grad_norm": 0.026281308382749557,
      "learning_rate": 0.0004053619302949062,
      "loss": 0.0703,
      "step": 1109
    },
    {
      "epoch": 2.975871313672922,
      "grad_norm": 0.037719063460826874,
      "learning_rate": 0.00040482573726541553,
      "loss": 0.0796,
      "step": 1110
    },
    {
      "epoch": 2.9785522788203753,
      "grad_norm": 0.031569186598062515,
      "learning_rate": 0.00040428954423592494,
      "loss": 0.0894,
      "step": 1111
    },
    {
      "epoch": 2.9812332439678286,
      "grad_norm": 0.03349839150905609,
      "learning_rate": 0.0004037533512064343,
      "loss": 0.0747,
      "step": 1112
    },
    {
      "epoch": 2.9839142091152815,
      "grad_norm": 0.03181237727403641,
      "learning_rate": 0.0004032171581769437,
      "loss": 0.0913,
      "step": 1113
    },
    {
      "epoch": 2.9865951742627344,
      "grad_norm": 0.056648217141628265,
      "learning_rate": 0.0004026809651474531,
      "loss": 0.0605,
      "step": 1114
    },
    {
      "epoch": 2.9892761394101877,
      "grad_norm": 0.07495208829641342,
      "learning_rate": 0.00040214477211796245,
      "loss": 0.0918,
      "step": 1115
    },
    {
      "epoch": 2.991957104557641,
      "grad_norm": 0.0336093045771122,
      "learning_rate": 0.00040160857908847186,
      "loss": 0.0728,
      "step": 1116
    },
    {
      "epoch": 2.994638069705094,
      "grad_norm": 0.03369176387786865,
      "learning_rate": 0.0004010723860589812,
      "loss": 0.0986,
      "step": 1117
    },
    {
      "epoch": 2.9973190348525467,
      "grad_norm": 0.03685678914189339,
      "learning_rate": 0.0004005361930294906,
      "loss": 0.0854,
      "step": 1118
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.03245530650019646,
      "learning_rate": 0.0004,
      "loss": 0.0547,
      "step": 1119
    },
    {
      "epoch": 3.002680965147453,
      "grad_norm": 0.03277844563126564,
      "learning_rate": 0.00039946380697050937,
      "loss": 0.0815,
      "step": 1120
    },
    {
      "epoch": 3.005361930294906,
      "grad_norm": 0.06806467473506927,
      "learning_rate": 0.0003989276139410188,
      "loss": 0.0874,
      "step": 1121
    },
    {
      "epoch": 3.008042895442359,
      "grad_norm": 0.041492853313684464,
      "learning_rate": 0.0003983914209115281,
      "loss": 0.0781,
      "step": 1122
    },
    {
      "epoch": 3.0107238605898123,
      "grad_norm": 0.03076254390180111,
      "learning_rate": 0.00039785522788203753,
      "loss": 0.0796,
      "step": 1123
    },
    {
      "epoch": 3.013404825737265,
      "grad_norm": 0.034003082662820816,
      "learning_rate": 0.00039731903485254694,
      "loss": 0.0757,
      "step": 1124
    },
    {
      "epoch": 3.0160857908847185,
      "grad_norm": 0.041023366153240204,
      "learning_rate": 0.0003967828418230563,
      "loss": 0.0981,
      "step": 1125
    },
    {
      "epoch": 3.0187667560321714,
      "grad_norm": 0.03503429889678955,
      "learning_rate": 0.0003962466487935657,
      "loss": 0.0786,
      "step": 1126
    },
    {
      "epoch": 3.0214477211796247,
      "grad_norm": 0.06694110482931137,
      "learning_rate": 0.00039571045576407504,
      "loss": 0.0923,
      "step": 1127
    },
    {
      "epoch": 3.0241286863270775,
      "grad_norm": 0.051724910736083984,
      "learning_rate": 0.00039517426273458445,
      "loss": 0.0742,
      "step": 1128
    },
    {
      "epoch": 3.026809651474531,
      "grad_norm": 0.03006994165480137,
      "learning_rate": 0.00039463806970509385,
      "loss": 0.0757,
      "step": 1129
    },
    {
      "epoch": 3.0294906166219837,
      "grad_norm": 0.06167367473244667,
      "learning_rate": 0.0003941018766756032,
      "loss": 0.0757,
      "step": 1130
    },
    {
      "epoch": 3.032171581769437,
      "grad_norm": 0.03927365690469742,
      "learning_rate": 0.0003935656836461126,
      "loss": 0.0649,
      "step": 1131
    },
    {
      "epoch": 3.03485254691689,
      "grad_norm": 0.04477586969733238,
      "learning_rate": 0.00039302949061662196,
      "loss": 0.0996,
      "step": 1132
    },
    {
      "epoch": 3.037533512064343,
      "grad_norm": 0.03170441463589668,
      "learning_rate": 0.00039249329758713137,
      "loss": 0.0723,
      "step": 1133
    },
    {
      "epoch": 3.040214477211796,
      "grad_norm": 0.03327858820557594,
      "learning_rate": 0.00039195710455764077,
      "loss": 0.0776,
      "step": 1134
    },
    {
      "epoch": 3.0428954423592494,
      "grad_norm": 0.0373825877904892,
      "learning_rate": 0.0003914209115281501,
      "loss": 0.0845,
      "step": 1135
    },
    {
      "epoch": 3.045576407506702,
      "grad_norm": 0.030060725286602974,
      "learning_rate": 0.00039088471849865953,
      "loss": 0.0713,
      "step": 1136
    },
    {
      "epoch": 3.0482573726541555,
      "grad_norm": 0.05186782777309418,
      "learning_rate": 0.00039034852546916893,
      "loss": 0.0703,
      "step": 1137
    },
    {
      "epoch": 3.0509383378016084,
      "grad_norm": 0.03249288350343704,
      "learning_rate": 0.0003898123324396783,
      "loss": 0.0889,
      "step": 1138
    },
    {
      "epoch": 3.0536193029490617,
      "grad_norm": 0.05311422795057297,
      "learning_rate": 0.0003892761394101877,
      "loss": 0.0815,
      "step": 1139
    },
    {
      "epoch": 3.0563002680965146,
      "grad_norm": 0.031951431185007095,
      "learning_rate": 0.00038873994638069704,
      "loss": 0.0684,
      "step": 1140
    },
    {
      "epoch": 3.058981233243968,
      "grad_norm": 0.02679007686674595,
      "learning_rate": 0.00038820375335120645,
      "loss": 0.0679,
      "step": 1141
    },
    {
      "epoch": 3.0616621983914207,
      "grad_norm": 0.05651160702109337,
      "learning_rate": 0.00038766756032171585,
      "loss": 0.0869,
      "step": 1142
    },
    {
      "epoch": 3.064343163538874,
      "grad_norm": 0.02834707498550415,
      "learning_rate": 0.0003871313672922252,
      "loss": 0.0601,
      "step": 1143
    },
    {
      "epoch": 3.067024128686327,
      "grad_norm": 0.02937077172100544,
      "learning_rate": 0.0003865951742627346,
      "loss": 0.0649,
      "step": 1144
    },
    {
      "epoch": 3.06970509383378,
      "grad_norm": 0.028337009251117706,
      "learning_rate": 0.00038605898123324396,
      "loss": 0.0674,
      "step": 1145
    },
    {
      "epoch": 3.072386058981233,
      "grad_norm": 0.03767447546124458,
      "learning_rate": 0.00038552278820375336,
      "loss": 0.0654,
      "step": 1146
    },
    {
      "epoch": 3.0750670241286864,
      "grad_norm": 0.02610686607658863,
      "learning_rate": 0.00038498659517426277,
      "loss": 0.064,
      "step": 1147
    },
    {
      "epoch": 3.0777479892761392,
      "grad_norm": 0.03487027809023857,
      "learning_rate": 0.0003844504021447721,
      "loss": 0.0991,
      "step": 1148
    },
    {
      "epoch": 3.0804289544235925,
      "grad_norm": 0.042496051639318466,
      "learning_rate": 0.0003839142091152815,
      "loss": 0.0908,
      "step": 1149
    },
    {
      "epoch": 3.0831099195710454,
      "grad_norm": 0.040199145674705505,
      "learning_rate": 0.0003833780160857909,
      "loss": 0.0781,
      "step": 1150
    },
    {
      "epoch": 3.0857908847184987,
      "grad_norm": 0.05231470614671707,
      "learning_rate": 0.0003828418230563003,
      "loss": 0.0732,
      "step": 1151
    },
    {
      "epoch": 3.0884718498659516,
      "grad_norm": 0.04808224365115166,
      "learning_rate": 0.0003823056300268097,
      "loss": 0.0869,
      "step": 1152
    },
    {
      "epoch": 3.091152815013405,
      "grad_norm": 0.03946748375892639,
      "learning_rate": 0.00038176943699731904,
      "loss": 0.0718,
      "step": 1153
    },
    {
      "epoch": 3.0938337801608577,
      "grad_norm": 0.04962804168462753,
      "learning_rate": 0.00038123324396782844,
      "loss": 0.082,
      "step": 1154
    },
    {
      "epoch": 3.096514745308311,
      "grad_norm": 0.03630224987864494,
      "learning_rate": 0.0003806970509383378,
      "loss": 0.0864,
      "step": 1155
    },
    {
      "epoch": 3.099195710455764,
      "grad_norm": 0.04379481077194214,
      "learning_rate": 0.0003801608579088472,
      "loss": 0.1064,
      "step": 1156
    },
    {
      "epoch": 3.1018766756032172,
      "grad_norm": 0.05480719730257988,
      "learning_rate": 0.0003796246648793566,
      "loss": 0.0981,
      "step": 1157
    },
    {
      "epoch": 3.10455764075067,
      "grad_norm": 0.03582946956157684,
      "learning_rate": 0.00037908847184986596,
      "loss": 0.0679,
      "step": 1158
    },
    {
      "epoch": 3.1072386058981234,
      "grad_norm": 0.036040231585502625,
      "learning_rate": 0.00037855227882037536,
      "loss": 0.0889,
      "step": 1159
    },
    {
      "epoch": 3.1099195710455763,
      "grad_norm": 0.05075172707438469,
      "learning_rate": 0.0003780160857908847,
      "loss": 0.0854,
      "step": 1160
    },
    {
      "epoch": 3.1126005361930296,
      "grad_norm": 0.03874471038579941,
      "learning_rate": 0.0003774798927613941,
      "loss": 0.0898,
      "step": 1161
    },
    {
      "epoch": 3.1152815013404824,
      "grad_norm": 0.03551827743649483,
      "learning_rate": 0.0003769436997319035,
      "loss": 0.0967,
      "step": 1162
    },
    {
      "epoch": 3.1179624664879357,
      "grad_norm": 0.05120358243584633,
      "learning_rate": 0.0003764075067024129,
      "loss": 0.0757,
      "step": 1163
    },
    {
      "epoch": 3.1206434316353886,
      "grad_norm": 0.031119104474782944,
      "learning_rate": 0.0003758713136729223,
      "loss": 0.0649,
      "step": 1164
    },
    {
      "epoch": 3.123324396782842,
      "grad_norm": 0.03743491321802139,
      "learning_rate": 0.00037533512064343163,
      "loss": 0.0569,
      "step": 1165
    },
    {
      "epoch": 3.1260053619302948,
      "grad_norm": 0.03263011574745178,
      "learning_rate": 0.00037479892761394103,
      "loss": 0.0957,
      "step": 1166
    },
    {
      "epoch": 3.128686327077748,
      "grad_norm": 0.03201788291335106,
      "learning_rate": 0.00037426273458445044,
      "loss": 0.0752,
      "step": 1167
    },
    {
      "epoch": 3.131367292225201,
      "grad_norm": 0.05748312920331955,
      "learning_rate": 0.0003737265415549598,
      "loss": 0.0933,
      "step": 1168
    },
    {
      "epoch": 3.1340482573726542,
      "grad_norm": 0.03199242055416107,
      "learning_rate": 0.0003731903485254692,
      "loss": 0.0659,
      "step": 1169
    },
    {
      "epoch": 3.136729222520107,
      "grad_norm": 0.034793272614479065,
      "learning_rate": 0.0003726541554959786,
      "loss": 0.0903,
      "step": 1170
    },
    {
      "epoch": 3.1394101876675604,
      "grad_norm": 0.031522106379270554,
      "learning_rate": 0.00037211796246648795,
      "loss": 0.0645,
      "step": 1171
    },
    {
      "epoch": 3.1420911528150133,
      "grad_norm": 0.030599724501371384,
      "learning_rate": 0.00037158176943699736,
      "loss": 0.0781,
      "step": 1172
    },
    {
      "epoch": 3.1447721179624666,
      "grad_norm": 0.05822475999593735,
      "learning_rate": 0.0003710455764075067,
      "loss": 0.0713,
      "step": 1173
    },
    {
      "epoch": 3.1474530831099194,
      "grad_norm": 0.04841301590204239,
      "learning_rate": 0.0003705093833780161,
      "loss": 0.0815,
      "step": 1174
    },
    {
      "epoch": 3.1501340482573728,
      "grad_norm": 0.02836763672530651,
      "learning_rate": 0.0003699731903485255,
      "loss": 0.0767,
      "step": 1175
    },
    {
      "epoch": 3.1528150134048256,
      "grad_norm": 0.05070895701646805,
      "learning_rate": 0.00036943699731903487,
      "loss": 0.0952,
      "step": 1176
    },
    {
      "epoch": 3.155495978552279,
      "grad_norm": 0.03582862392067909,
      "learning_rate": 0.0003689008042895443,
      "loss": 0.0835,
      "step": 1177
    },
    {
      "epoch": 3.158176943699732,
      "grad_norm": 0.036862485110759735,
      "learning_rate": 0.0003683646112600536,
      "loss": 0.0703,
      "step": 1178
    },
    {
      "epoch": 3.160857908847185,
      "grad_norm": 0.02986104041337967,
      "learning_rate": 0.00036782841823056303,
      "loss": 0.0815,
      "step": 1179
    },
    {
      "epoch": 3.163538873994638,
      "grad_norm": 0.04187887907028198,
      "learning_rate": 0.00036729222520107244,
      "loss": 0.0762,
      "step": 1180
    },
    {
      "epoch": 3.1662198391420913,
      "grad_norm": 0.03460077941417694,
      "learning_rate": 0.0003667560321715818,
      "loss": 0.0737,
      "step": 1181
    },
    {
      "epoch": 3.168900804289544,
      "grad_norm": 0.036015983670949936,
      "learning_rate": 0.0003662198391420912,
      "loss": 0.0845,
      "step": 1182
    },
    {
      "epoch": 3.1715817694369974,
      "grad_norm": 0.024904776364564896,
      "learning_rate": 0.00036568364611260054,
      "loss": 0.0713,
      "step": 1183
    },
    {
      "epoch": 3.1742627345844503,
      "grad_norm": 0.027834787964820862,
      "learning_rate": 0.00036514745308310995,
      "loss": 0.0693,
      "step": 1184
    },
    {
      "epoch": 3.1769436997319036,
      "grad_norm": 0.03998340666294098,
      "learning_rate": 0.00036461126005361935,
      "loss": 0.0815,
      "step": 1185
    },
    {
      "epoch": 3.1796246648793565,
      "grad_norm": 0.026167551055550575,
      "learning_rate": 0.0003640750670241287,
      "loss": 0.0703,
      "step": 1186
    },
    {
      "epoch": 3.1823056300268098,
      "grad_norm": 0.029190821573138237,
      "learning_rate": 0.0003635388739946381,
      "loss": 0.0669,
      "step": 1187
    },
    {
      "epoch": 3.1849865951742626,
      "grad_norm": 0.06588421761989594,
      "learning_rate": 0.00036300268096514746,
      "loss": 0.0903,
      "step": 1188
    },
    {
      "epoch": 3.187667560321716,
      "grad_norm": 0.03886842727661133,
      "learning_rate": 0.00036246648793565687,
      "loss": 0.0693,
      "step": 1189
    },
    {
      "epoch": 3.190348525469169,
      "grad_norm": 0.03674694523215294,
      "learning_rate": 0.00036193029490616627,
      "loss": 0.0625,
      "step": 1190
    },
    {
      "epoch": 3.193029490616622,
      "grad_norm": 0.02318837307393551,
      "learning_rate": 0.00036139410187667557,
      "loss": 0.0684,
      "step": 1191
    },
    {
      "epoch": 3.195710455764075,
      "grad_norm": 0.03618048503994942,
      "learning_rate": 0.000360857908847185,
      "loss": 0.0747,
      "step": 1192
    },
    {
      "epoch": 3.1983914209115283,
      "grad_norm": 0.04246396943926811,
      "learning_rate": 0.0003603217158176943,
      "loss": 0.0728,
      "step": 1193
    },
    {
      "epoch": 3.201072386058981,
      "grad_norm": 0.03609563037753105,
      "learning_rate": 0.00035978552278820373,
      "loss": 0.0771,
      "step": 1194
    },
    {
      "epoch": 3.2037533512064345,
      "grad_norm": 0.07818553596735,
      "learning_rate": 0.00035924932975871314,
      "loss": 0.0781,
      "step": 1195
    },
    {
      "epoch": 3.2064343163538873,
      "grad_norm": 0.05115446448326111,
      "learning_rate": 0.0003587131367292225,
      "loss": 0.0845,
      "step": 1196
    },
    {
      "epoch": 3.2091152815013406,
      "grad_norm": 0.03055175207555294,
      "learning_rate": 0.0003581769436997319,
      "loss": 0.0903,
      "step": 1197
    },
    {
      "epoch": 3.2117962466487935,
      "grad_norm": 0.03697918727993965,
      "learning_rate": 0.0003576407506702413,
      "loss": 0.0986,
      "step": 1198
    },
    {
      "epoch": 3.214477211796247,
      "grad_norm": 0.03510752320289612,
      "learning_rate": 0.00035710455764075065,
      "loss": 0.0776,
      "step": 1199
    },
    {
      "epoch": 3.2171581769436997,
      "grad_norm": 0.02908395230770111,
      "learning_rate": 0.00035656836461126005,
      "loss": 0.0659,
      "step": 1200
    },
    {
      "epoch": 3.219839142091153,
      "grad_norm": 0.032455626875162125,
      "learning_rate": 0.0003560321715817694,
      "loss": 0.0796,
      "step": 1201
    },
    {
      "epoch": 3.222520107238606,
      "grad_norm": 0.02606186456978321,
      "learning_rate": 0.0003554959785522788,
      "loss": 0.0623,
      "step": 1202
    },
    {
      "epoch": 3.225201072386059,
      "grad_norm": 0.053990159183740616,
      "learning_rate": 0.0003549597855227882,
      "loss": 0.0684,
      "step": 1203
    },
    {
      "epoch": 3.227882037533512,
      "grad_norm": 0.038250118494033813,
      "learning_rate": 0.00035442359249329757,
      "loss": 0.0923,
      "step": 1204
    },
    {
      "epoch": 3.2305630026809653,
      "grad_norm": 0.033119093626737595,
      "learning_rate": 0.00035388739946380697,
      "loss": 0.0752,
      "step": 1205
    },
    {
      "epoch": 3.233243967828418,
      "grad_norm": 0.03740472346544266,
      "learning_rate": 0.0003533512064343163,
      "loss": 0.0864,
      "step": 1206
    },
    {
      "epoch": 3.2359249329758715,
      "grad_norm": 0.030398106202483177,
      "learning_rate": 0.00035281501340482573,
      "loss": 0.0737,
      "step": 1207
    },
    {
      "epoch": 3.2386058981233243,
      "grad_norm": 0.029684768989682198,
      "learning_rate": 0.00035227882037533513,
      "loss": 0.0684,
      "step": 1208
    },
    {
      "epoch": 3.2412868632707776,
      "grad_norm": 0.03212269768118858,
      "learning_rate": 0.0003517426273458445,
      "loss": 0.0747,
      "step": 1209
    },
    {
      "epoch": 3.2439678284182305,
      "grad_norm": 0.028080709278583527,
      "learning_rate": 0.0003512064343163539,
      "loss": 0.0649,
      "step": 1210
    },
    {
      "epoch": 3.246648793565684,
      "grad_norm": 0.026197899132966995,
      "learning_rate": 0.00035067024128686324,
      "loss": 0.0708,
      "step": 1211
    },
    {
      "epoch": 3.2493297587131367,
      "grad_norm": 0.02543788030743599,
      "learning_rate": 0.00035013404825737265,
      "loss": 0.0698,
      "step": 1212
    },
    {
      "epoch": 3.25201072386059,
      "grad_norm": 0.04757139831781387,
      "learning_rate": 0.00034959785522788205,
      "loss": 0.0894,
      "step": 1213
    },
    {
      "epoch": 3.254691689008043,
      "grad_norm": 0.08300507068634033,
      "learning_rate": 0.0003490616621983914,
      "loss": 0.0942,
      "step": 1214
    },
    {
      "epoch": 3.257372654155496,
      "grad_norm": 0.02985581010580063,
      "learning_rate": 0.0003485254691689008,
      "loss": 0.0664,
      "step": 1215
    },
    {
      "epoch": 3.260053619302949,
      "grad_norm": 0.026353834196925163,
      "learning_rate": 0.00034798927613941016,
      "loss": 0.0688,
      "step": 1216
    },
    {
      "epoch": 3.2627345844504023,
      "grad_norm": 0.035963356494903564,
      "learning_rate": 0.00034745308310991956,
      "loss": 0.0874,
      "step": 1217
    },
    {
      "epoch": 3.265415549597855,
      "grad_norm": 0.08141160756349564,
      "learning_rate": 0.00034691689008042897,
      "loss": 0.0645,
      "step": 1218
    },
    {
      "epoch": 3.2680965147453085,
      "grad_norm": 0.03532231226563454,
      "learning_rate": 0.0003463806970509383,
      "loss": 0.0986,
      "step": 1219
    },
    {
      "epoch": 3.2707774798927614,
      "grad_norm": 0.04750114679336548,
      "learning_rate": 0.0003458445040214477,
      "loss": 0.0869,
      "step": 1220
    },
    {
      "epoch": 3.2734584450402147,
      "grad_norm": 0.035722360014915466,
      "learning_rate": 0.0003453083109919571,
      "loss": 0.0757,
      "step": 1221
    },
    {
      "epoch": 3.2761394101876675,
      "grad_norm": 0.06193907558917999,
      "learning_rate": 0.0003447721179624665,
      "loss": 0.0986,
      "step": 1222
    },
    {
      "epoch": 3.278820375335121,
      "grad_norm": 0.029415367171168327,
      "learning_rate": 0.0003442359249329759,
      "loss": 0.0728,
      "step": 1223
    },
    {
      "epoch": 3.2815013404825737,
      "grad_norm": 0.0652938112616539,
      "learning_rate": 0.00034369973190348524,
      "loss": 0.0859,
      "step": 1224
    },
    {
      "epoch": 3.284182305630027,
      "grad_norm": 0.029697133228182793,
      "learning_rate": 0.00034316353887399464,
      "loss": 0.0781,
      "step": 1225
    },
    {
      "epoch": 3.28686327077748,
      "grad_norm": 0.04141112044453621,
      "learning_rate": 0.000342627345844504,
      "loss": 0.083,
      "step": 1226
    },
    {
      "epoch": 3.289544235924933,
      "grad_norm": 0.03615032881498337,
      "learning_rate": 0.0003420911528150134,
      "loss": 0.0854,
      "step": 1227
    },
    {
      "epoch": 3.292225201072386,
      "grad_norm": 0.041690852493047714,
      "learning_rate": 0.0003415549597855228,
      "loss": 0.083,
      "step": 1228
    },
    {
      "epoch": 3.294906166219839,
      "grad_norm": 0.036837466061115265,
      "learning_rate": 0.00034101876675603215,
      "loss": 0.0845,
      "step": 1229
    },
    {
      "epoch": 3.297587131367292,
      "grad_norm": 0.03561146929860115,
      "learning_rate": 0.00034048257372654156,
      "loss": 0.0776,
      "step": 1230
    },
    {
      "epoch": 3.3002680965147455,
      "grad_norm": 0.03173920139670372,
      "learning_rate": 0.00033994638069705096,
      "loss": 0.0698,
      "step": 1231
    },
    {
      "epoch": 3.3029490616621984,
      "grad_norm": 0.035302452743053436,
      "learning_rate": 0.0003394101876675603,
      "loss": 0.0645,
      "step": 1232
    },
    {
      "epoch": 3.3056300268096512,
      "grad_norm": 0.02961171418428421,
      "learning_rate": 0.0003388739946380697,
      "loss": 0.0767,
      "step": 1233
    },
    {
      "epoch": 3.3083109919571045,
      "grad_norm": 0.04417235031723976,
      "learning_rate": 0.00033833780160857907,
      "loss": 0.0845,
      "step": 1234
    },
    {
      "epoch": 3.310991957104558,
      "grad_norm": 0.03309549391269684,
      "learning_rate": 0.0003378016085790885,
      "loss": 0.0952,
      "step": 1235
    },
    {
      "epoch": 3.3136729222520107,
      "grad_norm": 0.03206102177500725,
      "learning_rate": 0.0003372654155495979,
      "loss": 0.0977,
      "step": 1236
    },
    {
      "epoch": 3.3163538873994636,
      "grad_norm": 0.0547473318874836,
      "learning_rate": 0.00033672922252010723,
      "loss": 0.0674,
      "step": 1237
    },
    {
      "epoch": 3.319034852546917,
      "grad_norm": 0.05866596847772598,
      "learning_rate": 0.00033619302949061664,
      "loss": 0.0698,
      "step": 1238
    },
    {
      "epoch": 3.32171581769437,
      "grad_norm": 0.03863851726055145,
      "learning_rate": 0.000335656836461126,
      "loss": 0.0757,
      "step": 1239
    },
    {
      "epoch": 3.324396782841823,
      "grad_norm": 0.025782180950045586,
      "learning_rate": 0.0003351206434316354,
      "loss": 0.0713,
      "step": 1240
    },
    {
      "epoch": 3.327077747989276,
      "grad_norm": 0.025920294225215912,
      "learning_rate": 0.0003345844504021448,
      "loss": 0.0786,
      "step": 1241
    },
    {
      "epoch": 3.329758713136729,
      "grad_norm": 0.03911149874329567,
      "learning_rate": 0.00033404825737265415,
      "loss": 0.082,
      "step": 1242
    },
    {
      "epoch": 3.3324396782841825,
      "grad_norm": 0.03754686936736107,
      "learning_rate": 0.00033351206434316356,
      "loss": 0.0645,
      "step": 1243
    },
    {
      "epoch": 3.3351206434316354,
      "grad_norm": 0.03719719871878624,
      "learning_rate": 0.0003329758713136729,
      "loss": 0.0859,
      "step": 1244
    },
    {
      "epoch": 3.3378016085790883,
      "grad_norm": 0.03324700891971588,
      "learning_rate": 0.0003324396782841823,
      "loss": 0.0859,
      "step": 1245
    },
    {
      "epoch": 3.3404825737265416,
      "grad_norm": 0.04692884162068367,
      "learning_rate": 0.0003319034852546917,
      "loss": 0.0674,
      "step": 1246
    },
    {
      "epoch": 3.343163538873995,
      "grad_norm": 0.02390638180077076,
      "learning_rate": 0.00033136729222520107,
      "loss": 0.0625,
      "step": 1247
    },
    {
      "epoch": 3.3458445040214477,
      "grad_norm": 0.03016151860356331,
      "learning_rate": 0.0003308310991957105,
      "loss": 0.0894,
      "step": 1248
    },
    {
      "epoch": 3.3485254691689006,
      "grad_norm": 0.033840104937553406,
      "learning_rate": 0.0003302949061662198,
      "loss": 0.0737,
      "step": 1249
    },
    {
      "epoch": 3.351206434316354,
      "grad_norm": 0.029963430017232895,
      "learning_rate": 0.00032975871313672923,
      "loss": 0.0684,
      "step": 1250
    },
    {
      "epoch": 3.353887399463807,
      "grad_norm": 0.042990636080503464,
      "learning_rate": 0.00032922252010723864,
      "loss": 0.0742,
      "step": 1251
    },
    {
      "epoch": 3.35656836461126,
      "grad_norm": 0.02617058902978897,
      "learning_rate": 0.000328686327077748,
      "loss": 0.0708,
      "step": 1252
    },
    {
      "epoch": 3.359249329758713,
      "grad_norm": 0.029373887926340103,
      "learning_rate": 0.0003281501340482574,
      "loss": 0.0757,
      "step": 1253
    },
    {
      "epoch": 3.3619302949061662,
      "grad_norm": 0.02845337614417076,
      "learning_rate": 0.00032761394101876674,
      "loss": 0.0623,
      "step": 1254
    },
    {
      "epoch": 3.3646112600536195,
      "grad_norm": 0.028054095804691315,
      "learning_rate": 0.00032707774798927615,
      "loss": 0.0713,
      "step": 1255
    },
    {
      "epoch": 3.3672922252010724,
      "grad_norm": 0.021321898326277733,
      "learning_rate": 0.00032654155495978555,
      "loss": 0.0613,
      "step": 1256
    },
    {
      "epoch": 3.3699731903485253,
      "grad_norm": 0.04538533464074135,
      "learning_rate": 0.0003260053619302949,
      "loss": 0.0693,
      "step": 1257
    },
    {
      "epoch": 3.3726541554959786,
      "grad_norm": 0.0342772975564003,
      "learning_rate": 0.0003254691689008043,
      "loss": 0.1035,
      "step": 1258
    },
    {
      "epoch": 3.3753351206434314,
      "grad_norm": 0.03728620707988739,
      "learning_rate": 0.0003249329758713137,
      "loss": 0.0791,
      "step": 1259
    },
    {
      "epoch": 3.3780160857908847,
      "grad_norm": 0.039319779723882675,
      "learning_rate": 0.00032439678284182307,
      "loss": 0.1147,
      "step": 1260
    },
    {
      "epoch": 3.3806970509383376,
      "grad_norm": 0.05012175068259239,
      "learning_rate": 0.00032386058981233247,
      "loss": 0.0547,
      "step": 1261
    },
    {
      "epoch": 3.383378016085791,
      "grad_norm": 0.07105555385351181,
      "learning_rate": 0.0003233243967828418,
      "loss": 0.084,
      "step": 1262
    },
    {
      "epoch": 3.386058981233244,
      "grad_norm": 0.03161103278398514,
      "learning_rate": 0.00032278820375335123,
      "loss": 0.0757,
      "step": 1263
    },
    {
      "epoch": 3.388739946380697,
      "grad_norm": 0.029055580496788025,
      "learning_rate": 0.00032225201072386063,
      "loss": 0.0767,
      "step": 1264
    },
    {
      "epoch": 3.39142091152815,
      "grad_norm": 0.026977606117725372,
      "learning_rate": 0.00032171581769437,
      "loss": 0.0703,
      "step": 1265
    },
    {
      "epoch": 3.3941018766756033,
      "grad_norm": 0.025444334372878075,
      "learning_rate": 0.0003211796246648794,
      "loss": 0.0654,
      "step": 1266
    },
    {
      "epoch": 3.396782841823056,
      "grad_norm": 0.028205029666423798,
      "learning_rate": 0.00032064343163538874,
      "loss": 0.0747,
      "step": 1267
    },
    {
      "epoch": 3.3994638069705094,
      "grad_norm": 0.053283076733350754,
      "learning_rate": 0.00032010723860589815,
      "loss": 0.0825,
      "step": 1268
    },
    {
      "epoch": 3.4021447721179623,
      "grad_norm": 0.029031915590167046,
      "learning_rate": 0.00031957104557640755,
      "loss": 0.0598,
      "step": 1269
    },
    {
      "epoch": 3.4048257372654156,
      "grad_norm": 0.03494282811880112,
      "learning_rate": 0.0003190348525469169,
      "loss": 0.0938,
      "step": 1270
    },
    {
      "epoch": 3.4075067024128685,
      "grad_norm": 0.08644217252731323,
      "learning_rate": 0.0003184986595174263,
      "loss": 0.0791,
      "step": 1271
    },
    {
      "epoch": 3.4101876675603218,
      "grad_norm": 0.04720237851142883,
      "learning_rate": 0.00031796246648793566,
      "loss": 0.0718,
      "step": 1272
    },
    {
      "epoch": 3.4128686327077746,
      "grad_norm": 0.17512841522693634,
      "learning_rate": 0.00031742627345844506,
      "loss": 0.0869,
      "step": 1273
    },
    {
      "epoch": 3.415549597855228,
      "grad_norm": 0.10443127900362015,
      "learning_rate": 0.00031689008042895447,
      "loss": 0.0693,
      "step": 1274
    },
    {
      "epoch": 3.418230563002681,
      "grad_norm": 0.05488937348127365,
      "learning_rate": 0.0003163538873994638,
      "loss": 0.0767,
      "step": 1275
    },
    {
      "epoch": 3.420911528150134,
      "grad_norm": 0.03484296053647995,
      "learning_rate": 0.0003158176943699732,
      "loss": 0.0869,
      "step": 1276
    },
    {
      "epoch": 3.423592493297587,
      "grad_norm": 0.04889358580112457,
      "learning_rate": 0.0003152815013404826,
      "loss": 0.0967,
      "step": 1277
    },
    {
      "epoch": 3.4262734584450403,
      "grad_norm": 0.04709077998995781,
      "learning_rate": 0.000314745308310992,
      "loss": 0.0825,
      "step": 1278
    },
    {
      "epoch": 3.428954423592493,
      "grad_norm": 0.034053437411785126,
      "learning_rate": 0.0003142091152815014,
      "loss": 0.0786,
      "step": 1279
    },
    {
      "epoch": 3.4316353887399464,
      "grad_norm": 0.03263230621814728,
      "learning_rate": 0.00031367292225201074,
      "loss": 0.0698,
      "step": 1280
    },
    {
      "epoch": 3.4343163538873993,
      "grad_norm": 0.038577768951654434,
      "learning_rate": 0.00031313672922252014,
      "loss": 0.0879,
      "step": 1281
    },
    {
      "epoch": 3.4369973190348526,
      "grad_norm": 0.04602682590484619,
      "learning_rate": 0.0003126005361930295,
      "loss": 0.0732,
      "step": 1282
    },
    {
      "epoch": 3.4396782841823055,
      "grad_norm": 0.03250761702656746,
      "learning_rate": 0.0003120643431635389,
      "loss": 0.085,
      "step": 1283
    },
    {
      "epoch": 3.442359249329759,
      "grad_norm": 0.04057078808546066,
      "learning_rate": 0.0003115281501340483,
      "loss": 0.0796,
      "step": 1284
    },
    {
      "epoch": 3.4450402144772116,
      "grad_norm": 0.02891436591744423,
      "learning_rate": 0.00031099195710455765,
      "loss": 0.0654,
      "step": 1285
    },
    {
      "epoch": 3.447721179624665,
      "grad_norm": 0.030232498422265053,
      "learning_rate": 0.00031045576407506706,
      "loss": 0.0732,
      "step": 1286
    },
    {
      "epoch": 3.450402144772118,
      "grad_norm": 0.038392286747694016,
      "learning_rate": 0.0003099195710455764,
      "loss": 0.0938,
      "step": 1287
    },
    {
      "epoch": 3.453083109919571,
      "grad_norm": 0.028691567480564117,
      "learning_rate": 0.0003093833780160858,
      "loss": 0.0796,
      "step": 1288
    },
    {
      "epoch": 3.455764075067024,
      "grad_norm": 0.03625384718179703,
      "learning_rate": 0.0003088471849865952,
      "loss": 0.0806,
      "step": 1289
    },
    {
      "epoch": 3.4584450402144773,
      "grad_norm": 0.026191139593720436,
      "learning_rate": 0.00030831099195710457,
      "loss": 0.0674,
      "step": 1290
    },
    {
      "epoch": 3.46112600536193,
      "grad_norm": 0.031549014151096344,
      "learning_rate": 0.000307774798927614,
      "loss": 0.0635,
      "step": 1291
    },
    {
      "epoch": 3.4638069705093835,
      "grad_norm": 0.04131399467587471,
      "learning_rate": 0.0003072386058981234,
      "loss": 0.0713,
      "step": 1292
    },
    {
      "epoch": 3.4664879356568363,
      "grad_norm": 0.03805892914533615,
      "learning_rate": 0.00030670241286863273,
      "loss": 0.0825,
      "step": 1293
    },
    {
      "epoch": 3.4691689008042896,
      "grad_norm": 0.025211097672581673,
      "learning_rate": 0.00030616621983914214,
      "loss": 0.0708,
      "step": 1294
    },
    {
      "epoch": 3.4718498659517425,
      "grad_norm": 0.024309851229190826,
      "learning_rate": 0.00030563002680965144,
      "loss": 0.0688,
      "step": 1295
    },
    {
      "epoch": 3.474530831099196,
      "grad_norm": 0.05112079903483391,
      "learning_rate": 0.00030509383378016084,
      "loss": 0.0879,
      "step": 1296
    },
    {
      "epoch": 3.4772117962466487,
      "grad_norm": 0.033025965094566345,
      "learning_rate": 0.00030455764075067025,
      "loss": 0.0732,
      "step": 1297
    },
    {
      "epoch": 3.479892761394102,
      "grad_norm": 0.030088841915130615,
      "learning_rate": 0.0003040214477211796,
      "loss": 0.0713,
      "step": 1298
    },
    {
      "epoch": 3.482573726541555,
      "grad_norm": 0.05741027370095253,
      "learning_rate": 0.000303485254691689,
      "loss": 0.0645,
      "step": 1299
    },
    {
      "epoch": 3.485254691689008,
      "grad_norm": 0.028669338673353195,
      "learning_rate": 0.00030294906166219835,
      "loss": 0.0747,
      "step": 1300
    },
    {
      "epoch": 3.487935656836461,
      "grad_norm": 0.02672836370766163,
      "learning_rate": 0.00030241286863270776,
      "loss": 0.064,
      "step": 1301
    },
    {
      "epoch": 3.4906166219839143,
      "grad_norm": 0.029432857409119606,
      "learning_rate": 0.00030187667560321716,
      "loss": 0.0767,
      "step": 1302
    },
    {
      "epoch": 3.493297587131367,
      "grad_norm": 0.045758239924907684,
      "learning_rate": 0.0003013404825737265,
      "loss": 0.0649,
      "step": 1303
    },
    {
      "epoch": 3.4959785522788205,
      "grad_norm": 0.03553219884634018,
      "learning_rate": 0.0003008042895442359,
      "loss": 0.0806,
      "step": 1304
    },
    {
      "epoch": 3.4986595174262733,
      "grad_norm": 0.025657251477241516,
      "learning_rate": 0.00030026809651474527,
      "loss": 0.0659,
      "step": 1305
    },
    {
      "epoch": 3.5013404825737267,
      "grad_norm": 0.03380730003118515,
      "learning_rate": 0.0002997319034852547,
      "loss": 0.0625,
      "step": 1306
    },
    {
      "epoch": 3.5040214477211795,
      "grad_norm": 0.028105812147259712,
      "learning_rate": 0.0002991957104557641,
      "loss": 0.0776,
      "step": 1307
    },
    {
      "epoch": 3.506702412868633,
      "grad_norm": 0.032987676560878754,
      "learning_rate": 0.00029865951742627343,
      "loss": 0.0859,
      "step": 1308
    },
    {
      "epoch": 3.5093833780160857,
      "grad_norm": 0.05391217768192291,
      "learning_rate": 0.00029812332439678284,
      "loss": 0.0776,
      "step": 1309
    },
    {
      "epoch": 3.512064343163539,
      "grad_norm": 0.04665496572852135,
      "learning_rate": 0.0002975871313672922,
      "loss": 0.0796,
      "step": 1310
    },
    {
      "epoch": 3.514745308310992,
      "grad_norm": 0.03015374206006527,
      "learning_rate": 0.0002970509383378016,
      "loss": 0.0679,
      "step": 1311
    },
    {
      "epoch": 3.517426273458445,
      "grad_norm": 0.028533898293972015,
      "learning_rate": 0.000296514745308311,
      "loss": 0.0713,
      "step": 1312
    },
    {
      "epoch": 3.520107238605898,
      "grad_norm": 0.040630362927913666,
      "learning_rate": 0.00029597855227882035,
      "loss": 0.0889,
      "step": 1313
    },
    {
      "epoch": 3.5227882037533513,
      "grad_norm": 0.02584851160645485,
      "learning_rate": 0.00029544235924932976,
      "loss": 0.0688,
      "step": 1314
    },
    {
      "epoch": 3.525469168900804,
      "grad_norm": 0.0502389632165432,
      "learning_rate": 0.0002949061662198391,
      "loss": 0.0698,
      "step": 1315
    },
    {
      "epoch": 3.5281501340482575,
      "grad_norm": 0.03238983452320099,
      "learning_rate": 0.0002943699731903485,
      "loss": 0.0879,
      "step": 1316
    },
    {
      "epoch": 3.5308310991957104,
      "grad_norm": 0.04610384628176689,
      "learning_rate": 0.0002938337801608579,
      "loss": 0.083,
      "step": 1317
    },
    {
      "epoch": 3.5335120643431637,
      "grad_norm": 0.029464948922395706,
      "learning_rate": 0.00029329758713136727,
      "loss": 0.0728,
      "step": 1318
    },
    {
      "epoch": 3.5361930294906165,
      "grad_norm": 0.03304081782698631,
      "learning_rate": 0.0002927613941018767,
      "loss": 0.0747,
      "step": 1319
    },
    {
      "epoch": 3.53887399463807,
      "grad_norm": 0.025874337181448936,
      "learning_rate": 0.000292225201072386,
      "loss": 0.0664,
      "step": 1320
    },
    {
      "epoch": 3.5415549597855227,
      "grad_norm": 0.028072824701666832,
      "learning_rate": 0.00029168900804289543,
      "loss": 0.0767,
      "step": 1321
    },
    {
      "epoch": 3.544235924932976,
      "grad_norm": 0.029722319915890694,
      "learning_rate": 0.00029115281501340483,
      "loss": 0.0728,
      "step": 1322
    },
    {
      "epoch": 3.546916890080429,
      "grad_norm": 0.027455084025859833,
      "learning_rate": 0.0002906166219839142,
      "loss": 0.0732,
      "step": 1323
    },
    {
      "epoch": 3.549597855227882,
      "grad_norm": 0.0320967398583889,
      "learning_rate": 0.0002900804289544236,
      "loss": 0.0825,
      "step": 1324
    },
    {
      "epoch": 3.552278820375335,
      "grad_norm": 0.05319574475288391,
      "learning_rate": 0.000289544235924933,
      "loss": 0.0796,
      "step": 1325
    },
    {
      "epoch": 3.5549597855227884,
      "grad_norm": 0.039880137890577316,
      "learning_rate": 0.00028900804289544235,
      "loss": 0.0737,
      "step": 1326
    },
    {
      "epoch": 3.557640750670241,
      "grad_norm": 0.03388873487710953,
      "learning_rate": 0.00028847184986595175,
      "loss": 0.0952,
      "step": 1327
    },
    {
      "epoch": 3.5603217158176945,
      "grad_norm": 0.03622114285826683,
      "learning_rate": 0.0002879356568364611,
      "loss": 0.0825,
      "step": 1328
    },
    {
      "epoch": 3.5630026809651474,
      "grad_norm": 0.03615144267678261,
      "learning_rate": 0.0002873994638069705,
      "loss": 0.0684,
      "step": 1329
    },
    {
      "epoch": 3.5656836461126007,
      "grad_norm": 0.033781472593545914,
      "learning_rate": 0.0002868632707774799,
      "loss": 0.0933,
      "step": 1330
    },
    {
      "epoch": 3.5683646112600536,
      "grad_norm": 0.05969119817018509,
      "learning_rate": 0.00028632707774798926,
      "loss": 0.0825,
      "step": 1331
    },
    {
      "epoch": 3.571045576407507,
      "grad_norm": 0.036878202110528946,
      "learning_rate": 0.00028579088471849867,
      "loss": 0.064,
      "step": 1332
    },
    {
      "epoch": 3.5737265415549597,
      "grad_norm": 0.02871021255850792,
      "learning_rate": 0.000285254691689008,
      "loss": 0.0879,
      "step": 1333
    },
    {
      "epoch": 3.576407506702413,
      "grad_norm": 0.03327145799994469,
      "learning_rate": 0.0002847184986595174,
      "loss": 0.0732,
      "step": 1334
    },
    {
      "epoch": 3.579088471849866,
      "grad_norm": 0.0269267950206995,
      "learning_rate": 0.00028418230563002683,
      "loss": 0.0693,
      "step": 1335
    },
    {
      "epoch": 3.581769436997319,
      "grad_norm": 0.03655172139406204,
      "learning_rate": 0.0002836461126005362,
      "loss": 0.0747,
      "step": 1336
    },
    {
      "epoch": 3.584450402144772,
      "grad_norm": 0.029993301257491112,
      "learning_rate": 0.0002831099195710456,
      "loss": 0.0757,
      "step": 1337
    },
    {
      "epoch": 3.5871313672922254,
      "grad_norm": 0.03617629408836365,
      "learning_rate": 0.00028257372654155494,
      "loss": 0.0972,
      "step": 1338
    },
    {
      "epoch": 3.5898123324396782,
      "grad_norm": 0.03175419196486473,
      "learning_rate": 0.00028203753351206434,
      "loss": 0.0786,
      "step": 1339
    },
    {
      "epoch": 3.592493297587131,
      "grad_norm": 0.03464057296514511,
      "learning_rate": 0.00028150134048257375,
      "loss": 0.0732,
      "step": 1340
    },
    {
      "epoch": 3.5951742627345844,
      "grad_norm": 0.041157353669404984,
      "learning_rate": 0.0002809651474530831,
      "loss": 0.0781,
      "step": 1341
    },
    {
      "epoch": 3.5978552278820377,
      "grad_norm": 0.025731254369020462,
      "learning_rate": 0.0002804289544235925,
      "loss": 0.0781,
      "step": 1342
    },
    {
      "epoch": 3.6005361930294906,
      "grad_norm": 0.03445269912481308,
      "learning_rate": 0.00027989276139410186,
      "loss": 0.0801,
      "step": 1343
    },
    {
      "epoch": 3.6032171581769434,
      "grad_norm": 0.05613070726394653,
      "learning_rate": 0.00027935656836461126,
      "loss": 0.0732,
      "step": 1344
    },
    {
      "epoch": 3.6058981233243967,
      "grad_norm": 0.03786037489771843,
      "learning_rate": 0.00027882037533512067,
      "loss": 0.0747,
      "step": 1345
    },
    {
      "epoch": 3.60857908847185,
      "grad_norm": 0.03127570077776909,
      "learning_rate": 0.00027828418230563,
      "loss": 0.0757,
      "step": 1346
    },
    {
      "epoch": 3.611260053619303,
      "grad_norm": 0.027085814625024796,
      "learning_rate": 0.0002777479892761394,
      "loss": 0.0659,
      "step": 1347
    },
    {
      "epoch": 3.6139410187667558,
      "grad_norm": 0.03184673190116882,
      "learning_rate": 0.0002772117962466488,
      "loss": 0.0737,
      "step": 1348
    },
    {
      "epoch": 3.616621983914209,
      "grad_norm": 0.02787424623966217,
      "learning_rate": 0.0002766756032171582,
      "loss": 0.0623,
      "step": 1349
    },
    {
      "epoch": 3.6193029490616624,
      "grad_norm": 0.04375230148434639,
      "learning_rate": 0.0002761394101876676,
      "loss": 0.0757,
      "step": 1350
    },
    {
      "epoch": 3.6219839142091153,
      "grad_norm": 0.03027694672346115,
      "learning_rate": 0.00027560321715817694,
      "loss": 0.0544,
      "step": 1351
    },
    {
      "epoch": 3.624664879356568,
      "grad_norm": 0.04269568994641304,
      "learning_rate": 0.00027506702412868634,
      "loss": 0.083,
      "step": 1352
    },
    {
      "epoch": 3.6273458445040214,
      "grad_norm": 0.028388885781168938,
      "learning_rate": 0.00027453083109919575,
      "loss": 0.0674,
      "step": 1353
    },
    {
      "epoch": 3.6300268096514747,
      "grad_norm": 0.03852329030632973,
      "learning_rate": 0.0002739946380697051,
      "loss": 0.0859,
      "step": 1354
    },
    {
      "epoch": 3.6327077747989276,
      "grad_norm": 0.03670138865709305,
      "learning_rate": 0.0002734584450402145,
      "loss": 0.0718,
      "step": 1355
    },
    {
      "epoch": 3.6353887399463805,
      "grad_norm": 0.027223480865359306,
      "learning_rate": 0.00027292225201072385,
      "loss": 0.0601,
      "step": 1356
    },
    {
      "epoch": 3.6380697050938338,
      "grad_norm": 0.035214804112911224,
      "learning_rate": 0.00027238605898123326,
      "loss": 0.0806,
      "step": 1357
    },
    {
      "epoch": 3.640750670241287,
      "grad_norm": 0.03346500173211098,
      "learning_rate": 0.00027184986595174266,
      "loss": 0.0747,
      "step": 1358
    },
    {
      "epoch": 3.64343163538874,
      "grad_norm": 0.030663534998893738,
      "learning_rate": 0.000271313672922252,
      "loss": 0.063,
      "step": 1359
    },
    {
      "epoch": 3.646112600536193,
      "grad_norm": 0.030984990298748016,
      "learning_rate": 0.0002707774798927614,
      "loss": 0.0791,
      "step": 1360
    },
    {
      "epoch": 3.648793565683646,
      "grad_norm": 0.03463200852274895,
      "learning_rate": 0.00027024128686327077,
      "loss": 0.0747,
      "step": 1361
    },
    {
      "epoch": 3.6514745308310994,
      "grad_norm": 0.03247915953397751,
      "learning_rate": 0.0002697050938337802,
      "loss": 0.0713,
      "step": 1362
    },
    {
      "epoch": 3.6541554959785523,
      "grad_norm": 0.040813568979501724,
      "learning_rate": 0.0002691689008042896,
      "loss": 0.063,
      "step": 1363
    },
    {
      "epoch": 3.656836461126005,
      "grad_norm": 0.037628382444381714,
      "learning_rate": 0.00026863270777479893,
      "loss": 0.0762,
      "step": 1364
    },
    {
      "epoch": 3.6595174262734584,
      "grad_norm": 0.03197995573282242,
      "learning_rate": 0.00026809651474530834,
      "loss": 0.0889,
      "step": 1365
    },
    {
      "epoch": 3.6621983914209117,
      "grad_norm": 0.03789808973670006,
      "learning_rate": 0.0002675603217158177,
      "loss": 0.0752,
      "step": 1366
    },
    {
      "epoch": 3.6648793565683646,
      "grad_norm": 0.03462205454707146,
      "learning_rate": 0.0002670241286863271,
      "loss": 0.0776,
      "step": 1367
    },
    {
      "epoch": 3.6675603217158175,
      "grad_norm": 0.06897304207086563,
      "learning_rate": 0.0002664879356568365,
      "loss": 0.0977,
      "step": 1368
    },
    {
      "epoch": 3.670241286863271,
      "grad_norm": 0.05352567136287689,
      "learning_rate": 0.00026595174262734585,
      "loss": 0.0918,
      "step": 1369
    },
    {
      "epoch": 3.672922252010724,
      "grad_norm": 0.0332927405834198,
      "learning_rate": 0.00026541554959785526,
      "loss": 0.0791,
      "step": 1370
    },
    {
      "epoch": 3.675603217158177,
      "grad_norm": 0.050100117921829224,
      "learning_rate": 0.0002648793565683646,
      "loss": 0.085,
      "step": 1371
    },
    {
      "epoch": 3.67828418230563,
      "grad_norm": 0.035059135407209396,
      "learning_rate": 0.000264343163538874,
      "loss": 0.084,
      "step": 1372
    },
    {
      "epoch": 3.680965147453083,
      "grad_norm": 0.02889617718756199,
      "learning_rate": 0.0002638069705093834,
      "loss": 0.0762,
      "step": 1373
    },
    {
      "epoch": 3.6836461126005364,
      "grad_norm": 0.03887074068188667,
      "learning_rate": 0.00026327077747989277,
      "loss": 0.0806,
      "step": 1374
    },
    {
      "epoch": 3.6863270777479893,
      "grad_norm": 0.027488449588418007,
      "learning_rate": 0.0002627345844504022,
      "loss": 0.0562,
      "step": 1375
    },
    {
      "epoch": 3.689008042895442,
      "grad_norm": 0.06369186192750931,
      "learning_rate": 0.0002621983914209115,
      "loss": 0.0938,
      "step": 1376
    },
    {
      "epoch": 3.6916890080428955,
      "grad_norm": 0.03575332835316658,
      "learning_rate": 0.00026166219839142093,
      "loss": 0.0854,
      "step": 1377
    },
    {
      "epoch": 3.6943699731903488,
      "grad_norm": 0.04410622641444206,
      "learning_rate": 0.00026112600536193033,
      "loss": 0.0811,
      "step": 1378
    },
    {
      "epoch": 3.6970509383378016,
      "grad_norm": 0.07465777546167374,
      "learning_rate": 0.0002605898123324397,
      "loss": 0.1045,
      "step": 1379
    },
    {
      "epoch": 3.6997319034852545,
      "grad_norm": 0.046771418303251266,
      "learning_rate": 0.0002600536193029491,
      "loss": 0.0864,
      "step": 1380
    },
    {
      "epoch": 3.702412868632708,
      "grad_norm": 0.029207749292254448,
      "learning_rate": 0.00025951742627345844,
      "loss": 0.0596,
      "step": 1381
    },
    {
      "epoch": 3.705093833780161,
      "grad_norm": 0.02590559795498848,
      "learning_rate": 0.00025898123324396785,
      "loss": 0.0649,
      "step": 1382
    },
    {
      "epoch": 3.707774798927614,
      "grad_norm": 0.025705058127641678,
      "learning_rate": 0.00025844504021447725,
      "loss": 0.0752,
      "step": 1383
    },
    {
      "epoch": 3.710455764075067,
      "grad_norm": 0.031141260638833046,
      "learning_rate": 0.0002579088471849866,
      "loss": 0.0986,
      "step": 1384
    },
    {
      "epoch": 3.71313672922252,
      "grad_norm": 0.03165053203701973,
      "learning_rate": 0.000257372654155496,
      "loss": 0.0747,
      "step": 1385
    },
    {
      "epoch": 3.7158176943699734,
      "grad_norm": 0.02737121656537056,
      "learning_rate": 0.0002568364611260054,
      "loss": 0.0728,
      "step": 1386
    },
    {
      "epoch": 3.7184986595174263,
      "grad_norm": 0.030297838151454926,
      "learning_rate": 0.00025630026809651476,
      "loss": 0.0698,
      "step": 1387
    },
    {
      "epoch": 3.721179624664879,
      "grad_norm": 0.028656965121626854,
      "learning_rate": 0.00025576407506702417,
      "loss": 0.0859,
      "step": 1388
    },
    {
      "epoch": 3.7238605898123325,
      "grad_norm": 0.030278898775577545,
      "learning_rate": 0.0002552278820375335,
      "loss": 0.0796,
      "step": 1389
    },
    {
      "epoch": 3.726541554959786,
      "grad_norm": 0.046109698712825775,
      "learning_rate": 0.0002546916890080429,
      "loss": 0.0684,
      "step": 1390
    },
    {
      "epoch": 3.7292225201072386,
      "grad_norm": 0.057817455381155014,
      "learning_rate": 0.00025415549597855233,
      "loss": 0.0767,
      "step": 1391
    },
    {
      "epoch": 3.7319034852546915,
      "grad_norm": 0.0382850207388401,
      "learning_rate": 0.0002536193029490617,
      "loss": 0.0894,
      "step": 1392
    },
    {
      "epoch": 3.734584450402145,
      "grad_norm": 0.035840559750795364,
      "learning_rate": 0.0002530831099195711,
      "loss": 0.0884,
      "step": 1393
    },
    {
      "epoch": 3.737265415549598,
      "grad_norm": 0.028509462252259254,
      "learning_rate": 0.00025254691689008044,
      "loss": 0.0549,
      "step": 1394
    },
    {
      "epoch": 3.739946380697051,
      "grad_norm": 0.027534563094377518,
      "learning_rate": 0.00025201072386058984,
      "loss": 0.0752,
      "step": 1395
    },
    {
      "epoch": 3.742627345844504,
      "grad_norm": 0.03430178388953209,
      "learning_rate": 0.00025147453083109925,
      "loss": 0.0771,
      "step": 1396
    },
    {
      "epoch": 3.745308310991957,
      "grad_norm": 0.03755056485533714,
      "learning_rate": 0.0002509383378016086,
      "loss": 0.0781,
      "step": 1397
    },
    {
      "epoch": 3.7479892761394105,
      "grad_norm": 0.03677787259221077,
      "learning_rate": 0.000250402144772118,
      "loss": 0.0728,
      "step": 1398
    },
    {
      "epoch": 3.7506702412868633,
      "grad_norm": 0.03375609964132309,
      "learning_rate": 0.00024986595174262736,
      "loss": 0.082,
      "step": 1399
    },
    {
      "epoch": 3.753351206434316,
      "grad_norm": 0.03142120689153671,
      "learning_rate": 0.0002493297587131367,
      "loss": 0.083,
      "step": 1400
    },
    {
      "epoch": 3.7560321715817695,
      "grad_norm": 0.030626000836491585,
      "learning_rate": 0.0002487935656836461,
      "loss": 0.0569,
      "step": 1401
    },
    {
      "epoch": 3.7587131367292224,
      "grad_norm": 0.03432316705584526,
      "learning_rate": 0.00024825737265415546,
      "loss": 0.0791,
      "step": 1402
    },
    {
      "epoch": 3.7613941018766757,
      "grad_norm": 0.03319266811013222,
      "learning_rate": 0.00024772117962466487,
      "loss": 0.0825,
      "step": 1403
    },
    {
      "epoch": 3.7640750670241285,
      "grad_norm": 0.028432555496692657,
      "learning_rate": 0.0002471849865951743,
      "loss": 0.0845,
      "step": 1404
    },
    {
      "epoch": 3.766756032171582,
      "grad_norm": 0.033268507570028305,
      "learning_rate": 0.0002466487935656836,
      "loss": 0.0608,
      "step": 1405
    },
    {
      "epoch": 3.7694369973190347,
      "grad_norm": 0.04506564512848854,
      "learning_rate": 0.00024611260053619303,
      "loss": 0.0864,
      "step": 1406
    },
    {
      "epoch": 3.772117962466488,
      "grad_norm": 0.029169423505663872,
      "learning_rate": 0.00024557640750670244,
      "loss": 0.0737,
      "step": 1407
    },
    {
      "epoch": 3.774798927613941,
      "grad_norm": 0.033226385712623596,
      "learning_rate": 0.0002450402144772118,
      "loss": 0.0708,
      "step": 1408
    },
    {
      "epoch": 3.777479892761394,
      "grad_norm": 0.05100671201944351,
      "learning_rate": 0.0002445040214477212,
      "loss": 0.0806,
      "step": 1409
    },
    {
      "epoch": 3.780160857908847,
      "grad_norm": 0.02582046203315258,
      "learning_rate": 0.00024396782841823057,
      "loss": 0.0708,
      "step": 1410
    },
    {
      "epoch": 3.7828418230563003,
      "grad_norm": 0.02615724690258503,
      "learning_rate": 0.00024343163538873995,
      "loss": 0.0635,
      "step": 1411
    },
    {
      "epoch": 3.785522788203753,
      "grad_norm": 0.026739971712231636,
      "learning_rate": 0.00024289544235924935,
      "loss": 0.061,
      "step": 1412
    },
    {
      "epoch": 3.7882037533512065,
      "grad_norm": 0.06387200206518173,
      "learning_rate": 0.00024235924932975873,
      "loss": 0.0796,
      "step": 1413
    },
    {
      "epoch": 3.7908847184986594,
      "grad_norm": 0.03846714273095131,
      "learning_rate": 0.0002418230563002681,
      "loss": 0.0559,
      "step": 1414
    },
    {
      "epoch": 3.7935656836461127,
      "grad_norm": 0.04802985116839409,
      "learning_rate": 0.0002412868632707775,
      "loss": 0.084,
      "step": 1415
    },
    {
      "epoch": 3.7962466487935655,
      "grad_norm": 0.03172937035560608,
      "learning_rate": 0.00024075067024128687,
      "loss": 0.083,
      "step": 1416
    },
    {
      "epoch": 3.798927613941019,
      "grad_norm": 0.05227194353938103,
      "learning_rate": 0.00024021447721179627,
      "loss": 0.0664,
      "step": 1417
    },
    {
      "epoch": 3.8016085790884717,
      "grad_norm": 0.030537214130163193,
      "learning_rate": 0.00023967828418230565,
      "loss": 0.0781,
      "step": 1418
    },
    {
      "epoch": 3.804289544235925,
      "grad_norm": 0.023831214755773544,
      "learning_rate": 0.00023914209115281503,
      "loss": 0.0491,
      "step": 1419
    },
    {
      "epoch": 3.806970509383378,
      "grad_norm": 0.05554911866784096,
      "learning_rate": 0.0002386058981233244,
      "loss": 0.085,
      "step": 1420
    },
    {
      "epoch": 3.809651474530831,
      "grad_norm": 0.02727903611958027,
      "learning_rate": 0.00023806970509383378,
      "loss": 0.0801,
      "step": 1421
    },
    {
      "epoch": 3.812332439678284,
      "grad_norm": 0.03451027348637581,
      "learning_rate": 0.0002375335120643432,
      "loss": 0.0801,
      "step": 1422
    },
    {
      "epoch": 3.8150134048257374,
      "grad_norm": 0.02877757139503956,
      "learning_rate": 0.00023699731903485257,
      "loss": 0.0737,
      "step": 1423
    },
    {
      "epoch": 3.8176943699731902,
      "grad_norm": 0.030387260019779205,
      "learning_rate": 0.00023646112600536195,
      "loss": 0.0903,
      "step": 1424
    },
    {
      "epoch": 3.8203753351206435,
      "grad_norm": 0.027609892189502716,
      "learning_rate": 0.0002359249329758713,
      "loss": 0.0757,
      "step": 1425
    },
    {
      "epoch": 3.8230563002680964,
      "grad_norm": 0.1657690703868866,
      "learning_rate": 0.0002353887399463807,
      "loss": 0.0947,
      "step": 1426
    },
    {
      "epoch": 3.8257372654155497,
      "grad_norm": 0.029361678287386894,
      "learning_rate": 0.00023485254691689008,
      "loss": 0.0757,
      "step": 1427
    },
    {
      "epoch": 3.8284182305630026,
      "grad_norm": 0.024872561916708946,
      "learning_rate": 0.00023431635388739946,
      "loss": 0.0684,
      "step": 1428
    },
    {
      "epoch": 3.831099195710456,
      "grad_norm": 0.029727034270763397,
      "learning_rate": 0.00023378016085790884,
      "loss": 0.0952,
      "step": 1429
    },
    {
      "epoch": 3.8337801608579087,
      "grad_norm": 0.03690668195486069,
      "learning_rate": 0.00023324396782841821,
      "loss": 0.0928,
      "step": 1430
    },
    {
      "epoch": 3.836461126005362,
      "grad_norm": 0.029515238478779793,
      "learning_rate": 0.00023270777479892762,
      "loss": 0.0669,
      "step": 1431
    },
    {
      "epoch": 3.839142091152815,
      "grad_norm": 0.038878653198480606,
      "learning_rate": 0.000232171581769437,
      "loss": 0.0742,
      "step": 1432
    },
    {
      "epoch": 3.841823056300268,
      "grad_norm": 0.07269502431154251,
      "learning_rate": 0.00023163538873994638,
      "loss": 0.0757,
      "step": 1433
    },
    {
      "epoch": 3.844504021447721,
      "grad_norm": 0.027407802641391754,
      "learning_rate": 0.00023109919571045575,
      "loss": 0.0786,
      "step": 1434
    },
    {
      "epoch": 3.8471849865951744,
      "grad_norm": 0.02560478262603283,
      "learning_rate": 0.00023056300268096513,
      "loss": 0.0669,
      "step": 1435
    },
    {
      "epoch": 3.8498659517426272,
      "grad_norm": 0.02857394702732563,
      "learning_rate": 0.00023002680965147454,
      "loss": 0.085,
      "step": 1436
    },
    {
      "epoch": 3.8525469168900806,
      "grad_norm": 0.03363979607820511,
      "learning_rate": 0.00022949061662198391,
      "loss": 0.0815,
      "step": 1437
    },
    {
      "epoch": 3.8552278820375334,
      "grad_norm": 0.03130899742245674,
      "learning_rate": 0.0002289544235924933,
      "loss": 0.0645,
      "step": 1438
    },
    {
      "epoch": 3.8579088471849867,
      "grad_norm": 0.0241266917437315,
      "learning_rate": 0.00022841823056300267,
      "loss": 0.0635,
      "step": 1439
    },
    {
      "epoch": 3.8605898123324396,
      "grad_norm": 0.028780406340956688,
      "learning_rate": 0.00022788203753351208,
      "loss": 0.0845,
      "step": 1440
    },
    {
      "epoch": 3.863270777479893,
      "grad_norm": 0.027683580294251442,
      "learning_rate": 0.00022734584450402145,
      "loss": 0.0874,
      "step": 1441
    },
    {
      "epoch": 3.8659517426273458,
      "grad_norm": 0.03901103138923645,
      "learning_rate": 0.00022680965147453083,
      "loss": 0.0684,
      "step": 1442
    },
    {
      "epoch": 3.868632707774799,
      "grad_norm": 0.026853248476982117,
      "learning_rate": 0.0002262734584450402,
      "loss": 0.0596,
      "step": 1443
    },
    {
      "epoch": 3.871313672922252,
      "grad_norm": 0.02530730329453945,
      "learning_rate": 0.0002257372654155496,
      "loss": 0.0728,
      "step": 1444
    },
    {
      "epoch": 3.8739946380697052,
      "grad_norm": 0.03548326715826988,
      "learning_rate": 0.000225201072386059,
      "loss": 0.0918,
      "step": 1445
    },
    {
      "epoch": 3.876675603217158,
      "grad_norm": 0.03210477530956268,
      "learning_rate": 0.00022466487935656837,
      "loss": 0.0791,
      "step": 1446
    },
    {
      "epoch": 3.8793565683646114,
      "grad_norm": 0.04450000822544098,
      "learning_rate": 0.00022412868632707775,
      "loss": 0.0986,
      "step": 1447
    },
    {
      "epoch": 3.8820375335120643,
      "grad_norm": 0.036168329417705536,
      "learning_rate": 0.00022359249329758713,
      "loss": 0.0903,
      "step": 1448
    },
    {
      "epoch": 3.8847184986595176,
      "grad_norm": 0.023086490109562874,
      "learning_rate": 0.0002230563002680965,
      "loss": 0.0688,
      "step": 1449
    },
    {
      "epoch": 3.8873994638069704,
      "grad_norm": 0.04437140002846718,
      "learning_rate": 0.0002225201072386059,
      "loss": 0.0767,
      "step": 1450
    },
    {
      "epoch": 3.8900804289544237,
      "grad_norm": 0.060452427715063095,
      "learning_rate": 0.0002219839142091153,
      "loss": 0.0908,
      "step": 1451
    },
    {
      "epoch": 3.8927613941018766,
      "grad_norm": 0.028978072106838226,
      "learning_rate": 0.00022144772117962467,
      "loss": 0.0752,
      "step": 1452
    },
    {
      "epoch": 3.89544235924933,
      "grad_norm": 0.03206819295883179,
      "learning_rate": 0.00022091152815013405,
      "loss": 0.0664,
      "step": 1453
    },
    {
      "epoch": 3.8981233243967828,
      "grad_norm": 0.03421851620078087,
      "learning_rate": 0.00022037533512064345,
      "loss": 0.0791,
      "step": 1454
    },
    {
      "epoch": 3.900804289544236,
      "grad_norm": 0.04121163859963417,
      "learning_rate": 0.00021983914209115283,
      "loss": 0.0913,
      "step": 1455
    },
    {
      "epoch": 3.903485254691689,
      "grad_norm": 0.03354668617248535,
      "learning_rate": 0.0002193029490616622,
      "loss": 0.0981,
      "step": 1456
    },
    {
      "epoch": 3.9061662198391423,
      "grad_norm": 0.03789011389017105,
      "learning_rate": 0.00021876675603217159,
      "loss": 0.0698,
      "step": 1457
    },
    {
      "epoch": 3.908847184986595,
      "grad_norm": 0.0844682827591896,
      "learning_rate": 0.00021823056300268096,
      "loss": 0.082,
      "step": 1458
    },
    {
      "epoch": 3.9115281501340484,
      "grad_norm": 0.02814408391714096,
      "learning_rate": 0.00021769436997319037,
      "loss": 0.0605,
      "step": 1459
    },
    {
      "epoch": 3.9142091152815013,
      "grad_norm": 0.025226226076483727,
      "learning_rate": 0.00021715817694369975,
      "loss": 0.0679,
      "step": 1460
    },
    {
      "epoch": 3.9168900804289546,
      "grad_norm": 0.03298069164156914,
      "learning_rate": 0.00021662198391420913,
      "loss": 0.0928,
      "step": 1461
    },
    {
      "epoch": 3.9195710455764075,
      "grad_norm": 0.028502725064754486,
      "learning_rate": 0.0002160857908847185,
      "loss": 0.0564,
      "step": 1462
    },
    {
      "epoch": 3.9222520107238603,
      "grad_norm": 0.04132649302482605,
      "learning_rate": 0.00021554959785522788,
      "loss": 0.0757,
      "step": 1463
    },
    {
      "epoch": 3.9249329758713136,
      "grad_norm": 0.04317078739404678,
      "learning_rate": 0.0002150134048257373,
      "loss": 0.0762,
      "step": 1464
    },
    {
      "epoch": 3.927613941018767,
      "grad_norm": 0.03718637302517891,
      "learning_rate": 0.00021447721179624666,
      "loss": 0.084,
      "step": 1465
    },
    {
      "epoch": 3.93029490616622,
      "grad_norm": 0.028561478480696678,
      "learning_rate": 0.00021394101876675604,
      "loss": 0.0752,
      "step": 1466
    },
    {
      "epoch": 3.9329758713136727,
      "grad_norm": 0.026685994118452072,
      "learning_rate": 0.00021340482573726542,
      "loss": 0.0664,
      "step": 1467
    },
    {
      "epoch": 3.935656836461126,
      "grad_norm": 0.03432420268654823,
      "learning_rate": 0.0002128686327077748,
      "loss": 0.064,
      "step": 1468
    },
    {
      "epoch": 3.9383378016085793,
      "grad_norm": 0.026699906215071678,
      "learning_rate": 0.0002123324396782842,
      "loss": 0.064,
      "step": 1469
    },
    {
      "epoch": 3.941018766756032,
      "grad_norm": 0.03495703637599945,
      "learning_rate": 0.00021179624664879358,
      "loss": 0.0742,
      "step": 1470
    },
    {
      "epoch": 3.943699731903485,
      "grad_norm": 0.03648735210299492,
      "learning_rate": 0.00021126005361930296,
      "loss": 0.0884,
      "step": 1471
    },
    {
      "epoch": 3.9463806970509383,
      "grad_norm": 0.02957693487405777,
      "learning_rate": 0.00021072386058981234,
      "loss": 0.0742,
      "step": 1472
    },
    {
      "epoch": 3.9490616621983916,
      "grad_norm": 0.027258314192295074,
      "learning_rate": 0.00021018766756032174,
      "loss": 0.0771,
      "step": 1473
    },
    {
      "epoch": 3.9517426273458445,
      "grad_norm": 0.04356416314840317,
      "learning_rate": 0.00020965147453083112,
      "loss": 0.0688,
      "step": 1474
    },
    {
      "epoch": 3.9544235924932973,
      "grad_norm": 0.05204145982861519,
      "learning_rate": 0.0002091152815013405,
      "loss": 0.0781,
      "step": 1475
    },
    {
      "epoch": 3.9571045576407506,
      "grad_norm": 0.02451196499168873,
      "learning_rate": 0.00020857908847184988,
      "loss": 0.064,
      "step": 1476
    },
    {
      "epoch": 3.959785522788204,
      "grad_norm": 0.02594609372317791,
      "learning_rate": 0.00020804289544235923,
      "loss": 0.063,
      "step": 1477
    },
    {
      "epoch": 3.962466487935657,
      "grad_norm": 0.02952900342643261,
      "learning_rate": 0.00020750670241286863,
      "loss": 0.0574,
      "step": 1478
    },
    {
      "epoch": 3.9651474530831097,
      "grad_norm": 0.02745991386473179,
      "learning_rate": 0.000206970509383378,
      "loss": 0.0588,
      "step": 1479
    },
    {
      "epoch": 3.967828418230563,
      "grad_norm": 0.03156936913728714,
      "learning_rate": 0.0002064343163538874,
      "loss": 0.0684,
      "step": 1480
    },
    {
      "epoch": 3.9705093833780163,
      "grad_norm": 0.03145971894264221,
      "learning_rate": 0.00020589812332439677,
      "loss": 0.0732,
      "step": 1481
    },
    {
      "epoch": 3.973190348525469,
      "grad_norm": 0.03840748593211174,
      "learning_rate": 0.00020536193029490615,
      "loss": 0.0598,
      "step": 1482
    },
    {
      "epoch": 3.975871313672922,
      "grad_norm": 0.028943799436092377,
      "learning_rate": 0.00020482573726541555,
      "loss": 0.0903,
      "step": 1483
    },
    {
      "epoch": 3.9785522788203753,
      "grad_norm": 0.03710459545254707,
      "learning_rate": 0.00020428954423592493,
      "loss": 0.0664,
      "step": 1484
    },
    {
      "epoch": 3.9812332439678286,
      "grad_norm": 0.4653792679309845,
      "learning_rate": 0.0002037533512064343,
      "loss": 0.064,
      "step": 1485
    },
    {
      "epoch": 3.9839142091152815,
      "grad_norm": 0.032732877880334854,
      "learning_rate": 0.0002032171581769437,
      "loss": 0.0703,
      "step": 1486
    },
    {
      "epoch": 3.9865951742627344,
      "grad_norm": 0.029355160892009735,
      "learning_rate": 0.0002026809651474531,
      "loss": 0.0791,
      "step": 1487
    },
    {
      "epoch": 3.9892761394101877,
      "grad_norm": 0.036979202181100845,
      "learning_rate": 0.00020214477211796247,
      "loss": 0.0659,
      "step": 1488
    },
    {
      "epoch": 3.991957104557641,
      "grad_norm": 0.040669720619916916,
      "learning_rate": 0.00020160857908847185,
      "loss": 0.0693,
      "step": 1489
    },
    {
      "epoch": 3.994638069705094,
      "grad_norm": 0.028680935502052307,
      "learning_rate": 0.00020107238605898123,
      "loss": 0.0625,
      "step": 1490
    },
    {
      "epoch": 3.9973190348525467,
      "grad_norm": 0.02580038644373417,
      "learning_rate": 0.0002005361930294906,
      "loss": 0.0605,
      "step": 1491
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.056606560945510864,
      "learning_rate": 0.0002,
      "loss": 0.0996,
      "step": 1492
    },
    {
      "epoch": 4.002680965147453,
      "grad_norm": 0.0275904331356287,
      "learning_rate": 0.0001994638069705094,
      "loss": 0.0496,
      "step": 1493
    },
    {
      "epoch": 4.005361930294906,
      "grad_norm": 0.030375242233276367,
      "learning_rate": 0.00019892761394101877,
      "loss": 0.0659,
      "step": 1494
    },
    {
      "epoch": 4.008042895442359,
      "grad_norm": 0.03966427966952324,
      "learning_rate": 0.00019839142091152814,
      "loss": 0.0811,
      "step": 1495
    },
    {
      "epoch": 4.010723860589812,
      "grad_norm": 0.027734767645597458,
      "learning_rate": 0.00019785522788203752,
      "loss": 0.0776,
      "step": 1496
    },
    {
      "epoch": 4.013404825737266,
      "grad_norm": 0.03398522362112999,
      "learning_rate": 0.00019731903485254693,
      "loss": 0.0923,
      "step": 1497
    },
    {
      "epoch": 4.016085790884718,
      "grad_norm": 0.05954136699438095,
      "learning_rate": 0.0001967828418230563,
      "loss": 0.0796,
      "step": 1498
    },
    {
      "epoch": 4.018766756032171,
      "grad_norm": 0.03528664633631706,
      "learning_rate": 0.00019624664879356568,
      "loss": 0.0796,
      "step": 1499
    },
    {
      "epoch": 4.021447721179625,
      "grad_norm": 0.03911449760198593,
      "learning_rate": 0.00019571045576407506,
      "loss": 0.0913,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1865,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8238703533096960.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
