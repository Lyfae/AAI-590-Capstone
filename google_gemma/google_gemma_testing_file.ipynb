{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bits and bytes configuraiton to help handle precision and memory usage\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:29<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = 'google/gemma-2-9b'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, quantization_config=bnbConfig, device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:458: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i am fine, thanks. how are you?\n",
      "\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "new_model_name = 'fine-tuned_model'\n",
    "final_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "\n",
    "question = \"hi, how are you?\"\n",
    "\n",
    "inputs = tokenizer(question, return_tensors = 'pt').input_ids\n",
    "input_ids = inputs.to('cuda')\n",
    "\n",
    "peft_model_outputs = final_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(peft_model_text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "question = \"hi, how are you?\"\n",
    "\n",
    "inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = final_model.generate(**inputs, max_length=500, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, how are you?\n",
      "\n",
      "i am fine, thanks.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n",
      "\n",
      "i am going to the cinema tonight.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
