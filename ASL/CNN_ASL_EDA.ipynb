{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPYmn_wa21i"
      },
      "source": [
        "sign-language-mnist : https://www.kaggle.com/datasets/grassknoted/asl-alphabet/data?select=asl_alphabet_train\n",
        "\n",
        "chatbot dataset :  https://www.kaggle.com/datasets/kreeshrajani/3k-conversations-dataset-for-chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgVKIyLB2f0U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtz-3cQvcEZF",
        "outputId": "98183845-7259-4e33-b41c-b26f1d8d8a45"
      },
      "outputs": [],
      "source": [
        "# code to connect to google drive\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBsktls93TUI"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing your data\n",
        "data_dir = '..\\\\dataset\\\\asl_alphabet_train'\n",
        "\n",
        "# Initialize a list to store the data\n",
        "data = []\n",
        "\n",
        "# Loop through each folder and file in the directory\n",
        "for label in os.listdir(data_dir):\n",
        "    if os.path.isdir(os.path.join(data_dir, label)):  # Check if it's a directory\n",
        "        for image in os.listdir(os.path.join(data_dir, label)):\n",
        "            image_path = os.path.join(data_dir, label, image)\n",
        "            data.append([image_path, label])\n",
        "\n",
        "# Create a DataFrame\n",
        "asl_train_df = pd.DataFrame(data, columns=['image_path', 'label'])\n",
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "asl_train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQJPze0RN0Op",
        "outputId": "07d4a242-cb8e-4367-c1cd-056f858c851c"
      },
      "outputs": [],
      "source": [
        "asl_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asl_train_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU1VAP0NPjwS"
      },
      "source": [
        "# Quick EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dQ4EkKcMNxCy",
        "outputId": "ea00fb42-b502-4d87-ad5c-7b8c526c6914"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 8))  # Set the figure size\n",
        "sns.countplot(y='label', data=asl_train_df, order = asl_train_df['label'].value_counts().index)  # Create a countplot\n",
        "plt.title('Number of Images per Label')  # Set title\n",
        "plt.xlabel('Count')  # Set x-axis label\n",
        "plt.ylabel('Label')  # Set y-axis label\n",
        "plt.show()  # Show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Extraction ( Using Mean Pixel Intensity & Standard Deviation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define a function to extract mean intensity from an image\n",
        "def extract_mean_intensity(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert('L')  # Convert to grayscale\n",
        "    array = np.array(image)\n",
        "    return np.mean(array)\n",
        "\n",
        "# Define a function to extract standard deviation of intensity from an image\n",
        "def extract_std_deviation(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert('L')  # Convert to grayscale\n",
        "    array = np.array(image)\n",
        "    return np.std(array)\n",
        "\n",
        "\n",
        "# Apply feature extraction to each image in the DataFrame\n",
        "asl_train_df['mean_intensity'] = asl_train_df['image_path'].apply(extract_mean_intensity)\n",
        "asl_train_df['std_deviation'] = asl_train_df['image_path'].apply(extract_std_deviation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Matrix plot between Brightness and Contrast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSK-8IGbU5vi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotCorrelationMatrix(df, graphWidth):\n",
        "    df_numeric = df.select_dtypes(include=[np.number])  # Select only numeric columns\n",
        "    if df_numeric.shape[1] < 2:\n",
        "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df_numeric.shape[1]}) is less than 2')\n",
        "        return\n",
        "    corr = df_numeric.corr()\n",
        "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
        "    corrMat = plt.matshow(corr, fignum=1)\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
        "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "    plt.gca().xaxis.tick_bottom()\n",
        "    plt.colorbar(corrMat)\n",
        "    plt.title('Correlation Matrix', fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plotCorrelationMatrix(asl_train_df, graphWidth=8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In image processing and analysis, feature extraction such as mean pixel intensity and standard deviation is crucial for distilling complex image data into more interpretable, numerical forms. \n",
        "\n",
        "Mean pixel intensity provides a measure of the overall brightness or luminance of an image, which can be critical for tasks that involve brightness normalization or thresholding. \n",
        "\n",
        "Standard deviation, on the other hand, measures the variability or contrast within the pixel values, offering insights into the texture and detail present in the image. \n",
        "\n",
        "The correlation matrix plot, showing a negative correlation between these two features, suggests that images with higher average brightness tend to have less contrast and variability in their pixel values. Understanding this relationship helps in optimizing image processing tasks and improving algorithms for tasks like image classification, where consistent image quality can influence performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scatter and Density Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotScatterMatrix(df, plotSize, textSize):\n",
        "    df = df.select_dtypes(include=[np.number])  # Keep only numerical columns\n",
        "    df = df.dropna(axis=1)  # Drop columns with NaN\n",
        "    df = df[[col for col in df if df[col].nunique() > 1]]  # Keep columns with more than 1 unique value\n",
        "    columnNames = list(df)\n",
        "    if len(columnNames) > 10:  # Limit the number of columns to prevent overcrowding\n",
        "        columnNames = columnNames[:10]\n",
        "    df = df[columnNames]\n",
        "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
        "    corrs = df.corr().values\n",
        "    for i, j in zip(*plt.np.triu_indices_from(ax, k=1)):\n",
        "        ax[i, j].annotate(f'Corr. coef = {corrs[i, j]:.3f}', (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
        "    plt.suptitle('Scatter and Density Plot')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot\n",
        "plotScatterMatrix(asl_train_df, plotSize=10, textSize=8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scatter and density plot generated from ASL alphabet image dataset provides crucial insights into the relationships between mean intensity and standard deviation of pixel values. \n",
        "\n",
        "The density plots indicate predominant brightness levels and variability in image contrast, which are important for understanding image characteristics. \n",
        "\n",
        "The scatter plot reveals a slight positive correlation between mean intensity and standard deviation, suggesting these features are somewhat related but still provide independent information. \n",
        "\n",
        "This visualization is essential for guiding feature selection and engineering in machine learning models aimed at classifying ASL signs. Overall, it helps ensure that models are trained on informative, non-redundant features, potentially enhancing classification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Training & Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "asl_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_asl_df = asl_train_df.drop(['mean_intensity', 'std_deviation'], axis =1)\n",
        "new_asl_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the dataframe into training and validation sets\n",
        "asl_train, asl_val = train_test_split(new_asl_df, test_size=0.1, shuffle=True, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and validation sets\n",
        "print(\"Training set shape:\", asl_train.shape)\n",
        "print(\"Validation set shape:\", asl_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time  \n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Import ImageDataGenerator for data augmentation\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "image_size = (64, 64)\n",
        "\n",
        "# Initialize the ImageDataGenerator for training with data augmentation parameters\n",
        "train_generator = ImageDataGenerator(rescale=1./255,  # Rescale pixel values to [0, 1]\n",
        "                                    rotation_range=10,  # Randomly rotate images by up to 10 degrees\n",
        "                                    height_shift_range=0.1,  # Randomly shift images vertically by up to 10%\n",
        "                                    width_shift_range=0.1,  # Randomly shift images horizontally by up to 10%\n",
        "                                    shear_range=0.1,  # Apply shear transformations\n",
        "                                    zoom_range=0.1,  # Randomly zoom in on images by up to 10%\n",
        "                                    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "                                    fill_mode='nearest')  # Fill in new pixels with the nearest pixel values\n",
        "\n",
        "# Initialize the ImageDataGenerator for validation without data augmentation\n",
        "val_generator = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1]\n",
        "\n",
        "# Create the training image generator from the dataframe\n",
        "train_images = train_generator.flow_from_dataframe(asl_train, x_col='image_path', y_col='label',\n",
        "                                                   color_mode='grayscale', class_mode='categorical',\n",
        "                                                   batch_size=batch_size, target_size=image_size,\n",
        "                                                   shuffle=True, seed=0)  \n",
        "\n",
        "# Create the validation image generator from the dataframe\n",
        "val_images = val_generator.flow_from_dataframe(asl_val, x_col='image_path', y_col='label',\n",
        "                                               color_mode='grayscale', class_mode='categorical',\n",
        "                                               batch_size=batch_size, target_size=image_size)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "# Determine the output shape based on the number of unique labels in the training data\n",
        "output_shape = len(set(train_images.labels))\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Input(shape=image_size + (1,), name='input_layer'),  # Input layer with the shape of the images\n",
        "\n",
        "    Conv2D(16, (3, 3), activation='relu'),  # First convolutional layer with 16 filters\n",
        "    MaxPooling2D(pool_size=(2, 2)),  # Max pooling layer with pool size of 2x2\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu'),  # Second convolutional layer with 32 filters\n",
        "    MaxPooling2D(pool_size=(2, 2)),  # Max pooling layer with pool size of 2x2\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # Third convolutional layer with 64 filters\n",
        "    MaxPooling2D(pool_size=(2, 2)),  # Max pooling layer with pool size of 2x2\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),  # Fourth convolutional layer with 128 filters\n",
        "    \n",
        "    GlobalAveragePooling2D(),  # Global average pooling layer\n",
        "    BatchNormalization(),  # Batch normalization layer\n",
        "\n",
        "    Dense(512, activation='relu'),  # First dense layer with 512 units\n",
        "    Dropout(0.5),  # Dropout layer with 50% dropout rate\n",
        "\n",
        "    Dense(256, activation='relu'),  # Second dense layer with 256 units\n",
        "    Dropout(0.5),  # Dropout layer with 50% dropout rate\n",
        "\n",
        "    Dense(128, activation='relu'),  # Third dense layer with 128 units\n",
        "\n",
        "    Dense(output_shape, activation='softmax', name='output_layer')  # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), \n",
        "              loss=CategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# load the model\n",
        "model = load_model(\"asl_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',  # Monitor the validation loss\n",
        "                              factor=0.2,  # Factor by which the learning rate will be reduced\n",
        "                              patience=10,  # Number of epochs with no improvement after which learning rate will be reduced\n",
        "                              min_lr=1e-6)  # Lower bound on the learning rate\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5',  # File path to save the model\n",
        "                                    monitor='val_accuracy',  # Monitor validation accuracy\n",
        "                                    save_best_only=True,  # Save only the best model\n",
        "                                    mode='max',  # Save the model with the maximum accuracy\n",
        "                                    verbose=1)  # Print messages when saving\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_images,  # Training data\n",
        "    epochs=50,  # Number of epochs to train\n",
        "    validation_data=val_images,  # Validation data\n",
        "    callbacks=[reduce_lr, model_checkpoint]  # List of callbacks to be called during training\n",
        ")\n",
        "\n",
        "# Print the training history\n",
        "print(history.history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model to the current directory\n",
        "model.save('asl_model.h5')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Training and Validation Loss and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the history data\n",
        "history_dict = history.history\n",
        "\n",
        "# Extract the metrics\n",
        "train_acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "train_loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "# Create epochs range for x-axis\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
        "plt.plot(epochs, train_acc, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
        "plt.plot(epochs, train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation on ASL Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# load the model\n",
        "model = load_model(\"asl_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
